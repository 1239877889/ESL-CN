<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="The Elements of Statistical Learning(ESL) 的中文翻译">
        
        <link rel="canonical" href="https://szcf-weiya.github.io/ESL-CN/08 Model Inference and Averaging/8.5 The EM Algorithm/">
	<link rel="shortcut icon" href="../../img/favicon.ico">

	<title>8.5 EM算法 - ESL CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="../../css/newsprint.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-85046550-1', 'auto');
          ga('send', 'pageview');

        </script>
<!--
	<style>
	  .dropdown-menu {
    max-height: 500px;
    overflow-y: auto;
    overflow-x: auto;
}
	</style>
-->
    </head>
    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="">ESL CN</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../..">主页</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">目录 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">序言</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../Preface/2016-07-20-Preface-to-the-Second-Edition/">第二版序言</a>
</li>

        
            
<li >
    <a href="../../Preface/2016-07-21-Preface-to-the-First-Edition/">第一版序言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">1 简介</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../01 Introduction/2016-07-26-Chapter-1-Introduction/">1.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">2 监督学习概要</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.1 Introduction/">2.1 导言</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.2 Variable Types and Terminology/">2.2 变量类型和术语</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.3 Two Simple Approaches to Prediction/">2.3 两种预测的简单方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.4 Statistical Decision Theory/">2.4 统计判别理论</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.5 Local Methods in High Dimensions/">2.5 高维问题的局部方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.6 Statistical Models, Supervised Learning and Function Approximation/">2.6 统计模型，监督学习和函数逼近</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.7 Structured Regression Models/">2.7 结构化的回归模型</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.8 Classes of Restricted Estimators/">2.8 限制性估计的类别</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.9 Model Selection and the Bias-Variance Tradeoff/">2.9 模型选择和偏差-方差的权衡</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/Bibliographic Notes/">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">3 回归的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.1 Introduction/">3.1 导言</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.2 Linear Regression Models and Least Squares/">3.2 线性回归模型和最小二乘法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.3 Subset Selection/">3.3 子集的选择</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.4 Shrinkage Methods/">3.4 收缩的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.5 Methods Using Derived Input Directions/">3.5 运用派生输入方向的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.6 A Comparison of the Selection and Shrinkage Methods/">3.6 选择和收缩方法的比较</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.7 Multiple Outcome Shrinkage and Selection/">3.7 多重输出的收缩和选择</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">4 分类的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.1 Introduction/">4.1 导言</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.2 Linear Regression of an Indicator Matrix/">4.2 指示矩阵的线性回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.3 Linear Discriminant Analysis/">4.3 线性判别分析</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.4 Logistic Regression/">4.4 逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.5 Separating Hyperplanes/">4.5 分离超平面</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">5 基展开和正规化</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.1 Introduction/">5.1 导言</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.2 Piecewise Polynomials and Splines/">5.2 分段多项式和样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.3 Filtering and Feature Extraction/">5.3 滤波和特征提取</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.4 Smoothing Splines/">5.4 光滑样条</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">6 核光滑方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.0 Introduction/">6.0 导言</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.1 One-Dimensional Kernel Smoothers/">6.1 一维核光滑器</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.2 Selecting the Width of the Kernel/">6.2 选择核的宽度</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.3 Local Regression in R^p/">6.3 $R^p$中的局部回归</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.4 Structured Local Regression Models in R^p/">6.4 $R^p$中的结构化局部回归模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.5 Local Likelihood and Other Models/">6.5 局部似然和其他模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.6 Kernel Density Estimation and Classification/">6.6 核密度估计和分类</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">7 模型评估及选择</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.1 Introduction/">7.1 导言</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.2 Bias, Variance and Model Complexity/">7.2 偏差，方差和模型复杂度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.3 The Bias-Variance Decomposition/">7.3 偏差-方差分解</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.4 Optimism of the Training Error Rate/">7.4 测试误差率的乐观</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.5 Estimates of In-Sample Prediction Error/">7.5 样本内预测误差的估计</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.6 The Effective Number of Parameters/">7.6 参数的有效个数</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.7 The Bayesian Approach and BIC/">7.7 贝叶斯方法和BIC</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.8 Minimum Description Length/">7.8 最小描述长度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.9 Vapnik-Chervonenkis Dimension/">7.9 VC维</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.10 Cross-Validation/">7.10 交叉验证</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.11 Bootstrap Methods/">7.11 自助法</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.12 Conditional or Expected Test Error/">7.12 条件测试误差或期望测试误差</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/Bibliographic Notes/">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">8 模型推断和平均</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../8.1 Introduction/">8.1 导言</a>
</li>

        
            
<li >
    <a href="../8.2 The Bootstrap and Maximum Likelihood Methods/">8.2 自助法和最大似然法</a>
</li>

        
            
<li >
    <a href="../8.3 Bayesian Methods/">8.3 贝叶斯方法</a>
</li>

        
            
<li >
    <a href="../8.4 Relationship Between the Bootstrap and Bayesian Inference/">8.4 自助法和贝叶斯推断之间的关系</a>
</li>

        
            
<li class="active">
    <a href="./">8.5 EM算法</a>
</li>

        
            
<li >
    <a href="../8.6 MCMC for Sampling from the Posterior/">8.6 MCMC向后采样</a>
</li>

        
            
<li >
    <a href="../8.7 Bagging/">8.7 袋装法</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">9 增广模型，树，以及相关方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.0 Introduction/">9.0 导言</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.1 Generalized Additive Models/">9.1 广义加性模型</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.2 Tree-Based Methods/">9.2 基于树的方法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.3 PRIM(Bump Hunting)/">9.3 耐心规则归纳法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.4 MARS(Multivariate Adaptive Regression Splines)/">9.4 多变量自适应回归样条</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">10 增强和加性树</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.1 Boosting Methods/">10.1 增强方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.2 Boosting Fits an Additive Model/">10.2 对加性模型的增强拟合</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.3 Forward Stagewise Additive Modeling/">10.3 向前逐步加性建模</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.4 Exponential Loss and AdaBoost/">10.4 指数损失和AdaBoost</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.5 Why Exponential Loss/">10.5 为什么是指数损失</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.6 Loss Functions and Robustness/">10.6 损失函数和鲁棒性</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">11 神经网络</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../11 Neural Networks/11.1 Introduction/">11.1 导言</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.2 Projection Pursuit Regression/">11.2 投影寻踪回归</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.3 Neural Networks/">11.3 神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.4 Fitting Neural Networks/">11.4 拟合神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.5 Some Issues in Training Neural Networks/">11.5 训练神经网络的一些问题</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.6 Example of Simulated Data/">11.6 模拟数据的例子</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">12 支持向量机和灵活的判别方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.1 Introduction/">12.1 导言</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.2 The Support Vector Classifier/">12.2 支持向量分类器</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">13 原型方法和最近邻</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.1 Introduction/">13.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">14 非监督学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../14 Unsupervised Learning/14.1 Introduction/">14.1 导言</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.2 Association Rules/">14.2 关联规则</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.3 Cluster Analysis/">14.3 聚类分析</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">15 随机森林</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../15 Random Forests/15.1 Introduction/">15.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">17 无向图模型</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.1 Introduction/">17.1 导言</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.2 Markov Graphs and Their Properties/">17.2 马尔科夫图及其性质</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.3 Undirected Graphical Models for Continuous Variables/">17.3 连续变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.4 Undirected Graphical Models for Discrete Variables/">17.4 离散变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/Bibliographic Notes/">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
                <li >
                    <a rel="next" href="../8.4 Relationship Between the Bootstrap and Bayesian Inference/">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../8.6 MCMC for Sampling from the Posterior/">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                <li>
                    <a href="https://szcf-weiya.github.io">
                        
                        Szcf-Weiya
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#em">EM算法</a></li>
        
            <li><a href="#851">8.5.1 两个组分的混合模型</a></li>
        
            <li><a href="#852-em">8.5.2 一般情形下的EM算法</a></li>
        
            <li><a href="#em-">EM作为一个最大化-最大化的过程</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="em">EM算法</h1>
<table>
<thead>
<tr>
<th>原文</th>
<th><a href="../../book/The Elements of Statistical Learning.pdf">The Elements of Statistical Learning</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>翻译</td>
<td>szcf-weiya</td>
</tr>
<tr>
<td>时间</td>
<td>2016-12-20 &amp; 2017-02-01:2017-02-03</td>
</tr>
</tbody>
</table>
<p>EM算法是简化复杂极大似然问题的一种很受欢迎的工具。我们第一次在一个简单的混合模型中讨论它。</p>
<h2 id="851">8.5.1 两个组分的混合模型</h2>
<p>在这部分我们描述一种简单的用作密度估计的混合模型，以及对应的为求解极大似然估计的EM算法。这与贝叶斯推断中的Gibbs取样方法有着本质的联系。混合模型将在本书其他部分的章节讨论和演示，具体的是6.8,12.7和13.2.3节。</p>
<blockquote>
<p>weiya注：</p>
<p>Gibbs sampling：</p>
<p>假设我们需要从$\mathbf X=(x_1,x_2,\ldots,x_n)$中得到$k$个样本，联合分布为$p(x_1,x_2,\ldots,x_n)$.记第$i$个样本为$\mathbf X^{(i)}=(x_1^{(i)},\ldots,x_n^{(i)})$.我们按下列步骤进行：</p>
<ol>
<li>以初始值$X^{(i)}$开始</li>
<li>需要下一个样本，记为$X^{(i+1)}$.因为$\mathbf X^{(i+1)}=(x_1^{(i+1)},\ldots,x_n^{(i+1)})$是向量，我们需要对向量的每一个组分进行抽样，基于$p(x_j^{(i+1)}\mid x_1^{(i+1)},\ldots,x_{j-1}^{(i+1)},x_{j+1}^{(i)},\ldots,x_n^{(i)})$的分布对$x_j^{(i+1)}$抽样.</li>
<li>重复上述步骤$k$次。</li>
</ol>
</blockquote>
<p>图8.5的左图显示了20个在表8.1中的20个假想数据的直方图。</p>
<p><img alt="" src="../../img/08/fig8.5.png" /></p>
<blockquote>
<p>图8.5. 混合的例子。（左图：）数据的直方图。（右图：）高斯密度的最大似然拟合（红色实线）和观测值$y$的左边成分的解释度（绿色点线）作为$y$的函数。</p>
</blockquote>
<p><img alt="" src="../../img/08/tab8.1.png" /></p>
<blockquote>
<p>表8.1. 图8.5中两个组分混合的例子中使用的20个模拟数据。</p>
</blockquote>
<p>我们想要建立数据点的密度模型，然后由于数据点呈现明显的双峰，高斯分布不是合适的选择。似乎这里有两个潜在的分开的形式，所以我们将$Y$作为两个正态分布混合的模型：
<script type="math/tex; mode=display">
\begin{align}
Y_1&\sim N(\mu_1,\sigma^2_1)\\
Y_2&\sim N(\mu_2,\sigma_2^2)\qquad \qquad \qquad\qquad(8.36)\\
Y&=(1-\triangle)\cdot Y_1 + \triangle\cdot Y_2
\end{align}
</script>
其中$\triangle\in {0,1}$,且$Pr(\triangle =1)=\pi$. 产生过程是很显然的：以概率$\pi$产生$\triangle\in{0,1}$,然后根据输出结果，分配给$Y_1$或$Y_2$.令$\phi_{\theta}(x)$记为参数为$\theta=(\mu,\sigma^2)$的正态分布。则$Y$的密度为
<script type="math/tex; mode=display">
g_Y(y)=(1-\pi)\phi_{\theta_1}(y)+\pi\phi_{\theta_2}(y)\qquad (8.37)
</script>
现在假设我们希望通过极大似然估计来拟合图8.5中数据的模型。参数为
<script type="math/tex; mode=display">
\theta=(\pi,\theta_1,\theta_2)=(\pi,\mu_1,\sigma^2_1,\mu_2,\sigma_2^2)\qquad (8.38)
</script>
基于$N$个训练集的对数概率为
<script type="math/tex; mode=display">
\ell(\theta;\mathbf Z)=\sum\limits_{i=1}^Nlog[(1-\pi)\phi_{\theta_1}(y_i)+\pi\phi_{\theta_2}(y_i)]\qquad (8.39)
</script>
直接对$\ell(\theta;\mathbf Z)$进行最大化在数值上是很困难的，因为求和项在log函数里面。然而，这里有一个更简单的方式。我们考虑一个类似（8.36）中取0或1的潜变量$\triangle_i$：若$\triangle_i=1$则$Y_i$来自模型2，否则模型来自模型1.假设我们已经知道了$\triangle_i$的值。则对数概率为
<script type="math/tex; mode=display">
\begin{align}
\ell_0(\theta;\bf Z,\bf{\Delta})&=\sum\limits_{i=1}^N[(1-\Delta_i)log\phi_{\theta_1}(y_i)+\triangle_ilog\phi_{\theta_2}(y_i)]\\
&+\sum\limits_{i=1}^N[(1-\Delta_i)log(1-\pi)+\triangle_ilog\pi]\qquad(8.40)
\end{align}
</script>
而且$\mu_1$和$\sigma_1^2$的极大似然估计为$\triangle_i=0$时样本均值和方差，类似地对于$\mu_2$和$\sigma_2^2$的极大似然估计为$\triangle_i=1$时的样本均值和方差。$\pi$的估计为$\Delta_i=1$的比例。</p>
<p>因为$\Delta_i$的值实际上是不知道的，我们用一种迭代方式，替换（8.40）中的每个$\Delta_i$，他的期望值
<script type="math/tex; mode=display">
\gamma_i(\theta)=E(\Delta_i\mid\theta,\mathbf Z)=Pr(\Delta_i=1\mid \theta,\mathbf Z)\qquad (8.41)
</script>
也称为模型2对于每个观测$i$的责任（responsibility）。我们用一种称作EM算法（算法8.1中给出）的过程来求解这个特殊的高斯混合模型。在期望（expectation）这一步，我们对每一个模型的每一个观测做一个软赋值：根据每个模型下训练集点的相对密度，参数的当前估计用来给responsibilities赋值。在最大化（maximization）那一步，加权极大似然估计中使用的responsibilities用来更新参数估计。</p>
<p>构造初始的$\hat\mu_1$和$\hat\mu_2$的一种很好的方式便是简单地随机选择$y_i$中的两个。$\hat\sigma^2_1$和$\hat\sigma^2_2$都等于整体的样本方差$\sum_{i=1}^N(y_i-\hat y)^2/N$.最大比例的$\hat\pi$可以从0.5开始。</p>
<p>注意到实际中概率的最大值发生在当我们固定一个数据点，换句话说，对于一些$i$令$\hat\mu_1=y_i$,$\hat\sigma^2_1=0$.这给出了无限大的概率，但是这不是一个有用的解。因此实际上我们寻找概率的一个良好的局部最大值，满足$\hat\sigma^2_1,\hat\sigma^2_2&gt;0$。进一步，可以有多个局部最大值满足$\hat\sigma^2_1,\hat\sigma^2_2&gt;0$.在我们例子中，我们用一系列不同的初始参数值来运行EM算法，所有的都满足$\hat\sigma^2_k&gt;0.5$,然后选择是的概率最大的那个。图8.6显示了在最大化对数概率的EM算法的过程。表8.2显示了在给定迭代次数的EM过程下$\hat\pi=\sum_i\hat\gamma_i/N$是类别2中观测值比例的极大似然估计。</p>
<p><img alt="" src="../../img/08/alg8.1.png" /></p>
<p><img alt="" src="../../img/08/tab8.2.png" /></p>
<blockquote>
<p>表8.2. 对于混合模型选定的几次迭代的EM算法结果</p>
</blockquote>
<p><img alt="" src="../../img/08/fig8.6.png" /></p>
<blockquote>
<p>图8.6. EM算法：观测数据的对数似然关于迭代次数的函数</p>
</blockquote>
<p>最后的极大似然估计为
<script type="math/tex; mode=display">
\hat\mu_1=4.62\qquad \sigma^2_1=0.87\\
\hat\mu_2=1.06\qquad \sigma^2_2=0.77\\
\hat\pi=0.546
</script>
图8.5的右图显示了从这个过程估计的混合高斯分布的密度（实心红色曲线），以及responsibilities（绿色点曲线）。注意到混合在监督学习中也很有用；在6.7节我们显示了高斯混合模型怎样导出radial基函数的版本。</p>
<h2 id="852-em">8.5.2 一般情形下的EM算法</h2>
<p>上面的过程是对于特定问题的类别下最大化概率的EM（或者Baum-Welch）算法。这些问题的概率最大化是困难的，但是通过运用潜在数据（未观测）增大样本会变得简单。这也称作数据增广。这里潜在数据是模型成员$\Delta_i$.在其它问题中，潜在数据是理应被观测到的实际数据但是缺失了。</p>
<p>算法8.2给出了EM算法的一般形式。我们的观测数据是$\mathbf Z$,其对数概率$\ell(\theta;\mathbf Z)$取决于参数$\theta$.潜在数据或者缺失数据为$\mathbf Z^m$,因此完整数据为$\mathbf {T=(Z,Z^m)}$,对数似然函数为$\ell_0(\theta;\mathbf T)$,$\ell_0$基于完整的密度函数。在混合问题中，$(\mathbf{Z,Z^m)=(y,}\Delta)$,且$\ell_0(\theta;\mathbf T)$由（8.40）式给出。</p>
<p><img alt="" src="../../img/08/alg8.2.png" /></p>
<p>在我们的混合例子中，$E(\ell_0(\theta&rsquo;;\mathbf T)\mid \mathbf Z,\hat \theta^{(j)})$仅仅将式（8.40）中的$\Delta_i$替换成解释度$\hat\gamma_i(\hat \theta)$.第三步的最大化仅仅是加权均值和方差。</p>
<p>我们现在给出一个为什么一般情况下EM算法有用的解释。</p>
<p>因为
<script type="math/tex; mode=display">
Pr(\mathbf Z^m\mid \mathbf Z,\theta')=\frac{Pr(\mathbf Z^m,\mathbf Z\mid \theta')}{Pr(\mathbf Z\mid \theta')}\qquad (8.44)
</script>
我们可以写成
<script type="math/tex; mode=display">
Pr(\mathbf Z\mid \theta')=\frac{Pr(\mathbf T\mid \theta')}{Pr(\mathbf Z^m\mid \mathbf Z,\theta')}\qquad (8.45)
</script>
表示成对数似然函数，我们有$\ell(\theta&rsquo;;\mathbf Z)=\ell_0(\theta&rsquo;;\mathbf T)-\ell_1(\theta&rsquo;;\mathbf{Z^m\mid Z})$，其中$\ell_1$是基于条件密度$Pr(\mathbf{Z^m\mid Z,\theta&rsquo;})$.取关于由参数$\theta$确定的$\mathbf{T\mid Z}$分布的条件期望有
<script type="math/tex; mode=display">
\begin{align}
\ell(\theta';\mathbf Z)&=E[\ell_0(\theta';\mathbf T)\mid \mathbf Z,\theta]-E[\ell_1(\theta';\mathbf{Z^m\mid Z)\mid Z,\theta}]\\
&\equiv Q(\theta',\theta)-R(\theta',\theta)\qquad\qquad (8.46)
\end{align}
</script>
在最大化那一步，EM算法最大化关于$\theta&rsquo;$的$Q(\theta&rsquo;,\theta)$,而不是实际的目标函数$\ell(\theta&rsquo;;\mathbf Z)$.为什么这样能成功地最大化$\ell(\theta&rsquo;;\mathbf Z)$?注意到$R(\theta^<em>,\theta)$是相对于由$\theta$索引的相同密度的密度（由$\theta^</em>$索引）的对数似然函数的期望，因此（由琴生不等式）当$\theta^<em>=\theta$时（见练习8.1）最大化关于$\theta^</em>$的函数。所以如果$\theta&rsquo;$最大化$Q(\theta&rsquo;,\theta)$,我们可以看到
<script type="math/tex; mode=display">
\begin{align}
\ell(\theta';\mathbf Z) - \ell(\theta;\mathbf Z) &= [Q(\theta',\theta)-Q(\theta,\theta)]-[R(\theta',\theta)-R(\theta,\theta)]\\
&\ge 0\qquad\qquad\qquad \qquad (8.47)
\end{align}
</script>
因此EM迭代不会降低对数似然值。</p>
<p>这个论据也让我们明白在最大化那一步整体最大化不是必要的：我们仅仅需要找到一个值$\hat\theta^{(j+1)}$使得$Q(\theta&rsquo;,\hat\theta^{(j)})$关于第一个变量是增的，也就是$Q(\hat\theta^{(j+1)},\hat\theta^{(j)}) &gt; Q(\hat\theta^{(j)},\hat\theta^{(j)})$.这一过程称之为GEM（广义EM）算法。EM算法也可以看成是最小化的过程：见练习8.7.</p>
<h2 id="em-">EM作为一个最大化-最大化的过程</h2>
<p>这里从一个不同的角度来看EM过程，作为一个联合最大化算法。考虑函数
<script type="math/tex; mode=display">
F(\theta',\tilde P) = E_{\tilde P}[\ell_0(\theta';\mathbf T)] - E_{\tilde P}[\mathrm{log}\tilde P(\mathbf Z^m)]\qquad (8.48)
</script>
这里$\tilde P(\mathbf Z^m)$是关于潜在数据$\mathbf Z^m$的任意分布。在混合例子中，$\tilde P(\mathbf Z^m)$包含概率集合$\gamma_i=Pr(\Delta_i=1\mid \theta,\mathbf Z)$.注意到当$F$在$\tilde P(\mathbf Z^m)=Pr(\mathbf Z^m\mid \mathbf Z,\theta&rsquo;)$取值时是观察数据的对数似然函数（（8.46）式对所有$\theta$都成立，包含$\theta=\theta&rsquo;$）。函数$F$扩大了对数似然的定义域来帮助最大化。</p>
<p>EM算法可以看成$F$关于$\theta&rsquo;$和$\tilde P(\mathbf Z^m)$的联合最大化，通过固定一个变量来最大化另外一个变量。固定$\theta&rsquo;$来对$\tilde P(\mathbf Z^m)$最大化可以证明是
<script type="math/tex; mode=display">
\tilde P(\mathbf Z^m) = Pr(\mathbf Z^m\mid \mathbf Z,\theta')\qquad (8.49)
</script>
（练习8.2）.这是在求期望的步骤E计算得到的分布，举个例子，如在混合的例子中计算得到的（8.42）。在最大化的步骤M，我们固定$\tilde P$来对$\theta&rsquo;$最大化$F(\theta&rsquo;,\tilde P)$：因为第二项不涉及$\theta&rsquo;$,所以这与最大化第一项$E_{\tilde P}[\ell_0(\theta&rsquo;;\mathbf T)\mid \mathbf Z,\theta]$是一样的。</p>
<p><img alt="" src="../../img/08/fig8.7.png" /></p>
<blockquote>
<p>图8.7. EM算法的最大化-最大化角度。图中画出了（增广）观测数据对数似然函数$F(\theta&rsquo;,\tilde P)$的等高线。步骤E等价于在潜在数据分布的参数上最大化对数似然函数。步骤M在对数似然参数上进行最大化。红色曲线对应观测数据的对数似然函数，这是对每个$\theta&rsquo;$值进行最大化$F(\theta&rsquo;,\tilde P)$得到的曲线。</p>
</blockquote>
<p>最后，因为当$\tilde P(\mathbf Z^m)=Pr(\mathbf Z^m\mid \mathbf Z,\theta&rsquo;)$时，$F(\theta&rsquo;,\tilde P)$和观测数据的对数似然函数是一致的，对前者的最大化也实现了对后者的最大化。图8.7展现了这一过程的整体角度。EM算法的这个角度导致了另一个最大化过程。举个例子，不需要立刻对所有潜在数据参数进行最大化，但是可以每次最大化其中的一个，通过在步骤M来轮流实现。</p>

<div id="disqus_thread"></div>
<script>

/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
/*
var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//weiya.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script id="dsq-count-scr" src="//weiya.disqus.com/count.js" async></script></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
                <center>Copyright &copy; 2016-2017 weiya</center>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
        <script src="../../js/base.js"></script>
	<script src="../../js/jquery.dropdown.js"></script>
        <script src="../../js/MathJax/MathJax.js?config=TeX-AMS_CHTML"></script>
        <script src="../../js/mymathjax.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>