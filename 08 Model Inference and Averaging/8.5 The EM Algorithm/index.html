<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="The Elements of Statistical Learning(ESL) 的中文笔记">
        
        <link rel="canonical" href="https://esl.hohoweiya.xyz/08 Model Inference and Averaging/8.5 The EM Algorithm/index.html">
	<link rel="shortcut icon" href="../../img/favicon.ico">

	<title>8.5 EM算法 - ESL CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css?v201801062" rel="stylesheet">
        <link href="../../css/newsprint.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-85046550-1', 'auto');
          ga('send', 'pageview');

        </script>
        <!--mathjax-->
        <script data-cfasync="false" type="text/x-mathjax-config">
          MathJax.Hub.Config({
          tex2jax: {
          	inlineMath: [['$','$'], ['\\(','\\)']],
          	processEscapes:true
          },
          TeX: {
            Macros: {
              LOG: "{\\mathrm{log }}",
              E: "{\\mathrm{E }}",
              1: "{\\boldsymbol 1}"
            },
          	entensions: ["color.js"]
          }
          });
        </script>
        <script data-cfasync="false" type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.2-beta.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
<!--
	<style>
	  .dropdown-menu {
    max-height: 500px;
    overflow-y: auto;
    overflow-x: auto;
}
	</style>
-->
    </head>
    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="">ESL CN</a>

        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../../index.html">主页</a>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">上篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">序言</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../Preface/2016-07-20-Preface-to-the-Second-Edition/index.html">第二版序言</a>
</li>

        
            
<li >
    <a href="../../Preface/2016-07-21-Preface-to-the-First-Edition/index.html">第一版序言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">1 简介</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../01 Introduction/2016-07-26-Chapter-1-Introduction/index.html">1.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">2 监督学习概要</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.1 Introduction/index.html">2.1 导言</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.2 Variable Types and Terminology/index.html">2.2 变量类型和术语</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.3 Two Simple Approaches to Prediction/index.html">2.3 两种预测的简单方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.4 Statistical Decision Theory/index.html">2.4 统计判别理论</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.5 Local Methods in High Dimensions/index.html">2.5 高维问题的局部方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.6 Statistical Models, Supervised Learning and Function Approximation/index.html">2.6 统计模型，监督学习和函数逼近</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.7 Structured Regression Models/index.html">2.7 结构化的回归模型</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.8 Classes of Restricted Estimators/index.html">2.8 限制性估计的类别</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.9 Model Selection and the Bias-Variance Tradeoff/index.html">2.9 模型选择和偏差-方差的权衡</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">3 回归的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.1 Introduction/index.html">3.1 导言</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.2 Linear Regression Models and Least Squares/index.html">3.2 线性回归模型和最小二乘法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.3 Subset Selection/index.html">3.3 子集的选择</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.4 Shrinkage Methods/index.html">3.4 收缩的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.5 Methods Using Derived Input Directions/index.html">3.5 运用派生输入方向的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.6 A Comparison of the Selection and Shrinkage Methods/index.html">3.6 选择和收缩方法的比较</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.7 Multiple Outcome Shrinkage and Selection/index.html">3.7 多重输出的收缩和选择</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.8 More on the Lasso and Related Path Algorithms/index.html">3.8 Lasso和相关路径算法的补充</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.9 Computational Considerations/index.html">3.9 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">4 分类的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.1 Introduction/index.html">4.1 导言</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.2 Linear Regression of an Indicator Matrix/index.html">4.2 指示矩阵的线性回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.3 Linear Discriminant Analysis/index.html">4.3 线性判别分析</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.4 Logistic Regression/index.html">4.4 逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.5 Separating Hyperplanes/index.html">4.5 分离超平面</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">5 基展开和正规化</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.1 Introduction/index.html">5.1 导言</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.2 Piecewise Polynomials and Splines/index.html">5.2 分段多项式和样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.3 Filtering and Feature Extraction/index.html">5.3 滤波和特征提取</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.4 Smoothing Splines/index.html">5.4 光滑样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters/index.html">5.5 光滑参数的自动选择</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.6 Nonparametric Logistic Regression/index.html">5.6 非参逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.7-Multidimensional-Splines/index.html">5.7 多维样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html">5.8 正则化和再生核希尔伯特空间理论</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html">5.9 小波光滑</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/Appendix-Computations-for-B-splines/index.html">附录-B样条的计算</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">6 核光滑方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.0 Introduction/index.html">6.0 导言</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.1 One-Dimensional Kernel Smoothers/index.html">6.1 一维核光滑器</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.2 Selecting the Width of the Kernel/index.html">6.2 选择核的宽度</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.3 Local Regression in R^p/index.html">6.3 $R^p$中的局部回归</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.4 Structured Local Regression Models in R^p/index.html">6.4 $R^p$中的结构化局部回归模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.5 Local Likelihood and Other Models/index.html">6.5 局部似然和其他模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.6 Kernel Density Estimation and Classification/index.html">6.6 核密度估计和分类</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.7 Radial Basis Functions and Kernels/index.html">6.7 径向基函数和核</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification/index.html">6.8 混合模型的密度估计和分类</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.9-Computational-Consoderations/index.html">6.9 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">中篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">7 模型评估及选择</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.1 Introduction/index.html">7.1 导言</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.2 Bias, Variance and Model Complexity/index.html">7.2 偏差，方差和模型复杂度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.3 The Bias-Variance Decomposition/index.html">7.3 偏差-方差分解</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.4 Optimism of the Training Error Rate/index.html">7.4 测试误差率的optimism</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.5 Estimates of In-Sample Prediction Error/index.html">7.5 样本内预测误差的估计</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.6 The Effective Number of Parameters/index.html">7.6 参数的有效个数</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.7 The Bayesian Approach and BIC/index.html">7.7 贝叶斯方法和BIC</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.8 Minimum Description Length/index.html">7.8 最小描述长度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.9 Vapnik-Chervonenkis Dimension/index.html">7.9 VC维</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.10 Cross-Validation/index.html">7.10 交叉验证</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.11 Bootstrap Methods/index.html">7.11 自助法</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.12 Conditional or Expected Test Error/index.html">7.12 条件测试误差或期望测试误差</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">8 模型推断和平均</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../8.1 Introduction/index.html">8.1 导言</a>
</li>

        
            
<li >
    <a href="../8.2 The Bootstrap and Maximum Likelihood Methods/index.html">8.2 自助法和最大似然法</a>
</li>

        
            
<li >
    <a href="../8.3 Bayesian Methods/index.html">8.3 贝叶斯方法</a>
</li>

        
            
<li >
    <a href="../8.4 Relationship Between the Bootstrap and Bayesian Inference/index.html">8.4 自助法和贝叶斯推断之间的关系</a>
</li>

        
            
<li class="active">
    <a href="index.html">8.5 EM算法</a>
</li>

        
            
<li >
    <a href="../8.6 MCMC for Sampling from the Posterior/index.html">8.6 MCMC向后采样</a>
</li>

        
            
<li >
    <a href="../8.7 Bagging/index.html">8.7 袋装法</a>
</li>

        
            
<li >
    <a href="../8.8 Model Averaging and Stacking/index.html">8.8 模型平均和堆栈</a>
</li>

        
            
<li >
    <a href="../8.9 Stochastic Search/index.html">8.9 随机搜索</a>
</li>

        
            
<li >
    <a href="../Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">9 增广模型，树，以及相关方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.0 Introduction/index.html">9.0 导言</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.1 Generalized Additive Models/index.html">9.1 广义加性模型</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.2 Tree-Based Methods(CART)/index.html">9.2 基于树的方法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.3 PRIM(Bump Hunting)/index.html">9.3 耐心规则归纳法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.4 MARS(Multivariate Adaptive Regression Splines)/index.html">9.4 多变量自适应回归样条</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.5 Hierarchical Mixtures of Experts/index.html">9.5 专家的系统混合</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.6 Missing Data/index.html">9.6 缺失数据</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.7 Computational Considerations/index.html">9.7 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">10 增强和加性树</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.1 Boosting Methods/index.html">10.1 增强方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.2 Boosting Fits an Additive Model/index.html">10.2 boosting拟合可加模型</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.3 Forward Stagewise Additive Modeling/index.html">10.3 向前逐步加性建模</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.4 Exponential Loss and AdaBoost/index.html">10.4 指数损失和AdaBoost</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.5 Why Exponential Loss/index.html">10.5 为什么是指数损失</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.6 Loss Functions and Robustness/index.html">10.6 损失函数和鲁棒性</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.7 Off-the-Shelf Procedures for Data Mining/index.html">10.7 数据挖掘的现货方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.8 Spam Data/index.html">10.8 垃圾邮件的例子</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.9 Boosting Trees/index.html">10.9 boosting树</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.10 Numerical Optimization via Gradient Boosting/index.html">10.10 利用梯度boosting的数值优化</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.11 Right-Sized Trees for Boosting/index.html">10.11 大小合适的boosting树</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.12 Regularization/index.html">10.12 正则化</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">11 神经网络</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../11 Neural Networks/11.1 Introduction/index.html">11.1 导言</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.2 Projection Pursuit Regression/index.html">11.2 投影寻踪回归</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.3 Neural Networks/index.html">11.3 神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.4 Fitting Neural Networks/index.html">11.4 拟合神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.5 Some Issues in Training Neural Networks/index.html">11.5 训练神经网络的一些问题</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.6 Example of Simulated Data/index.html">11.6 模拟数据的例子</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.7-Example-ZIP-Code-Data/index.html">11.7 邮编数字的例子</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">12 支持向量机和灵活的判别方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.1 Introduction/index.html">12.1 导言</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.2 The Support Vector Classifier/index.html">12.2 支持向量分类器</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.3 Support Vector Machines and Kernels/index.html">12.3 支持向量机和核</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">下篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">13 原型方法和最近邻</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.1 Introduction/index.html">13.1 导言</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.2 Prototype Methods/index.html">13.2 原型方法</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.3 k-Nearest-Neighbor Classifiers/index.html">13.3 k最近邻分类器</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.4 Adaptive Nearest-Neighbor Methods/index.html">13.4 自适应的最近邻方法</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.5 Computational Considerations/index.html">13.5 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">14 非监督学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../14 Unsupervised Learning/14.1 Introduction/index.html">14.1 导言</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.2 Association Rules/index.html">14.2 关联规则</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.3 Cluster Analysis/index.html">14.3 聚类分析</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.4 Self-Organizing Maps/index.html">14.4 自组织图</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.5 Principal Components, Curves and Surfaces/index.html">14.5 主成分，主曲线以及主曲面</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.6 Non-negative Matrix Factorization/index.html">14.6 非负矩阵分解</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.7 Independent Component Analysis and Exploratory Projection Pursuit/index.html">14.7 独立分量分析和探索投射寻踪</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.8 Multidimensional Scaling/index.html">14.8 多维缩放</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling/index.html">14.9 非线性降维和局部多维缩放</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.10 The Google PageRank Algorithm/index.html">14.10 谷歌的PageRank算法</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">15 随机森林</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../15 Random Forests/15.1 Introduction/index.html">15.1 导言</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.2 Definition of Random Forests/index.html">15.2 随机森林的定义</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.3 Details of Random Forests/index.html">15.3 随机森林的细节</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.4-Analysis-of-Random-Forests/index.html">15.4 随机森林的分析</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">16 集成学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../16 Ensemble Learning/16.1 Introduction/index.html">16.1 导言</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/16.2 Boosting and Regularization Paths/index.html">16.2 增强和正则路径</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/16.3 Learning Ensembles/index.html">16.3 学习集成</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">17 无向图模型</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.1 Introduction/index.html">17.1 导言</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.2 Markov Graphs and Their Properties/index.html">17.2 马尔科夫图及其性质</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.3 Undirected Graphical Models for Continuous Variables/index.html">17.3 连续变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.4 Undirected Graphical Models for Discrete Variables/index.html">17.4 离散变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">18 高维问题</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.1 When p is Much Bigger than N/index.html">18.1 当p大于N</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.2 Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids/index.html">18.2 对角线性判别分析和最近收缩重心</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.3 Linear Classifiers with Quadratic Regularization/index.html">18.3 二次正则的线性分类器</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.4 Linear Classifiers with L1 Regularization/index.html">18.4 一次正则的线性分类器</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.5 Classification When Features are Unavailable/index.html">18.5 当特征不可用时的分类</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.6 High-Dimensional Regression/index.html">18.6 有监督的主成分</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.7 Feature Assessment and the Multiple-Testing Problem/index.html">18.7 特征评估和多重检验问题</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/Bioliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">个人总结 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
<li >
    <a href="../../summary/sim-7-3/index.html">模拟图7.3</a>
</li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
              <!--
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
              -->
              <!--
                <li>
                  <a href="https://esl.hohoweiya.xyz/08 Model Inference and Averaging/8.5 The EM Algorithm/index.html#disqus_thread">0 Comments</a>
                </li>
              -->
                <li >
                    <a rel="next" href="../8.4 Relationship Between the Bootstrap and Bayesian Inference/index.html">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../8.6 MCMC for Sampling from the Posterior/index.html">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>


                
                <!--
                <li>
                    <a href="https://github.com/szcf-weiya">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
              -->
                <li>
                  <a href="https://github.com/szcf-weiya"><i class="fa fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                <li>
                  <a href="https://stats.hohoweiya.xyz"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                <li>
                  <a href="https://blog.hohoweiya.xyz"><i class="fa fa-pencil" aria-hidden="true"></i> 随笔</a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#85-em">8.5 EM算法</a></li>
        
            <li><a href="#_1">两个组分的混合模型</a></li>
        
            <li><a href="#em">一般情形下的EM算法</a></li>
        
            <li><a href="#em-">EM作为一个最大化-最大化的过程</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="85-em">8.5 EM算法</h1>
<table>
<thead>
<tr>
<th>原文</th>
<th><a href="../../book/The Elements of Statistical Learning.pdf">The Elements of Statistical Learning</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>翻译</td>
<td>szcf-weiya</td>
</tr>
<tr>
<td>时间</td>
<td>2016-12-20 &amp; 2017-02-01:2017-02-03</td>
</tr>
</tbody>
</table>
<p>EM算法是简化复杂极大似然问题的一种很受欢迎的工具。我们首先在一个简单的混合模型中讨论它。</p>
<h2 id="_1">两个组分的混合模型</h2>
<p>这一节我们描述一个估计密度的简单混合模型，以及对应的求解极大似然估计的EM算法。这与贝叶斯推断中的Gibbs取样方法有着本质的联系。混合模型将在本书其他部分的章节讨论和演示，特别是6.8，12.7和13.2.3节。</p>
<div class="admonition note">
<p class="admonition-title">weiya注：</p>
<p>Gibbs sampling：</p>
<p>假设我们需要从$\mathbf X=(x_1,x_2,\ldots,x_n)$中得到$k$个样本，联合分布为$p(x_1,x_2,\ldots,x_n)$。</p>
<p>记第$i$个样本为$\mathbf X^{(i)}=(x_1^{(i)},\ldots,x_n^{(i)})$.我们按下列步骤进行：
<br><br></p>
<ol>
<li>
<p>以初始值$X^{(i)}$开始</p>
</li>
<li>
<p>需要下一个样本，记为$X^{(i+1)}$.因为$\mathbf X^{(i+1)}=(x_1^{(i+1)},\ldots,x_n^{(i+1)})$是向量，我们需要对向量的每一个组分进行抽样，基于$p(x_j^{(i+1)}\mid x_1^{(i+1)},\ldots,x_{j-1}^{(i+1)},x_{j+1}^{(i)},\ldots,x_n^{(i)})$的分布对$x_j^{(i+1)}$抽样.</p>
</li>
<li>
<p>重复上述步骤$k$次。</p>
</li>
</ol>
</div>
<p>图8.5的左图显示了20个在表8.1中的20个模拟数据的直方图。</p>
<p><img alt="" src="../../img/08/fig8.5.png" /></p>
<blockquote>
<p>图8.5. 混合模型的例子。（左图：）数据的直方图。（右图：）高斯密度的最大似然拟合（红色实线）和观测值$y$的左边成分的解释度（绿色点线）作为$y$的函数。</p>
</blockquote>
<p><img alt="" src="../../img/08/tab8.1.png" /></p>
<blockquote>
<p>表8.1. 图8.5中两个组分混合的例子中使用的20个模拟数据。</p>
</blockquote>
<p>我们想要建立数据点的密度模型，然后由于数据点呈现明显的双峰，高斯分布不是合适的选择。这里似乎有两个潜在的分开的形式，所以我们将$Y$作为两个正态分布混合的模型：
<script type="math/tex; mode=display">
\begin{align}
Y_1&\sim N(\mu_1,\sigma^2_1)\\
Y_2&\sim N(\mu_2,\sigma_2^2)\qquad \qquad \qquad\qquad(8.36)\\
Y&=(1-\Delta)\cdot Y_1 + \Delta\cdot Y_2
\end{align}
</script>
其中$\Delta\in {0,1}$,且$Pr(\Delta =1)=\pi$. 产生过程是很显然的：以概率$\pi$产生$\Delta\in\{0,1\}$,然后根据输出结果，分配给$Y_1$或$Y_2$.令$\phi_{\theta}(x)$记为参数为$\theta=(\mu,\sigma^2)$的正态分布。则$Y$的密度为
<script type="math/tex; mode=display">
g_Y(y)=(1-\pi)\phi_{\theta_1}(y)+\pi\phi_{\theta_2}(y)\qquad (8.37)
</script>
现在假设我们希望通过极大似然估计来拟合图8.5中数据的模型。参数为
<script type="math/tex; mode=display">
\theta=(\pi,\theta_1,\theta_2)=(\pi,\mu_1,\sigma^2_1,\mu_2,\sigma_2^2)\qquad (8.38)
</script>
基于$N$个训练集的对数概率为
<script type="math/tex; mode=display">
\ell(\theta;\mathbf Z)=\sum\limits_{i=1}^Nlog[(1-\pi)\phi_{\theta_1}(y_i)+\pi\phi_{\theta_2}(y_i)]\qquad (8.39)
</script>
直接对$\ell(\theta;\mathbf Z)$进行最大化在数值上是很困难的，因为求和项在log函数里面。然而，这里有一个更简单的方式。我们考虑一个类似（8.36）中取0或1的潜变量$\Delta_i$：若$\Delta_i=1$则 $Y_i$来自模型2，否则来自模型1.假设我们已经知道了$\Delta_i$的值。则对数概率为
<script type="math/tex; mode=display">
\begin{align}
\ell_0(\theta;\bf Z,\bf{\Delta})&=\sum\limits_{i=1}^N[(1-\Delta_i)log\phi_{\theta_1}(y_i)+\Delta_ilog\phi_{\theta_2}(y_i)]\\
&+\sum\limits_{i=1}^N[(1-\Delta_i)log(1-\pi)+\Delta_ilog\pi]\qquad(8.40)
\end{align}
</script>
</p>
<div class="admonition note">
<p class="admonition-title">weiya 注：</p>
<p>假设我们已经知道$\Delta_i$的值，设$\Delta_i=0,i\in \cal I$，且$ \Delta_i=1,i\in\cal J$。注意到$p(\Delta_i=0)=1-\pi;p(\Delta_i=1)=\pi$，则似然函数为
<script type="math/tex; mode=display">
L(\theta,\mathbf{Z,\Delta}) = \prod\limits_{i\in \cal I} (1-\Delta_i)(1-\pi)\phi_{\theta_1}(y_i)\prod\limits_{i\in \cal J}\Delta_i\pi\phi_{\theta_2}(y_i)
</script>
上式$i\in \cal I$部分，乘以$(1-\Delta_i)$也就相当于乘以1，同理对于$i\in \cal J$。这样取对数似然便有(8.40)式。</p>
</div>
<p>而且$\mu_1$和$\sigma_1^2$的极大似然估计为$\Delta_i=0$时样本均值和方差，类似地对于$\mu_2$和$\sigma_2^2$的极大似然估计为$\Delta_i=1$时的样本均值和方差。$\pi$的估计为$\Delta_i=1$的比例。</p>
<p>因为$\Delta_i$的值实际上是不知道的，我们用一种迭代方式，替换（8.40）中的每个$\Delta_i$，它的期望值
<script type="math/tex; mode=display">
\gamma_i(\theta)=E(\Delta_i\mid\theta,\mathbf Z)=Pr(\Delta_i=1\mid \theta,\mathbf Z)\qquad (8.41)
</script>
也称为模型2对于每个观测$i$的责任（responsibility）。我们用一种称作EM算法（算法8.1中给出）的过程来求解这个特殊的高斯混合模型。在期望（expectation）这一步，我们对每一个模型的每一个观测做一个软赋值：根据每个模型下训练集点的相对密度，参数的当前估计用来给responsibilities赋值。在最大化（maximization）那一步，加权极大似然估计中使用的responsibilities用来更新参数估计。</p>
<p>构造初始的$\hat\mu_1$和$\hat\mu_2$的一种很好的方式便是简单地随机选择$y_i$中的两个值。$\hat\sigma^2_1$和$\hat\sigma^2_2$都等于整体的样本方差$\sum_{i=1}^N(y_i-\hat y)^2/N$.最大比例的$\hat\pi$可以从0.5开始。</p>
<p>注意到实际中概率的最大值发生在当我们固定一个数据点，换句话说，对于一些$i$令$\hat\mu_1=y_i$,$\hat\sigma^2_1=0$.这给出了无限大的概率，但是这不是一个有用的解。因此实际上我们寻找概率的一个良好的局部最大值，满足$\hat\sigma^2_1,\hat\sigma^2_2&gt;0$。进一步，可以有多个局部最大值满足$\hat\sigma^2_1,\hat\sigma^2_2&gt;0$.在我们例子中，我们用一系列不同的初始参数值来运行EM算法，所有的都满足$\hat\sigma^2_k&gt;0.5$,然后选择使得概率最大的那个。图8.6显示了在最大化对数概率的EM算法的过程。表8.2显示了在给定迭代次数的EM过程下$\hat\pi=\sum_i\hat\gamma_i/N$是类别2中观测值比例的极大似然估计。</p>
<p><img alt="" src="../../img/08/alg8.1.png" /></p>
<p><img alt="" src="../../img/08/tab8.2.png" /></p>
<blockquote>
<p>表8.2. 对于混合模型选定的几次迭代的EM算法结果</p>
</blockquote>
<p><img alt="" src="../../img/08/fig8.6.png" /></p>
<blockquote>
<p>图8.6. EM算法：观测数据的对数似然关于迭代次数的函数</p>
</blockquote>
<p>最后的极大似然估计为
<script type="math/tex; mode=display">
\hat\mu_1=4.62\qquad \sigma^2_1=0.87\\
\hat\mu_2=1.06\qquad \sigma^2_2=0.77\\
\hat\pi=0.546
</script>
图8.5的右图显示了从这个过程估计的混合高斯分布的密度（实心红色曲线），以及responsibilities（绿色点曲线）。注意到混合在监督学习中也很有用；在6.7节我们显示了高斯混合模型怎样导出radial基函数的版本。</p>
<div class="admonition note">
<p class="admonition-title">weiya注<p>自己实现了上述的模拟过程，具体R代码在<a href="https://github.com/szcf-weiya/ESL-CN/blob/master/code/EM/em.R">这里</a></p>
</p>
</div>
<h2 id="em">一般情形下的EM算法</h2>
<p>上面的过程是对于特定问题的类别下最大化概率的EM（或者Baum-Welch）算法。这些问题的概率最大化是困难的，但是通过运用潜在数据（未观测）增大样本会变得简单。这也称作数据增广。这里潜在数据是模型成员$\Delta_i$.在其它问题中，潜在数据是理应被观测到的实际数据但是缺失了。</p>
<p>算法8.2给出了EM算法的一般形式。我们的观测数据是$\mathbf Z$，其对数概率$\ell(\theta;\mathbf Z)$取决于参数$\theta$。潜在数据或者缺失数据为$\mathbf Z^m$，因此完整数据为$\mathbf {T=(Z,Z^m)}$，对数似然函数为$\ell_0(\theta;\mathbf T)$，$\ell_0$基于完整的密度函数。在混合问题中，$(\mathbf{Z,Z^m)=(y,}\Delta)$，且$\ell_0(\theta;\mathbf T)$由（8.40）式给出。</p>
<p><img alt="" src="../../img/08/alg8.2.png" /></p>
<p>在我们的混合例子中，$E(\ell_0(\theta&rsquo;;\mathbf T)\mid \mathbf Z,\hat \theta^{(j)})$是式（8.40），至少后者中的$\Delta_i$替换成了解释度$\hat\gamma_i(\hat \theta)$.第三步的最大化仅仅是加权均值和方差。</p>
<p>我们现在给出一个为什么一般情况下EM算法有用的解释。</p>
<p>因为
<script type="math/tex; mode=display">
Pr(\mathbf Z^m\mid \mathbf Z,\theta')=\frac{Pr(\mathbf Z^m,\mathbf Z\mid \theta')}{Pr(\mathbf Z\mid \theta')}\qquad (8.44)
</script>
我们可以写成
<script type="math/tex; mode=display">
Pr(\mathbf Z\mid \theta')=\frac{Pr(\mathbf T\mid \theta')}{Pr(\mathbf Z^m\mid \mathbf Z,\theta')}\qquad (8.45)
</script>
表示成对数似然函数，我们有$\ell(\theta&rsquo;;\mathbf Z)=\ell_0(\theta&rsquo;;\mathbf T)-\ell_1(\theta&rsquo;;\mathbf{Z^m\mid Z})$，其中$\ell_1$是基于条件密度$Pr(\mathbf{Z^m\mid Z,\theta&rsquo;})$.取关于由参数$\theta$确定的$\mathbf{T\mid Z}$分布的条件期望有
<script type="math/tex; mode=display">
\begin{align}
\ell(\theta';\mathbf Z)&=E[\ell_0(\theta';\mathbf T)\mid \mathbf Z,\theta]-E[\ell_1(\theta';\mathbf{Z^m\mid Z)\mid Z,\theta}]\\
&\equiv Q(\theta',\theta)-R(\theta',\theta)\qquad\qquad (8.46)
\end{align}
</script>
在最大化那一步，EM算法最大化关于$\theta&rsquo;$的$Q(\theta&rsquo;,\theta)$,而不是实际的目标函数$\ell(\theta&rsquo;;\mathbf Z)$。为什么这样能成功地最大化$\ell(\theta&rsquo;;\mathbf Z)$？注意到$R(\theta^*,\theta)$是由$\theta^*$指示的密度的期望，这个密度同时由$\theta$指示，因此（由琴生不等式）当$\theta^*=\theta$时（见练习8.1）最大化关于$\theta^*$的函数。</p>
<div class="admonition note">
<p class="admonition-title">weiya注<p>练习8.1如下：
<img alt="" src="../../img/08/ex8.1.png" />
证明想法如下:
在给定$Y=y$的情况下，记$x=r(y), c=q(y)$
<script type="math/tex; mode=display">
\phi(x)=-\mathrm{log}\frac{c}{x}=\mathrm{log}x-\mathrm{log}c
</script>
为凸函数，则有
<script type="math/tex; mode=display">
\E(\LOG(\frac{r(y)}{q(y)}))=\E(-\LOG(\frac{q(y)}{r(y)}))\ge-\LOG\frac{q(y)}{E(r(y))}=\LOG \E\frac{r(y)}{q(y)}
</script>
当$r(y)=q(y)$时取等。
于是$\E(\LOG r(Y)/q(Y))$在$r(y)=q(y)$时取等。
对于$R(\theta&rsquo;,\theta)$，我们有
<script type="math/tex; mode=display">
R(\theta',\theta)=\frac{\E (\1_{\mathbf Z,\theta}\ell_1(\theta';\mathbf Z^m\mid \mathbf Z))}{P(\mathbf Z,\theta)}
</script>
注意到$\ell_1$其实是对数似然，分母为是与$\theta$有关的常数，所以满足题目中结论的形式，于是$R(\theta,\theta)\ge R(\theta&rsquo;,\theta)$</p>
</p>
</div>
<p>所以如果$\theta&rsquo;$最大化$Q(\theta&rsquo;,\theta)$,我们可以看到
<script type="math/tex; mode=display">
\begin{align}
\ell(\theta';\mathbf Z) - \ell(\theta;\mathbf Z) &= [Q(\theta',\theta)-Q(\theta,\theta)]-[R(\theta',\theta)-R(\theta,\theta)]\\
&\ge 0\qquad\qquad\qquad \qquad (8.47)
\end{align}
</script>
因此EM迭代不会降低对数似然值。</p>
<p>这个论据也让我们明白在最大化那一步整体最大化不是必要的：我们仅仅需要找到一个值$\hat\theta^{(j+1)}$使得$Q(\theta&rsquo;,\hat\theta^{(j)})$关于第一个变量是增的，也就是$Q(\hat\theta^{(j+1)},\hat\theta^{(j)}) &gt; Q(\hat\theta^{(j)},\hat\theta^{(j)})$.这一过程称之为GEM（广义EM）算法。EM算法也可以看成是最小化的过程：见练习8.7.</p>
<h2 id="em-">EM作为一个最大化-最大化的过程</h2>
<p>这里从一个不同的角度来看EM过程，看成一个联合最大化算法。考虑函数
<script type="math/tex; mode=display">
F(\theta',\tilde P) = E_{\tilde P}[\ell_0(\theta';\mathbf T)] - E_{\tilde P}[\mathrm{log}\tilde P(\mathbf Z^m)]\qquad (8.48)
</script>
这里$\tilde P(\mathbf Z^m)$是关于潜在数据$\mathbf Z^m$的任意分布。在混合例子中，$\tilde P(\mathbf Z^m)$包含概率集合$\gamma_i=Pr(\Delta_i=1\mid \theta,\mathbf Z)$。注意到从(8.46)式看，$F$在$\tilde P(\mathbf Z^m)=Pr(\mathbf Z^m\mid \mathbf Z,\theta&rsquo;)$取值，它是观测数据的对数似然函数（（8.46）式对所有$\theta$都成立，包含$\theta=\theta&rsquo;$）。函数$F$扩大了对数似然的定义域来帮助最大化过程。</p>
<p>EM算法可以看成$F$关于$\theta&rsquo;$和$\tilde P(\mathbf Z^m)$的联合最大化，通过固定一个变量来最大化另外一个变量。固定$\theta&rsquo;$来对$\tilde P(\mathbf Z^m)$最大化可以证明是
<script type="math/tex; mode=display">
\tilde P(\mathbf Z^m) = Pr(\mathbf Z^m\mid \mathbf Z,\theta')\qquad (8.49)
</script>
（练习8.2）.这是在求期望的步骤E计算得到的分布，举个例子，如在混合的例子中计算得到的（8.42）。在最大化的步骤M，我们固定$\tilde P$来对$\theta&rsquo;$最大化$F(\theta&rsquo;,\tilde P)$：因为第二项不涉及$\theta&rsquo;$,所以这与最大化第一项$E_{\tilde P}[\ell_0(\theta&rsquo;;\mathbf T)\mid \mathbf Z,\theta]$是一样的。</p>
<p><img alt="" src="../../img/08/fig8.7.png" /></p>
<blockquote>
<p>图8.7. EM算法的最大化-最大化角度。图中画出了（增广）观测数据对数似然函数$F(\theta&rsquo;,\tilde P)$的等高线。步骤E等价于在潜在数据分布的参数上最大化对数似然函数。步骤M在对数似然参数上进行最大化。红色曲线对应观测数据的对数似然函数，这是对每个$\theta&rsquo;$值进行最大化$F(\theta&rsquo;,\tilde P)$得到的曲线。</p>
</blockquote>
<p>最后，因为当$\tilde P(\mathbf Z^m)=Pr(\mathbf Z^m\mid \mathbf Z,\theta&rsquo;)$时，$F(\theta&rsquo;,\tilde P)$和观测数据的对数似然函数是一致的，对前者的最大化也实现了对后者的最大化。图8.7展现了这一过程的示意图。EM算法的这个角度导出了另一个最大化过程。举个例子，不需要立刻对所有潜在数据参数进行最大化，但是可以每次最大化其中的一个，通过在步骤M来轮流实现。</p>
<div class="admonition note">
<p class="admonition-title">weiya注<p>有点类似于坐标轮换(univariate search)，关于坐标轮换及其其他优化方法的介绍，可以参见<a href="https://sites.ualberta.ca/~aksikas/nlpm.pdf">nlpm</a></p>
</p>
</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://esl.hohoweiya.xyz/08 Model Inference and Averaging/8.5 The EM Algorithm/index.html";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "https://esl.hohoweiya.xyz/08 Model Inference and Averaging/8.5 The EM Algorithm/index.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://esl-hohoweiya-xyz.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//esl-hohoweiya-xyz.disqus.com/count.js" async></script></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
                <center>Copyright &copy; 2016-2017 weiya</center>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
        <script src="../../js/base.js"></script>
	<script src="../../js/jquery.dropdown.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>