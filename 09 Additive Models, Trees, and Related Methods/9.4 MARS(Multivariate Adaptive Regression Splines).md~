#MARS: 多变量自适应回归样条 

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-03-13                               |

MARS是回归的自适应过程，并且非常适合高维问题（比如，大量的输入）。这可以看成逐步线性回归的推广或者改动CART方法使得改进回归设定中的表现。我们以第一种观点引入MARS，接着与CART联系起来。

MARS采用形式为$(x-t)_+$和$(t-x)_+$的分段线性基函数的展开。“+”表示正的部分，所以
$$
(x-t)_+=\left\{
\begin{array}
x-t& \text{if }x>t\\
0&\text{otherwise}
\end{array}
\right.
\text{  and  }
(t-x)_+=\left\{
\begin{array}
t-x& \text{if }x<t\\
0&\text{otherwise}
\end{array}
\right.
$$
作为一个例子，函数$(x-0.5)_+$和$(0.5-x)_+$显示在图9.9中。

![](../img/09/fig9.9.png)

> 图9.9. MARS采用的基函数$(x-t)_+$(实心橘黄色)和$(t-x)_+$(蓝色虚线)

每个函数是分段线性的，在值$t$处有一个结点。用第五章的术语说它们是线性样条。在下面的讨论中我们将这两个函数称为反射对(reflected pair)。想法是，对于每个输入$X_j$，取结点为每个输入的观测值。因此，基函数集合为
$$
{\cal C}=\{(X_j-t)_+,(t-X_j)_+\}_{t\in\{x_{1j},x_{2j},\ldots,x_{Nj}\},j=1,2,\ldots,p}\qquad (9.18)
$$
如果所有的输入值都不同，则总共有$2Np$个基函数。注意到尽管每个基函数仅仅取决于单个的$X_j$，举个例子，$h(X)=(X_j-t)_+$，被看成是整个输入空间$R^p$中的函数。

建立模型的策略类似向前逐步线性回归，但是不是使用原始输入，我们允许使用从集合$\cal C$的函数和它们的乘积。因此模型有如下形式
$$
f(X)=\beta_0+\sum\limits_{m=1}^M\beta_mh_m(X)\qquad (9.19)
$$
其中每个$h_m(X)$是$\cal C$中的函数，或者两个或者更多这样函数的乘积。

给定$h_m$的选择，系数$\beta_m$通过最小残差平方和来估计，也就是，通过标准线性回归。然而，实际的艺术是函数$h_m(x)$的构造。我们在我们的模型中以常数函数$h_0(X)=1$来开始，集合$\cal C$中的所有函数都是候选函数。这展示在图9.10中。

![](../img/09/fig9.10.png)

> 图9.10. MARS向前建模的过程示意图。左边是已经在模型中的基函数：初始时，是常值函数$h(X)=1$。右图中是所有考虑加进模型的候选基函数。如图9.9中所示有成对分段线性基函数，结点$t$在每个预测变量$X_j$的所有的唯一观测值$x_{ij}$处。在每一步，我们考虑模型中的基函数和候选反射对的所有乘积。乘积使得残差下降最多则被加进当前模型中。我们图示了这个过程的前三步，选择的函数用红色表示。

在每一步我们将模型集合$\cal M$中函数$h_m$的与$\cal C$中某个反射对的所有乘积看成是新的基函数。我们往模型$\cal M$中加入如下形式的项，该项使得训练误差有最大下降。
$$
\hat\beta_{M+1}h_\ell(X)\cdot (X_j-t)_++\hat\beta_{M+2}h_\ell(X)\cdot(t-X_j)_+,h_\ell\in \cal M
$$
这里$\hat\beta_{M+1}$和$\hat \beta_{M+2}$是由最小二乘估计的系数，和模型中其他的$M+1$个系数一样。接着“胜出”的乘积加入到模型中，这个过程不断继续直到模型集合$\cal M$包含一些预设定的最大数目的项。

举个例子，在第一步我们考虑往模型中加入形如$\beta_1(X_j-t)_++\beta_2(t-X_j)_+;t\in\{x_{ij}\}$的函数，因为乘以常数仅仅得到函数本身。假设最优选择为$\hat\beta_1(X_2-x_{72})_++\hat\beta_2(x_{72}-X_2)_+$。则这个基函数对加入到模型$\cal M$中，并且在下一步我们考虑加入如下形式的乘积对
$$
h_m(X)\cdot(X_j-t)_+\text{     and    }h_m(X)\cdot (t-X_j)_+,t\in\{x_{ij}\}
$$
其中对于$h_m$我们有如下选择
$$
\begin{align}
h_0(X)&=1\\
h_1(X)&=(X_2-x_{72})_+\\
h_2(X)&=(x_{72}-X_2)_+
\end{align}
$$
第三个选择得到形如$(X_1-x_{51})_+\cdot (x_{72}-X_2)_+$的函数，如图9.11所示。

![](../img/09/fig9.11.png)

> 图9.11. 函数$h(X_1,X_2)=(X_1-x_{51})_+\cdot (x_{72}-X_2)_+$，从两个分段线性的MARS基函数相乘得到。

这个过程的最后我们有形如(9.19)的大量模型。这些模型一般对数据过拟合，所以应用向后删除过程。每一步中，删掉的项使得残差平方和增长最小，得到每个大小$\lambda$（项的个数）下的最优模型的估计$\hat f_\lambda$。可以采用交叉验证来估计最优的$\lambda$，但是为了节省计算，MARS过程采用的是广义交叉验证。准则定义为
$$
GCV(\lambda)=\frac{\sum_{i=1}^N(y_i-\hat f_\lambda(x_i))^2}{(1-M(\lambda)/N)^2}\qquad (9.20)
$$
$M(\lambda)$是模型中有效参数的个数：这包含模型中项的个数，加上在选择最优结点位置的参数个数。一些数学和拟合结果表明应该为在分段线性回归中选择结点的三个参数付出代价。

!!! notes "weiya 注"

GCV(\lambda) =

如果模型中含有$r$个线性独立基函数，并且在向前过程中选择$K$个结点，公式为$M(\lambda)=r+cK$，其中$c=3$。（当模型限定为加性——细节如下——则使用$c=2$的惩罚）。采用这些，我们沿着向后序列选择使得$GCV(\lambda)$最小的模型。

