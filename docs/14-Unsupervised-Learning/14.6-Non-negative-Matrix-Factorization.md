# 14.6 非负矩阵分解

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-09-03                   |

**非负矩阵分解** (Lee and Seung, 1999)[^1]是最近提出来的一个用来替代主成分分析的方法，在这个方法中数据以及成分假定为非负的。这对于建立非负数据的模型很有用，比如图象数据。

$N\times p$ 数据矩阵 $\mathbf X$ 由下式近似

$$
\mathbf X \approx \mathbf W\mathbf H\qquad (14.72)
$$

其中 $\mathbf W$ 阶为 $N\times r$，$\mathbf H$ 阶为 $r\times p, r\le \max(N,p)$。我们假设 $x_{ij}, w_{ik}, h_{kj}\ge 0$。

通过最大化下式来确定 $\mathbf W$ 和 $\mathbf H$

$$
L(\mathbf W, \mathbf H)=\sum\limits_{i=1}^N\sum\limits_{j=1}^p[x_{ij}\log(\mathbf {WH}_{ij}-(\mathbf{WH})_{ij})]\qquad (14.73)
$$

这是模型的对数似然，模型中 $x_{ij}$ 服从均值为 $(\mathbf{WH}_{ij})$ 的 Poisson 分布——这对于正值数据是很合理的。

下面的替代算法 (Lee and Seung, 2001)[^2] 收敛到 $L(\mathbf W, \mathbf H)$ 的局部最大值：

<!-- 
$$
\begin{array}{ll}
w_{ik}&\leftarrow w_{ik}\frac{\sum_{j=1}^ph_{kj}x_{ij}/(\mathbf{WH}_{ij})}{\sum_{j=1}^ph_{kj}}\\
h_{kj}&\leftarrow h_{kj}\frac{\sum_{j=1}^pw_{ik}x_{ij}/(\mathbf{WH}_{ij})}{\sum_{j=1}^pw_{ik}}
\end{array}\qquad (14.74)
$$
-->


$$
\begin{align}
w_{ik}&\leftarrow w_{ik}\frac{\sum_{j=1}^ph_{kj}x_{ij}/(\mathbf{WH}_{ij})}{\sum_{j=1}^ph_{kj}}\\
h_{kj}&\leftarrow h_{kj}\frac{\sum_{j=1}^pw_{ik}x_{ij}/(\mathbf{WH}_{ij})}{\sum_{j=1}^pw_{ik}}
\end{align}\qquad (14.74)
$$

这个算法可以由最大化 $L(\mathbf W, \mathbf H)$ 的最小过程来导出（[练习14.23](https://github.com/szcf-weiya/ESL-CN/issues/86)），并且与对数线性模型的 **迭代比例缩放算法 (iterative-proportional-scaling algorithm)** 相关（[练习14.24](https://github.com/szcf-weiya/ESL-CN/issues/87)）。

TODO

## 典型分析

TODO


[^1]: Lee, D. and Seung, H. (1999). Learning the parts of objects by non-negative matrix factorization, Nature 401: 788.
[^2]: Lee, D. and Seung, H. (2001). Algorithms for non-negative matrix factorization, Advances in Neural Information Processing Systems, (NIPS 2001), Vol. 13, Morgan Kaufman, Denver., pp. 556–562.
