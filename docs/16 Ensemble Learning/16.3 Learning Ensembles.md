# 学习集成

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-09-25                               |

前面章节的知识可以用来产生更高效的集成模型。我们继续考虑下面形式的函数

$$
f(x)=\alpha_0+\sum\limits_{T_k\in \cal T}\alpha_kT_k(x)\qquad (16.8)
$$

其中$\cal T$是基函数的字典，一般是树。对于gradient boosting和随机森林，$\vert\cal T\vert$非常大，而且最后的模型一般会涉及上千棵树。

Friedman and Popescu (2003)提出混合的方式，它将这个过程分解为下面两步：

- 从训练集中导出有限的基函数字典集$\cal T_L=\{T_1(x), T_2(x),\ldots, T_M(x)\}$;
- 通过在字典集中拟合lasso路径来构造$f_\lambda(x)$函数族。
$$
\alpha(\lambda)=arg\;\underset{\alpha}{min}\sum\limits_{i=1}^NL[y_i, \alpha_0+\sum\limits_{m=1}^M\alpha_mT_m(x_i)]+\lambda\sum\limits_{m=1}^M\vert \alpha_m\vert\qquad (16.9)
$$

在这种简单的形式里面，将$\cal T_L$看成是由梯度boosting算法或者随机森林算法得到的树的集合，该模型可以看成是post-processing boosting或者是随机森林的某种方式。通过对这些树进行lasso路径拟合，一般我们会得到更加简化的集合，它会大大减少未来预测的计算量和存储量。在下一节，我们将讨论这种方法的变种，进而降低$\cal T_L$的相关性，并且提高lasso post processor的表现。

作为初步的说明，我们将这个过程应用到对spam数据集成的随机森林上。

![](../img/16/fig16.6.png)

图16.6 展示了lasso post-processing对随机森林（蓝色曲线）有了较大程度的改善，并且将森林降至了40棵树，而不是原来的1000棵。这个post-processed表现与gradient boosting 相当。橘黄色曲线展示了修改版的随机森林，它是为了更大程度上降低树之间的相关性。这里训练样本$5\%$的子样本（无放回）用来生成每棵树，并且每棵树限制得很低（大约6个终止结点）。这种情形下post-processing有更显著的改善，而且训练花费大概降低了100倍。然而，这个post-processed模型的表现有点比蓝色曲线差。
