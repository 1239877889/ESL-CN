# 增强和正则路径

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-03-09                               |

在本书的第10.12.2节中，我们表明了由梯度加速算法产生的模型序列与在高维特征空间中拟合的正则模型是相似的。这最初是观测到线性模型的加速版本和lasso之间（第3.4.2节）的紧密联系得到的。我们和其他人一起研究这些联系，并且在这里放上在这个领域我们当前的思考。我们从最初的动机开始，这在本章中很自然地适合于集成学习。

## 惩罚回归

梯度加速的收缩策略(10.41)的成功直觉上可以通过类比含有很大的基展开的惩罚线性回归得到。考虑所有可能的$J$终止结点的回归树$\cal T=\{T_k\}$的字典作为$R^p$的基函数，这在训练数据上可以实现。线性模型为
$$
f(x)=\sum\limits_{k=1}^K\alpha_kT_k(x)\qquad (16.1)
$$

其中$T=card(\cal T)$。假设系数通过最小二乘进行估计。因为这些树的个数可能比最大训练集大很多，需要一些形式的正则。令$\hat\alpha(\lambda)$使得
$$
\underset{\alpha}{min}\{\sum\limits_{i=1}^N(y_i-\sum\limits_{k=1}^K\alpha_kT_k(x_i))^2+\lambda\cdot J(\alpha)\}\qquad (16.2)
$$

$J(\alpha)$是关于系数的函数，并且一般惩罚较大的值。比如
$$
\begin{align}
J(\alpha) &=\sum\limits_{k=1}^K\vert \alpha_k\vert^2\qquad \text{岭回归}\qquad (16.3)\\
J(\alpha) &=\sum\limits_{k=1}^K\vert\alpha_K\vert\qquad\text{lasso}\qquad (16.4)
\end{align}
$$
这些都在3.4节介绍了。正如这里讨论的那样，从中等到较大的$\lambda$的lasso问题的解趋向于是稀疏的；许多$\hat\alpha_k(\lambda)=0$. 也就是，只有所有可能树的一小部分会选进模型(16.1)。这似乎是合理的，因为似乎只有所有可能树的一小部分对近似任意特定的目标函数是恰当的。那些没有设为0的系数被lasso收缩了，因为它们的绝对值比对应最小二乘值要小：$\vert\hat\alpha_k(\lambda)\vert<\vert\hat\alpha_k(0)\vert$。当$\lambda$增大时，系数都进行了收缩，每一个最终都变为0。

由于非常大量的基函数$T_k$，直接求解含有lasso惩罚(16.4)的(16.2)是不可能的。然而，存在可行的向前逐步策略，对lasso的影响有非常大的近似，并且与增强和向前逐步算法10.2非常相似。算法16.1给出了细节。尽管是用树的基函数$T_k$表示的，算法也可以用来任意基函数的集合中。	