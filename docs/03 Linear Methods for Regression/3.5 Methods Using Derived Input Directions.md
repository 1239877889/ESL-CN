# 运用派生输入方向（Derived Input Directions）的方法

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-10-14:2016-10-14                    |

在很多情形下我们有很多输入，这些输入相关性经常是非常强的。这一小节中的方法得到较少的原输入变量$X_j$的线性组合$Z_m,m=1,2,\ldots,M$，然后$Z_m$用来代替$X_j$来作为回归的输入。这些方法区别于怎样构造线性组合。

## 主成分回归

在这种方法下，使用的线性组合$Z_m$是在前面3.4.1节中定义的主成分。

主成分回归构造派生的输入列$\mathbf z_m=\mathbf Xv_m$，然后在$\mathbf z_1,\mathbf z_2,\ldots,\mathbf z_M,\; M\le p$上回归$\mathbf y$.因为$\mathbf z_m$是正交的，这个回归仅仅是一个单变量回归的和
$$
\hat{\mathbf y}^{pcr}_{(M)}=\bar y\mathbf 1+\sum\limits_{m=1}^M\hat{\theta}_m\mathbf z_m\qquad (3.61)
$$
其中，$\hat\theta_m=\langle \mathbf z_m,\mathbf y\rangle/\langle\mathbf z_m,\mathbf z_m\rangle$. 因为$\mathbf z_m$是原输入变量$\mathbf x_j$的每个线性组合，我们可以将解（3.61）表达成关于$\mathbf x_j$的函数（练习3.13）
$$
\hat\beta^{pcr}(M)=\sum\limits_{m=1}^M\hat\theta_mv_m\qquad (3.62)
$$

岭回归下，主成分依赖输入$\mathbf x_j$的放缩尺度