# Lasso和相关路径算法的补充

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-10-21:2016-10-21                    |

自从LAR算法（Efron等人2004）的提出，许多研究都在发展对于不同问题的正则化拟合算法。另外，$L_1$正则有它自己的用处，它引领了信号处理领域的压缩传感（compressed sensing）.（Donoho,2006a;Candes, 2006）.在这部分我们讨论一些相关的提议和以LAR算法为先驱的其它路径算法。

## 增长的向前逐渐回归

这里我们提出另外一种类似LAR的算法，这次集中在向前逐渐回归。有趣地是，理解一个灵活的非线性回归过程的努力导出了线性模型的一个新算法。在阅读本书的第一版时，第16章的向前逐渐算法16.1，Brad Efron意识到对于线性模型，可以明确地构造出如图3.10所示的分段线性的lasso路径。这导致他提出3.4.4节介绍的LAR过程，以及这里提到的向前逐渐回归的增长版本。

****
**算法 3.4** 增长的向前逐渐回归——$FS_\epsilon$
****
1. 从残差向量$\mathbf r$等于$\mathbf y$开始，$\beta_1,\beta_2,\ldots,\beta_p=0$. 所有的预测变量进行标准化使得均值为0、方差为1.
2. 寻找与残差向量$\mathbf r$最相关的预测变量$\mathbf x_j$
3. 更新$\beta_j\leftarrow\beta_j+\delta_j$, 其中$\delta_j=\epsilon\cdot sign[\langle \mathbf x_j,\mathbf r\rangle]$并且$\epsilon>0$是一个很小的步长，然后令$\mathbf r=\mathbf r-\delta_j\mathbf x_j$
4. 重复步骤2和步骤3，直到所有的残差向量与所有的预测变量都不相关。

****

考虑16.1节（p608）提出的向前逐渐推进（the forward-stagewise）算法16.1的线性版本。它通过重复更新（$\epsilon$很小）与当前残差最相关的系数产生系数曲线。算法3.4给出了具体的细节。图3.19（左边）显示了在前列腺癌数据中步长为$\epsilon=0.01$的过程。如果$\delta_j=\langle \mathbf x_j,\mathbf r\rangle$(残差在第$j$个预测变量的最小二乘系数)，则恰巧是在3.3节介绍的普通向前逐渐过程（FS）。

这里我们主要对小的$\epsilon$值感兴趣。令$\epsilon\rightarrow0$便得到了图3.19的右图，在这种情形下与图3.10的lasso路径相同。我们称这个极限过程为无限小的向前逐渐回归或者$FS_0$。这个过程在非线性中有着很重要的作用，自适应方法比如boosting（第10和16章），并且是增长的向前逐渐回归的最能禁得起理论分析的版本。Buhlmann和Hothorn(2008)称这个过程为"L2boost",因为它与boosting的关系。

Efron最初认为LAR算法3.2是$FS_0$的一个实现，允许每个tied变量以一种平衡的方式更新他们的系数，并且在相关性方面保持tied. 然而，他然后意识到LAR最小二乘拟合在这些tied预测变量中间可以导致系数从相反的方向移动到相关的方向，这是在算法3.4中不可能发生的。下面对LAR算法的修正实现了$FS_0$：

****
**算法 3.2b** 最小角回归：$FS_0$修正
****
4.通过求解下面的约束最小二乘问题找到新的方向
$$
\underset{b}{min}\Vert\mathbf r-\mathbf X_{\cal A}b\Vert^2_2 \;s.t.\; b_js_j\ge 0,\;j\in\cal A
$$
其中，$s_j$是$\langle\mathbf x_j,\mathbf r \rangle$的方向。
****

这个修正相当于一个非负的最小二乘拟合, 保持系数的符号与相关系数的符号一致.可以证明它实现了对于最大相关性的tied变量的无限小"更新"的最优平衡。（Hastie等人，2007）。类似lasso，全$FS_0$路径可以通过LAR算法非常有效地计算出来。

作为这些事实的结果，如果LAR图象是单调不减或者单调不增，正如3.19所示，则LAR，lasso，以及$FS_0$这三种算法给出了相同的图象。如果图象不是单调的但是不穿过0，则LAR和lasso是一样的。

因为$FS_0$与lasso不同，很自然地问它是否完善了准则。答案比lasso更加的复杂；$FS_0$系数曲线是微分方程的一个解。然而lasso在降低系数向量$\beta$的$L_1$范数的单位残差平方和增长方面实现了最优化，$FS_0$在沿着系数路径的$L_1$弧长的单位增长是最优的。因此它的系数曲线不会经常改变方向。

$FS_0$比lasso更约束，事实上也可以看成是lasso的单调版本；看614页的图16.3生动的例子。$FS_0$可能在$p>>N$情形下很有用，它的系数曲线更加的光滑，因此比lasso有更小的方差。更多关于$FS_0$的细节将在16.2.3节给出以及Hastie等人的工作（2007）。图3.16包含了$FS_0$, 它的表现非常类似于lasso。

