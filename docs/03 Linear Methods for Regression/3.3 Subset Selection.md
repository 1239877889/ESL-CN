# 子集的选择

原文     | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf)
      ---|---
翻译     | szcf-weiya
时间     | 2016-08-05

There are two reasons why we are often not satisfied with the least squares estimates (3.6).

两个原因使得我们经常不满足最小二乘估计（3.6）

> - The first is prediction accuracy: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coefficients to zero. By doing so we sacrifice a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.
> - The second reason is interpretation. With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest effects. In order to get the “big picture,” we are willing to sacrifice some of the small details.

- 第一个是预测的精确性（prediction accuracy）：最小二乘估计经常有小偏差大方差。预测精确性有时可以通过收缩或者令某些系数为0来提高。通过这些方法我们牺牲一点偏差降低预测值的方差，因此可能提高整个预测的精确性。
- 第二个原因是可解释性（interpretation）：有大量的预测变量，我们经常去确定一个小的子集来保持最强的影响。为了得到大的图片，我们愿意牺牲一些小的细节。

> In this section we describe a number of approaches to variable subset selection with linear regression. In later sections we discuss shrinkage and hybrid approaches for controlling variance, as well as other dimension-reduction strategies. These all fall under the general heading model selection. Model selection is not restricted to linear models; Chapter 7 covers this topic in some detail.

这部分我们描述一些线性回归选择变量子集的方法。在后面的部分中我们讨论控制方差的收缩和混合的方法，以及其它降维的策略。这些都属于一个一般的方向——模型选择（model selection）。模型选择不局限于线性模型；第7章将详细介绍这个主题。

> With subset selection we retain only a subset of the variables, and eliminate the rest from the model. Least squares regression is used to estimate the coefficients of the inputs that are retained. There are a number of different strategies for choosing the subset.

子集选择我们只保留变量的一个子集，并且除去模型中的剩余部分。最小二乘回归用来预测保留下来的输入的系数。这里有一系列不同的选择子集的策略。

#### 3.3.1 最优集的选择

> Best subset regression finds for each $k \in  {0,1,2,... ,p}$ the subset of size $k$ that gives smallest residual sum of squares (3.2). An efficient algorithm—the leaps and bounds procedure (Furnival and Wilson, 1974)—makes this feasible for $p$ as large as 30 or 40. Figure 3.5 shows all the subset models for the prostate cancer example. The lower boundary represents the models that are eligible for selection by the best-subsets approach. Note that the best subset of size 2, for example, need not include the variable that was in the best subset of size 1 (for this example all the subsets are nested). The best-subset curve (red lower boundary in Figure 3.5) is necessarily decreasing, so cannot be used to select the subset size k. The question of how to choose k involves the tradeoff between bias and variance, along with the more subjective desire for parsimony. There are a number of criteria that one may use; typically we choose the smallest model that minimizes an estimate of the expected prediction error.

对于每个$k\in \{0,1,2,\ldots,p\}$，最优子集回归要找出残差平方和（3.2）最小的规模为$k$的子集。一个有效的算法——leaps and bounds 过程(Furnival and Wilson, 1974)——使得可行的$p$值达到30或40。图3.5展示了前列腺癌例子中所有的子集模型。下界代表通过最优子集方法选择的符合 条件的模型。注意到最优子集规模为2，举个例子，不需要包含规模为1最优子集中的变量（这个例子中所有的子集是嵌套的）。最优子集曲线（图3.5中的红色下边界）必然地下降，所以不能用来选择子集的规模$k$。怎样选择$k$涉及偏差和方差之间的平衡，伴随着更加主观的要求。有一些可能会使用的准则。典型地，我们选择最小的模型使得预测误差期望值的估计最小。

![](fig3.5.png)

图3.5：前列腺癌例子中所有可能的子集模型。在每个子集规模下显示了该规模下每个模型的残差平方和。

> Many of the other approaches that we discuss in this chapter are similar, in that they use the training data to produce a sequence of models varying in complexity and indexed by a single parameter. In the next section we use cross-validation to estimate prediction error and select k; the AIC criterion is a popular alternative. We defer more detailed discussion of these and other approaches to Chapter 7.

本章中我们讨论的许多方法都是相似的，因为它们使用训练数据去得到区别于复杂度和由单参数编码的模型序列。下一节我们采用交叉验证去估计预测误差并选择$k$；AIC准则是一个受欢迎的选择。我们将更多的细节讨论和气体的方法推迟到第7章讨论。

#### 3.3.2 向前和向后逐步选择

> Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them. Forwardstepwise selection starts with the intercept, and then sequentially adds into the model the predictor that most improves the fit. With many candidate predictors, this might seem like a lot of computation; however, clever updating algorithms can exploit the QR decomposition for the current fit to rapidly establish the next candidate (Exercise 3.9). Like best-subset regression, forward stepwise produces a sequence of models indexed by k, the subset size, which must be determined.

与其搜索所有可能的子集（当$p$大于40不可行），我们可以利用它们寻找一个很好的途径。向前逐步选择从截距开始，然后向模型中依次添加最大程度提升拟合效果的预测变量。有许多备选预测变量时需要大量的计算；然而，聪明地更新算法可以由QR分解从当前拟合快速建立下一步的备选预测变量（练习3.9）。类似最优子集回归，向前逐步产生由$k$编码的模型序列，$k$为子集规模，必须确定$k$值。

> Forward-stepwise selection is a greedy algorithm, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection. However, there are several reasons why it might be
> preferred:

向前逐步选择是贪心算法，产生一个嵌套的模型序列。从这点来看与最优子集选择相比似乎是次优的。然而，有许多原因显示向前逐步可能是更好的：

> - Computational; for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence (even when $p >> N$).
> - Statistical; a price is paid in variance for selecting the best subset
>   of each size; forward stepwise is a more constrained search, and will
>   have lower variance, but perhaps more bias.

- 计算（computational）；对于大的$p$值，我们不能计算最优子集序列，但是我们总是可以计算向前逐步序列（即使$p>>N$）
- 统计（statistical）；在每个规模下选择最优子集需要在方差上付出代价；向前逐步是一种有更多约束的搜索，而且将会有更低的方差，但是可能更大的偏差。

> Backward-stepwise selection starts with the full model, and sequentially deletes the predictor that has the least impact on the fit. The candidate for dropping is the variable with the smallest Z-score (Exercise 3.10). Backward selection can only be used when N > p, while forward stepwise can always be used.

向后逐步选择从整个模型开始，并且逐步删掉对拟合影响最低的预测变量。要删掉的候选变量是Z分数最低的变量（练习3.10）。向后只能用于$N>p$时，而向前逐步总是可以使用。

> Figure 3.6 shows the results of a small simulation study to compare
> best-subset regression with the simpler alternatives forward and backward selection. Their performance is very similar, as is often the case. Included in the figure is forward stagewise regression (next section), which takes longer to reach minimum error.

图3.6展示了一个用于比较最优子集回归和简单的向前向后选择的小型仿真研究的结果。它们的表现非常相似，as is often the case。图中也包含了向前Stagewise回归（下一节），它需要更长时间达到最小误差。

![](fig3.6.png)
图3.6 在一个仿真的线性回归问题$Y=X^T\beta+\varepsilon$中四种子集选择的比较。在$p=31$个标准高斯变量有$N=300$个观测，pairwise相关系数都等于0.85。其中10个变量的系数是从$N(0,0.4)$分布中随机选取的，其它为0。噪声$\varepsilon \sim N(0,6.25)$,信噪比为0.64。结果取自50次仿真的平均值。图中展示了每一步系数$\hat{\beta}(k)$估计值与真值$\beta$的均方误差

> On the prostate cancer example, best-subset, forward and backward selection all gave exactly the same sequence of terms.

在前列腺癌例子中，最优子集、向前和向后选择都给出了完全相同的项的序列。

> Some software packages implement hybrid stepwise-selection strategies that consider both forward and backward moves at each step, and select the “best” of the two. For example in the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step an add or drop will be performed that minimizes the AIC score.

一些软件包实现混合的逐步选择策略，在每一步同时考虑向前和向后的移动，燃尽选择两者中最好的一个。举个例子，在R包中step函数使用AIC准则来加权选择，合理考虑到拟合参数的个数；在每一步执行添加或删除来最小化AIC分数。

> Other more traditional packages base the selection on F-statistics, adding “significant” terms, and dropping “non-significant” terms. These are out of fashion, since they do not take proper account of the multiple testing issues. It is also tempting after a model search to print out a summary of the chosen model, such as in Table 3.2; however, the standard errors are not valid, since they do not account for the search process. The bootstrap (Section 8.2) can be useful in such settings.

其它传统的包中的选择基于$F$统计量，加入“显著性”的项，然后删掉“非显著性”的项。这些不再流行，因为它们没有合理考虑到多重检验的问题。模型搜索后打印出所选择的模型的摘要是很吸引人的，如表3.2所示；然而，标准误差不是有效的，因为它们不考虑搜索的过程。自助法（8.2节）在这些设定下是有用的。

> Finally, we note that often variables come in groups (such as the dummy variables that code a multi-level categorical predictor). Smart stepwise procedures (such as step in R) will add or drop whole groups at a time, taking proper account of their degrees-of-freedom.

最后，我们注意到变量经常成群出现（比如用来编码多层次类别型预测变量的虚拟变量）。智能逐步过程（比如R中的step函数）会合理考虑到它们的自由度会一次添加或删除整个群体。

#### 3.3.3 向前Q[^Stagewise]回归

> Forward-stagewise regression (FS) is even more constrained than forward-stepwise regression. It starts like forward-stepwise regression, with an intercept equal to $\bar{y}$, and centered predictors with coefficients initially all 0. At each step the algorithm identifies the variable most correlated with the current residual. It then computes the simple linear regression coefficient of the residual on this chosen variable, and then adds it to the current coefficient for that variable. This is continued till none of the variables have correlation with the residuals—i.e. the least-squares fit when $N > p$.

向前Stage回归比向前逐步回归有更多限制。开始类似向前逐步回归，由等于$\bar{y}$的截距开始，位于中心的预测变量系数都初始化为0。每一步算法鉴别出与当前残差最相关的变量。然后计算所选择变量的残差的简单线性回归系数，并且添加到该变量的当前系数。这个过程一直继续直到没有变量与残差有相关性——i.e. 当$N>p$时的最小二乘拟合。

> Unlike forward-stepwise regression, none of the other variables are adjusted when a term is added to the model. As a consequence, forward stagewise can take many more than $p$ steps to reach the least squares fit, and historically has been dismissed as being inefficient. It turns out that this “slow fitting” can pay dividends in high-dimensional problems. We see in Section 3.8.1 that both forward stagewise and a variant which is slowed down even further are quite competitive, especially in very high-dimensional problems.

不同于向前逐步回归，当一项添加到模型中没有其他的变量需要调整。结果是，向前stage可以采取多余$p$步达到最小二乘拟合，当变得无效时删掉historically has been dismissed as being inefficient。结果是这种“慢拟合”pay dividends高纬问题中。我们在3.8.1节看到向前stage和变体which is slowed down even further are quite competitive, especially in very high-dimensional problems.

> Forward-stagewise regression is included in Figure 3.6. In this example it takes over 1000 steps to get all the correlations below $10^{−4}$. For subset size $k$, we plotted the error for the last step for which there where $k$ nonzero coefficients. Although it catches up with the best fit, it takes longer to do so.

向前stage回归包含在图3.6中。在这个例子中花了1000步使得所有相关系数低于$10^{-4}$。对于规模为$k$的子集，我们画出最后一步的误差，此时有$k$个非零系数。尽管catch up with，但需要花更长的时间。

#### 3.3.4 前列腺癌例子（继续）

> Table 3.3 shows the coefficients from a number of different selection and shrinkage methods. They are best-subset selection using an all-subsets search, ridge regression, the lasso, principal components regression and partial least squares. Each method has a complexity parameter, and this was chosen to minimize an estimate of prediction error based on tenfold cross-validation; full details are given in Section 7.10. Briefly, cross-validation works by dividing the training data randomly into ten equal parts. The learning method is fit—for a range of values of the complexity parameter—to nine-tenths of the data, and the prediction error is computed on the remaining one-tenth. This is done in turn for each one-tenth of the data, and the ten prediction error estimates are averaged. From this we obtain an estimated prediction error curve as a function of the complexity parameter.

表3.3展示了一系列不同选择和收缩方法的系数。它们是使用所有子集搜索的最优子集选择，ridge 回归，lasso，主成分回归和最小二偏差。每种方法有一个复杂度参数，并且基于10折交叉验证最小化预测误差来选择模型；7.10节中给出了全部细节。简短地说，交叉验证通过将训练数据随机分成10等份。学习方法为拟合——对于复杂度参数的取值范围——数据的十分之九，然后对于剩下的十分之一的数据计算预测误差。依次对每个十分之一的数据进行上述计算，然后对10个预测误差的估计进行平均。从这里我们可以得到预测误差估计作为复杂度函数的曲线。

![](tab3.3.png)

> Note that we have already divided these data into a training set of size 67 and a test set of size 30. Cross-validation is applied to the training set, since selecting the shrinkage parameter is part of the training process. The test set is there to judge the performance of the selected model.

注意到我们已经把这些数据分成了规模为67的训练集和规模为30的测试集。交叉验证应用到训练集，因为选择收缩参数是训练过程的一部分。测试集是用来判断所选择的模型的表现。

> The estimated prediction error curves are shown in Figure 3.7. Many of the curves are very flat over large ranges near their minimum. Included are estimated standard error bands for each estimated error rate, based on the ten error estimates computed by cross-validation. We have used the “one-standard-error” rule—we pick the most parsimonious model within one standard error of the minimum (Section 7.10, page 244). Such a rule acknowledges the fact that the tradeoff curve is estimated with error, and hence takes a conservative approach.

图3.7展示了估计的预测误差曲线。在它们最小值附近的大范围内许多曲线都是非常平坦的。基于由交叉验证计算得到的十个误差估计，每个估计的误差率Included are estimated standard error bands for each estimated error rate, based on the ten error estimates computed by cross-validation. 我们已经使用“一个标准误差”规则——在最小值的一个标准误差范围内我们选取最“吝啬”的模型（p244,7.10节）。这个规则承认权衡曲线估计存在误差这一事实，并且因此采取一个保守的方式。

> Best-subset selection chose to use the two predictors lcvol and lcweight. The last two lines of the table give the average prediction error (and its estimated standard error) over the test set.

最优集的选择决定使用两个预测变量lcvol和lcweight。表格的最后两行给出了测试集上预测误差的平均值（和它的标准误差估计）

![](../img/03/fig3.7.png)
图3.7 不同选择和收缩方法的预测误差的估计值曲线和它们的标准误差。每条曲线绘制成关于该方法对应的复杂度参数的函数。选定水平坐标轴则当我们从左侧移动到右侧模型复杂度增加。预测误差的估计和它们的标准误差由10着交叉验证得到；全部的细节在7.10节给出。在一个标准误差范围内的复杂度最低的模型选定，用粉红色的垂直虚线表示。
