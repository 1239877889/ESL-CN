# 4.1 导言

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-12-06                               |

在这章我们重新回到分类问题并且集中在分类的线性方法上。因为我们的预测变量$G(x)$是在离散集$\cal G$中取值，我们总是可以把输入空间分成一些区域的集合，这些区域根据分类来打上标签。我们在第2章看到这些区域的边界依赖于预测函数，边界会变得粗糙或者光滑。对于这些过程的一个很重要的类，这些判别边界（decision boundaries）都是线性的；这也是我们说的分类的线性方法。

线性判别边界可以通过许多不同的方式找到。在第二章中，我们根据类别指示变量拟合线性回归模型，根据最大的拟合来分类。假设有$K$个类别，为了方便记为$1,2,\ldots,K$,第$k$个响应预测变量的拟合好的线性模型为$\hat f_k(x)=\hat\beta_{k0}+\hat\beta_k^Tx$.第$k$类和第$l$类的判别边界是满足$\hat f_k(x)=\hat f_\ell(x)$的点的集合，也就是集合$\{x:(\hat\beta_{k0}-\hat\beta_{\ell 0})+(\hat\beta_k-\hat\beta_\ell)^Tx=0\}$,这是一个仿射集或者超平面（**作者注：严格来说，超平面需要通过原点，而仿射集不需要。我们有时忽略它们之间的区别并且一般指超平面**）。对于任意两个类别这也是正确的，输入空间被分段超平面判别边界分成了常值分类的区域。回归方法是对每个类别建立判别函数$\delta_k(x)$方法中的一个，然后将$x$分到有最大值的判别函数上。建立后验概率$Pr(G=k\mid X=x)$也是属于这种类别。说的更清楚点，只要$\delta_k(x)$或者$Pr(G=k\mid X=x)$关于$x$是线性的，则判别边界将也是线性的。

实际上，我们所有的要求是$\delta_k$或者$Pr(G=k\mid X=x)$的一些单调变换关于判别边界是线性的。举个例子，如果这里有两个类，一个关于后验概率很流行的模型是
$$
\begin{array}{ll}
Pr(G=1\mid X=x)&=\dfrac{exp(\beta_0+\beta^Tx)}{1+exp(\beta_0+\beta^Tx)}\\
Pr(G=2\mid X=x)&=\dfrac{1}{1+exp(\beta_0+\beta^Tx)}
\end{array}
\qquad \qquad(4.1)
$$

这里单调变换是logit变换：$log[p/(1-p)]$，事实上我们看到
$$
log\dfrac{Pr(G=1\mid X=x)}{Pr(G=2\mid X=x)}=\beta_0+\beta^Tx\qquad\qquad (4.2)
$$
判别边界是*log-odds*等于0的点的几何，这是由$\{x\mid\beta_0+\beta^Tx=0\}$定义的超平面。我们讨论两种非常流行但是不同的方法，都是线性的log-odds或者logits：线性判别分析和线性logistic回归。尽管他们推导不相同，但是他们关键的区别在于线性函数根据训练数据拟合的方式。

一个清晰地在类别之间建立边界的更加直接的方法便是线性的。对于在$p$维输入空间的两个类别的问题，这意味着把超平面作为判别边界来界定——换句话说，一个正态向量和一个分隔点。我们将清楚地看到两种方法都是寻找“分隔超平面”。第一个是Rosenblatt(1958)的感知器（*perceptron*）模型，它由一个从训练数据中寻找分离超平面的算法，如果存在超平面的话。第二个方法要归功于Vapnik(1996)，寻找一个最优分离超平面如果超平面存在的话，不存在的话则寻找一个超平面使得训练数据中的重叠最小。我们这里处理离散的情形，非离散的情形将推到第12章讨论。

尽管整个章节都在谈论线性判别边界，考虑一般倾向也是可以的。举个例子，我们可以将我们的变量集$X_1,X_2,\ldots,X_p$ 通过加上他们的交叉积$X_1^2,X_2^2,\ldots,X_1X_2,\ldots,$ 因此加入了$p(p+1)/2$ 个额外的变量。在增广空间的线性函数投影到原空间的线性函数——因此从线性判别边界转换为二次判别边界。图4.1描述了这一想法。数据集是一样的，左边在二维空间中用线性判别函数，而右边图在增广的五维空间中用线性判别函数。这种方式可以跟任意基变换$h(X)$一起使用，其中$h:R^p\rightarrow R^q,q>p$,这将在后面章节中讨论。

 ![](../img/04/fig4.1.png)

图4.1：左图显示了三个类别的一些数据，以及通过线性判别分析得到的线性判别边界。右图显示了二次判别边界。这是通过在五维空间$X_1,X_2,X_1X_2,X_1^2,X_2^2$找到线性边界得到。这个空间里面的线性不等式是原空间的二次不等式。

