# 4.1 导言

在这章我们重新回到分类问题并且注重在分类的线性方法。因为我们的预测变量$G(x)$是在离散集$\cal G$中取值，我们总是可以把输入空间分成一些区域的集合，这些区域根据分类来打上标签。我们在第2章看到这些区域的边界依赖于预测函数，边界会变得粗糙或者光滑。对于这些过程的一个很重要的类，这些判别边界（decision boundaries）都是线性的；这也是我们说的分类的线性方法。

线性判别边界可以通过许多不同的方式找到。在第二章中，我们根据类别指示变量拟合线性回归模型，根据最大的拟合来分类。假设有$K$个类别，为了方便记为$1,2,\ldots,K$,第$k$个响应预测变量的拟合好的线性模型为$\hat f_k(x)=\hat\beta_{k0}+\hat\beta_k^Tx$.第$k$类和第$l$类的判别边界是满足$\hat f_k(x)=\hat f_\ell(x)$的点的集合，也就是集合$\{x:(\hat\beta_{k0}-\hat\beta_{\ell 0})+(\hat\beta_k-\hat\beta_\ell)^Tx=0\}$,这是一个仿射集或者超平面（**作者注：严格来说，超平面需要通过原点，而仿射集不需要。我们有时忽略它们之间的区别并且一般指超平面**）。对于任意两个类别这也是正确的，输入空间被分段超平面判别边界分成了常值分类的区域。回归方法是对每个类别建立判别函数$\delta_k(x)$方法中的一个，然后将$x$分到有最大值的判别函数上。建立后验概率$Pr(G=k\mid X=x)$也是属于这种类别。说的更清楚点，只要$\delta_k(x)$或者$Pr(G=k\mid X=x)$关于$x$是线性的，则判别边界将也是线性的。

实际上，我们所有的要求是$\delta_k$或者$Pr(G=k\mid X=x)$的一些单调变换关于判别边界是线性的。举个例子，如果这里有两个类，一个关于后验概率很流行的模型是
$$
\begin{array}{ll}
Pr(G=1\mid X=x)&=\dfrac{exp(\beta_0+\beta^Tx)}{1+exp(\beta_0+\beta^Tx)}\\
Pr(G=2\mid X=x)&=\dfrac{1}{1+exp(\beta_0+\beta^Tx)}
\end{array}
\qquad \qquad(4.1)
$$

这里单调变换是logit变换：$log[p/(1-p)]$，事实上我们看到
$$
log\dfrac{Pr(G=1\mid X=x)}{Pr(G=2\mid X=x)}=\beta_0+\beta^Tx\qquad\qquad (4.2)
$$
判别边界是*log-odds*等于0的点的几何，这是由$\{x\mid\beta_0+\beta^Tx=0\}$的超平面。我们讨论两种非常流行但是不同的方法，都是线性的log-odds或者logits：线性判别分析和线性logistic回归。尽管他们推导不相同，但是他们关键的区别在于线性函数根据训练数据拟合的方式。

清晰地在类别之间建立边界的更加直接的方法便是线性的。对于在$p$维输入空间的两个类别的问题，这意味着把超平面作为判别边界来界定——换句话说，一个正态向量和一个分隔点。我们将清楚地看到两种方法都是寻找“分隔超平面”。第一个是Rosenblatt(1958)的感知器（*perceptron*）模型，它由一个从训练数据中寻找分离超平面的算法，如果存在超平面的话。第二个方法要归功于Vapnik(1996)，寻找一个最优分离超平面如果超平面存在的话，不存在的话则寻找一个超平面使得训练数据中的重叠最小。我们这里处理离散的情形，非离散的情形将推到第12章讨论。

