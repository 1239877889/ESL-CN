# 4.4 逻辑斯蒂回归

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-12-09:2016-12-10                    |

逻辑斯蒂回归来源于通过关于$x$的线性函数来建立$K$个类后验概率的模型的需要，同时保证它们的和为1且每一个都在$[0,1]$范围内。模型有如下形式
$$
\begin{array}{ll}
log\dfrac{Pr(G=1\mid X=x)}{Pr(G=K\mid X=x)}&=\beta_{10}+\beta_1^Tx\\
log\dfrac{Pr(G=2\mid X=x)}{Pr(G=K\mid X=x)}&=\beta_{20}+\beta_2^Tx\\
&\ldots\\
log\dfrac{Pr(G=K-1\mid X=x)}{Pr(G=K\mid X=x)}&=\beta_{(K-1)0}+\beta_{K-1}^Tx\\
\end{array}
\qquad (4.17)
$$
模型由$K-1$个log-odds或logit变换来确定（反映了概率相加和为1的约束）。虽然模型采用最后一类来作为odds-rations的分母，分母的选择其实是任意的，因为在这个选择下估计是相等的。简单地计算得到
$$
\begin{array}{ll}
Pr(G=k\mid X=x)=\dfrac{exp(\beta_{k0}+\beta_k^Tx)}{1+\sum\limits_{\ell=1}^{K-1}exp(\beta_{\ell0+\beta_\ell^Tx})},& k=1,\ldots, K-1\\
Pr(G=K\mid X=x)=\dfrac{1}{1+\sum\limits_{\ell=1}^{K-1}exp(\beta_{\ell0+\beta_\ell^Tx})}&
\end{array}
\qquad (4.18)
$$
显然它们相加等于1。为了强调对参数集$\theta=\{\beta_{10},\beta_1^T,\ldots,\beta_{(K-1)0},\beta_{K-1}^T\}$的依赖，我们将概率记为$p_k(x,\theta)$.

当$K=2$时，模型非常简单，因为只有一个单线性函数。在生物统计应用中应用很广，因为经常会有二进制（两个类别）的响应变量。举个例子，病人获救或死亡，患心脏病和不患心脏病，或者某个条件存在与否。

## 4.4.1 拟合逻辑斯蒂回归模型

逻辑斯蒂回归经常通过极大似然法，采用在给定$X$的条件下$G$的条件概率。因为$Pr(G\mid X)$完全明确了条件分布，多元正态是合适的选择。$N$个观测的概率密度的对数为
$$
\ell(\theta)=\sum\limits_{i=1}^Nlog\,p_{g_i}(x_i;\theta)\qquad (4.19)
$$
其中，$p_k(x_i;\theta)=Pr(G=k\mid X=x_i;\theta)$

我们将线性讨论两个类别的情形，因为算法可以相当简化。我们可以用0/1来对两个类别$g_i$的响应变量$y_i$进行编码，当$g_i=1$时$y_i=1$,当$g_i=2$时$y_i=0$.令$p_1(x;\theta)=p(x,\theta)$,$p_2(x;\theta)=1-p(x;\theta)$,概率的对数为
$$
\begin{array}{lll}
\ell(\beta)&=&\sum\limits_{i=1}^N\{y_ilogp(x_i;\beta)+(1-y_i)log(1-p(x_i;\beta))\}\\
&=&\sum\limits_{i=1}^N\{y_i\beta^Tx_i-log(1+e^{\beta^Tx_i})\}
\end{array}
\qquad (4.20)
$$
这里$\beta=\{\beta_{10},\beta_1\}$,而且我们假设输入向量$x_i$包含表示截距的项1.

为了最大化概率的对数，我们令微分为0，得到
$$
\dfrac{\partial \ell(\beta)}{\partial \beta}=\sum\limits_{i=1}^Nx_i(y_i-p(x_i;\beta))=0,\qquad (4.21)
$$
这是关于$\beta$的$p+1$个非线性等式。注意到因为$x_i$的第一个组分为1,第一个得分等式为$\sum_{i=1}^Ny_i=\sum_{i=1}^Np(x_i;\beta)$;期望的类别数与观测的个数一致（因此有两个类别）

为了求解（4.21）的得分等式，我们采用Newton-Raphson算法，需要Hessian矩阵
$$
\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}=\sum\limits_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta))\qquad (4.22)
$$
以$\beta^{old}$开始，新的Newton更新为
$$
\beta^{new}=\beta^{old}-(\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T})^{-1}\dfrac{\partial \ell(\beta)}{\partial(\beta)}\qquad (4.23)
$$
其中微分值由$\beta^{old}$处的值得到。

把得分和Hessian写成矩阵形式是很方便的。记$\mathbf y$为$y_i$的值，$\mathbf X$是值为$x_i$的$N\times (p+1)$矩阵，$\mathbf p$是拟合概率的向量且第$i$个元素为$p(x_i;\beta^{odd})$，$\mathbf W$是第$i$个对角元为$p(x_i;\beta^{odd})(1-p(x_i;\beta^{odd}))$的$N\times N$的对角矩阵。则我们有
$$
\begin{array}{ll}
\dfrac{\partial \ell(\beta)}{\partial \beta}&=\mathbf{X^T(y-p)}\qquad (4.24)\\
\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}&=-\mathbf{X^TWX}\qquad (4.25)
\end{array}
$$
牛顿迭代为
$$
\begin{array}{ll}
\beta^{new}&=\beta^{old}+\mathbf{(X^TWX)^{-1}X^T(y-p)}\\
&=\mathbf {(X^TWX)^{-1}X^TW(X\beta^{old}+W^{-1}(y-p))}\\
&=\mathbf{(X^TWX)^{-1}X^TWz}\qquad (4.26)
\end{array}
$$

在第二和第三行我们已经重新把牛顿迭代表达成最小二乘迭代，响应变量为
$$
\mathbf z=\mathbf X\beta^{old}+\mathbf{W^{-1}(y-p)}\qquad (4.27)
$$
有时也被称作调整后的响应变量。这些方程重复地进行求解，每一次迭代$\mathbf p$改变，因此$\mathbf W$和$\mathbf z$也改变。这个算法被称作是加权迭代最小二乘或者IRILS，因为每次迭代求解加权最小二乘问题：
$$
\beta^{new}\leftarrow arg\,\underset{\beta}{min}(\mathbf z-\mathbf X\beta)^T\mathbf W(\mathbf z-\mathbf X\beta)\qquad (4.28)
$$
似乎$\beta=0$是迭代过程一个很好的初始值，尽管收敛性不会保证。一般地算法确实是收敛的，因为概率的对数是凹的，但是可以出现过收敛的情况。如果在罕见的情形下概率的对数值下降，步长二分法会保证收敛性。

对于多类别的情况$K\ge 3$,牛顿算法也可以表达成加权迭代最小二乘，只是对于每一个观测用一个$K-1$的响应向量和非对角系数矩阵。后者阻碍了任何简化算法，在这种情况下直接使用展开向量$\theta$数值上会更加方便（练习4.4）.另一种是坐标下降方法（3.8.6节）也可以有效地最大化概率的log值。R语言的glmnet包（Friedman等人2008a）可以有效地拟合$N$和$p$都非常大的逻辑斯蒂回归。尽管是为了拟合正规化模型设计的，但是对于非正规化的拟合也适用。

逻辑斯蒂回归经常被用作一种数据分析和推断的工具，目标就是理解在解释输出时输入变量的角色。一般地，许多模型是通过寻找涉及变量的最简介的模型，很可能还有一些交叉项。下面的例子说明了涉及到的一些问题。

## 4.4.2 例子：南非心脏病

这里我们展示一个用二值数据分析去说明逻辑斯蒂回归的传统统计学应用。图4.12中的数据取自CORIS调查的部分数据，这个调查在南非Western Cape的三个乡村进行（Rousseauw等人1983）.调查的目标是建立在高发生率的地区的缺血性心脏病影响因子的强度。数据中有15到64岁的白人男性，响应变量是MI的存在或者缺失（该地区整个MI流行程度为5.1%）。在我们数据集中有160个案例，以及302个控制变量。数据在Hastie和Tibshirani（1987）的工作中有详细描述。

我们通过极大似然法拟合了逻辑斯蒂回归模型，给出了如表4.2所示的结果。结果概要包含了模型中每一个系数的$Z$分数（系数出一他们的标准误差）；不重要的$Z$分数表明该系数可以从模型中剔除。这对应着检验该系数为0而其它系数不为0的零假设（也被称作Wald检验）。$Z$分数的绝对值大于2表明在5%的水平下是显著的。

在系数表中有一些比较奇怪的结果，这必须认真对待。收缩压sbp竟然不显著！肥胖（obesity）也不显著，它的符号是负的。这个混乱是因为预测变量之间的相关关系。单独地来看，sbp和obesity都是显著的，而且都是正号。然而，当有其它相关变量时它们便不再需要了（甚至可以得到一个负号）。

这时分析者可能会做一些模型选择；寻找能够充分解释他们在chd上联合影响的子集变量。一种方式是删掉显著性最低的系数然后重新拟合模型。这个可以重复做下去知道没有更多的项可以从模型中剔除。这样得到了表4.3所示例的模型。

一个更好但是需要花费更多时间的策略是对每一个剔除一个变量后的模型进行重新拟合，然后进行偏差分析（analysis of deviance）确定哪个变量需要剔除。拟合模型的残差是减去两倍的概率对数，两个模型的偏差是它们个体残差的差别（类似于平方和）这个策略给出了上面同样的最终结果。

举个例子，怎么解释tobacco的系数0.081（标准偏差为0.026）？tobacco是一生中使用的烟草总千克数，控制集中位数为1.0kg，而这个案例为4.1kg。因此每增加1kg的烟草使用量意味着冠状心脏病的几率为$exp(0.081)=1.084$(或者8.4%).结合标准误差我们得到95%的置信区间$exp(0.081\pm 2\times 0.026)=(1.03,1.14)$

我们将在第5章再次用到这些数据，我们将会看到一些变量会有非线性影响，当进行合适的建模后不会被剔除模型。

## 4.4.3 二次拟合和推断

极大似然法的参数估计$\hat\beta$满足一个自一致性关系：它们是加权最小二乘拟合的系数，其中响应变量为
$$
z_i=x_i^T\hat\beta+\dfrac{y_i-\hat p_i}{\hat p_i(1-\hat p_i)}\qquad (4.29)
$$
