# 13.4 自适应最近邻方法

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-08-29                               |


当在高维特征空间中做最近邻分类时，最近邻的点可以非常远，导致出偏差，并且降低了分类的效果。

为了定量化这一点，考虑在单位立方体$[-\frac{1}{2},\frac{1}{2}]^p$中均匀分布的数据点。令$R$为中心在原点的1-最近邻的半径。则

$$
\text{median(R)} = v_p^{-frac{1}{p}}(1-\frac{1}{2}^{1/N})^{1/p}\qquad (13.7)
$$

其中$v_pr^p$是在$p$维空间半径为$r$的球的体积。图13.12显示了不同的训练样本大小和维度下半径的中位数。我们看到半径的中位数很快达到0.5，也就是到立方体的边的距离。

![](../img/13/fig13.12.png)

这个问题我们可以怎么做呢？考虑图13.13的两个类别的情形。

![](../img/13/fig13.12.png)

图中有两个特征，并且用圆形区域画出了查询点的最近邻。最近邻分类的隐含假设是类别概率在邻域内近似为常值，也因此简单的平均会得到不错的估计。然而，在这个例子中，只变化了水平方向上的类别概率。如果知道这些，我们将邻居拉伸为长方形区域。这会降低估计的偏差，同时方差不变。

一般地，这导致了对最近邻分类中使用的度量的自适应更新，所以得到的邻域在类别不会改变太多的方向上进行了拉伸。在高维特征空间中，类别概率可能仅仅在一个低纬度的子空间中有所改变，因此自适应度量是很重要的优点。

TODO

## 例子

## 最近邻的全局维度降低
