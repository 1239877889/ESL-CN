# 随机森林的定义

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-05-17                               |
|更新| 2018-01-02|

bagging的核心思想是对许多有噪声但近似无偏的模型进行平均，也因此降低了方差。树是用于bagging的理想模型，因为它们可以捕捉数据中复杂的交叉项，而且如果树长得足够深的话，会有相对较低的偏差。因为树是充满噪声的，所以通过平均可以很大程度地改善它们。而且，因为bagging中产生的每棵树是同分布的，$B$棵这样的树的平均的期望与它们中的任一个是相同的。这意味着经过bagged的树的偏差与单个(bootstrap)树的偏差是一样的，而且唯一改善的希望是通过降低方差。这与boosting是相反的，在boosting中树以一种自适应的方式减小偏差，因此不是同分布的。

$B$棵独立同分布、方差为$\sigma^2$的随机变量的平均的方差为$\frac{1}{B}\sigma^2$。如果变量是简单的同分布（不一定独立），有着正的成对相关系数$\rho$，平均的方差为（练习15.1）
$$
\rho\sigma^2+\frac{1-\rho}{B}\sigma^2 \qquad (15.1)
$$

!!! note "weiya注"
    已知$X_i,i=1,\ldots, B$独立同分布，方差均为$Var(X_i)=\sigma^2$。则
    $$
    Var(\frac 1B \sum X_i)=\frac{1}{B^2}Var(\sum X_i)
    $$
    而
    $$
    \begin{align}
    Var(\sum X_i)&=E(\sum X_i-E(\sum X_i))^2\\
    &=E(\sum(X_i-E X_i))^2\\
    &=\sum E(X_i-EX_i)^2+2\sum\limits_{1\le i<j\le B}E(X_i-EX_i)(X_j-EX_j)\\
    &=B\sigma^2+B(B-1)\rho\sigma^2
    \end{align}
    $$
    则
    $$
    Var(\frac 1B \sum X_i)=\rho\sigma^2+\frac{1-\rho}{B}\sigma^2
    $$
    在Ex 15.1中，额外问了为什么在$\rho$小于0的情形不成立。其实我们只要从
    $$
    Var(\sum  X_i)=B\sigma^2+B(B-1)\rho\sigma^2\ge 0
    $$
    得到
    $$
    \rho\ge \frac{-1}{B-1}
    $$
    当$B\rightarrow \infty$，有$\rho\ge 0$。

当$B$增大时，第二项消失，但是第一项还在，而且成对bagged的树的相关系数的大小限制了平均的好处。随机森林的想法（算法15.1）是通过降低树之间的相关系数来改善方差的降低，而不使方差增长过大。这可以通过在生成树的过程中对输入变量进行随机选择来实现。

![](../img/15/alg15.1.png)

特别地，当在一个bootstrapped数据集中生成树时：

> 在每次分割时，随机选择$m\le p$个输入变量作为候选变量用来分割

一般地，$m$取为$\sqrt{p}$，或者甚至小到取1。

生成$B$棵这样的树$\\{T(x;\Theta_b)\\}_1^B$，随机森林（回归）预测变量为
$$
\hat f_{rf}^B(x) = \frac{1}{B}\sum\limits_{b=1}^BT(x;\Theta_b)\qquad (15.2)
$$

如10.9节一样，$\Theta_b$根据分离变量、每个结点的分离值以及终止结点的值来表征第$b$个随机森林树。直观上，降低$m$会降低任意两棵树之间的相关系数，也因此通过（15.1）来降低平均的方差。

并非所有的估计都可以通过像这样震荡数据来改善。似乎强非线性估计，比如树，改善最大。对于其bootstrapped树，$\rho$一般很小（一般是0.05或更小；见图15.9），而$\sigma^2$不比原树的方差要大。另一方面，bagging不会改变线性估计，比如样本均值（因此也不会改变方差）；其bootstrapped均值之间的成对相关系数大约为50%。

!!! note "weiya注"
    bagging可以看成是特殊的随机森林，即$m=p$的随机森林。

随机森林很流行。Leo Breiman的合作者Adele Cutler维护一个随机森林的网站[NOT FOUND](http://www.math.usu.edu/∼adele/forests/)。R语言中有一个`randomForest`的包，由Andy Liaw维护，可以从`CRAN`网站上下载。

这些作者们声称随机森林取得了巨大的成功：“更精确”，“更有解释性”，以及类似这样的。在我们的经验中，随机森林确实做得很好，只需要非常少的调参。随机森林在`spam`测试集上取得了4.88%的误分类率，与其他方法相比表现得很好，没有显著性地比gradient boosting（4.5%）差。Bagging达到5.4%，显著性地比其他方法要差，所以这个例子中额外的随机会有帮助。

图15.1显示了这三个方法在2500棵树上测试误差的变化。在这种情形下，尽管10折交叉验证选择了所有的2500棵树，但有证据表明gradient boosting开始过拟合。

![](../img/15/fig15.1.png)

图15.2展现了在nested spheres问题上随机森林与gradient boosting模拟的结果。这里boosting很容易地比随机森林表现要好。注意点这里较小的$m$会更好，尽管部分是因为真实的判别边界是可加的。

!!! note "weiya注"
    nested spheres问题在[10.1 boosting方法](https://esl.hohoweiya.xyz/10%20Boosting%20and%20Additive%20Trees/10.1%20Boosting%20Methods/index.html)中定义，即
    特征$X_1,\ldots,X_{10}$是标准独立高斯分布，目标$Y$定义如下
    $$
    Y=
    \left\{
    \begin{array}{ll}
    1&\text{if } \sum_{j=1}^{10}X_j^2>\chi_{10}^2(0.5)\\
    -1 & \text{otherwise}
    \end{array}
    \right.
    \qquad (10.2)
    $$

![](../img/15/fig15.2.png)

图15.3比较了在一个回归问题中随机森林与（带收缩的）boosting，该问题采用加利福利亚住房数据（10.14.1节）。

![](../img/15/fig15.3.png)

表现出的两个很强的特点是

- 随机森林在大概200棵树时稳定了，而在1000棵树的时候boosting仍然在改善。boosting被收缩放慢了，同时也因为树更小的事实。
- 这里boosting比随机森林更好。在1000项的时候，弱boosting模型(深度为4的GBM)比更强的随机森林（RF $m=6$）有更小的误差；对均值绝对误差进行Wilcoxon检验得到$p$值为0.007。对于较大的$m$，随机森林不会表现得更好。
