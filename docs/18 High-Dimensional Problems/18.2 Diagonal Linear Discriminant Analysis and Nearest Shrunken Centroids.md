# 对角线性判别分析和最近的收缩重心

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-03-14:2017-03-14                    |

基因表达阵列是生物中一项重要的新的技术，并且在第1章和第14章有讨论。我们下一个例子的数据够早了2308个基因（列）和63个样本（行）的矩阵，从一系列微阵列实验中得到。每个表达值是对数比率$log(R/G)$。 $R$为目标样本中基因特异性RNA的数目，目标样本混合到微阵列上特定（基因特异性）的位点，并且$G$是参考样本中对应的RNA数目。从在孩子上发现的小、圆蓝色细胞肿瘤(SRBCT)得到样本，并且分成4个主要的类别：BL (Burkitt lymphoma), EWS (Ewing’s sarcoma), NB (neuroblastoma), 和RMS (rhabdomyosarcoma)。另外有包含20个观测的额外数据集。我们将不再讨论其背景。

因为$p>>N$，我们不可以对数据采用全线性判别分析(LDA)；需要一些种类的正则化。我们这里描述的方法类似在第4.3.1节中描述的方法，但是为了达到特征选择有更重要的改动。正则的最简单的形式假设特征在每一类中独立，也就是，类间协方差阵是对角阵。尽管事实上特征很少在一个类别中独立，当$p>>N$时，我们没有充足的数据来估计它们的依赖性。独立性的假设很大成都上降低了模型中参数的个数，并且经常导出更有效和更有解释性的分类器。

因此我们考虑对角协方差LDA准则来对类别分类。类别$k$的判别得分[p110的式(4.12)]是
$$
\delta_k(x^*)=-\sum\limits_{j=1}^p\frac{(x_j^*-\bar x_{kj})^2}{s_j^2}+2log\pi_k\qquad (18.2)
$$

这里$x*=(x_1^*,x_2^*,\ldots,x_p^*)^T$是测试观测的表达值的向量，$s_j$为第$j$个基因的混合类间标准误差，并且$\bar x_{kj}=\sum_{i\in C_k}x_{ij}/N_k$是类别$k$中基因$j$的 $N_k$的值的平均，其中$C_k$是类别$k$的索引集。我们称$\tilde x_k=(\bar x_{k1},\bar x_{k2},\ldots,\bar x_{kp})^T$为类别$k$的重心。(18.2)的第一部分是$x^*$到第$k$个类别重心的（负）标准平方距离。第二部分是基于类别先验概率$\pi_k$的矫正，其中$\sum_{k=1}^K\pi_k=1$。分类规则于是为

$$
C(x^*)=\ell\text{ if } \delta_{\ell}(x^*)=max_k\delta_k(x^*)\qquad (18.3)
$$

我们看到对角LDA分类器经过合适的标准化后等价于最近的重心分类器。这也是朴素贝叶斯分类器的一个特殊情形，正如在第6.6.3节中描述的那样。假设每个类别的特征有相同方差的独立高斯分布。

对角LDA分类器通常在高维设定中很有效。在Bickel和Levina(2004)等人的工作中称为独立规则，理论上证明经常在高维问题中比标准线性回归表现要好。

