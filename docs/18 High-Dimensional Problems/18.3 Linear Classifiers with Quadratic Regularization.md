# 

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-04-27:2017-04-27                    |

Ramaswamy等人(2001)提出一个更加困难的微阵列分类问题，涉及144个病人的14种癌症类型的训练集，以及54个病人的测试集。16063个基因的表达值已知。

表18.1显示了通过8个不同的分类方法得到的预测结果。每个病人的数据经过一次标准化后使得均值为0，方差为1；似乎提高了整个例子的预测正确性，表明每个基因表达谱的形状是很重要的，而不是表达的绝对水平。在每种情形中，正则参数的选取使得交叉验证误差最小，并且展现了每个参数值的测试误差。当有多余一个的正则参数得到最小的交叉验证误差，报告该值对应的测试误差的平均值。

RDA(正则判别分析)，正则多元逻辑斯底回归，以及支持向量机是更复杂的方法，试图研究数据的多变量信息。我们依次描述每个方法，以及一系列正则化方法，包括$L_1$和 $L_2$，以及两个都有的方法。

## 正则化判别分析

正则化判别分析(RDA)在4.3.1节已经描述。线性判别分析涉及$p\times p$类间协方差阵的逆。当$p>>N$，该矩阵可以很大，秩至多为$N<p$，因此是奇异矩阵。RDA克服了对类间协方差矩阵的估计$\hat\Sigma$正则化的问题。这里我们一个将$\hat\Sigma$收缩到其对角阵版本的RDA：
$$
\hat\Sigma(\gamma) = \gamma\hat\Sigma + (1-\gamma) diag(\hat\Sigma), \;\text{with}\; \gamma\in [0,1]\qquad (18.9)
$$

注意到$\gamma = 0$对应对角LDA，也是最近收缩重心的“无收缩”版本。(18.9)的收缩形式很像岭回归(3.4.1节)，将整个特征的协方差矩阵收缩到对角（标量值）矩阵。实际上，将线性判别分析看成是类别响应变量的最优得分的线性回归（见12.6节的(12.57)），这个等价性变得更加精确。

求大型$p\times p$矩阵逆的计算负担通过采用18.3.5节的方法可以克服。$\gamma$的值通过交叉验证取为表18.1中第二行的值；所有$\gamma\in(0.002,0.550)$给出了相同的CV和测试误差。RDA进一步的发展，除了协方差矩阵还包括重心的收缩，可以在Guo等人(2006)的工作中找到。

## 二次正则的逻辑斯底回归

逻辑斯底回归(4.4节)可以用相似的方式进行改进用来处理$p>>N$的情形。在$K$个类别的情形下，我们采用p119的多类别逻辑斯底回归模型(4.17):

$$
Pr(G=k\mid X=x)=\frac{exp(\beta_{k0}+x^T\beta_k)}{\sum_{\ell=1}^Kexp(\beta_{\ell0}+x^T\beta_\ell)}\qquad (18.10)
$$

这有$K$个对数几率参数$\beta_1,\beta_2,\ldots,\beta_K$的系数向量。我们通过最大化惩罚的对数似然来正则化拟合

$$
\underset{\{\beta_{0k},\beta_k\}_1^K}{max}[\sum\limits_{i=1}^NlogPr(g_i\mid x_i)-\frac{\lambda}{2}\sum\limits_{k=1}^K\Vert\beta_k\Vert]_2^2\qquad (18.11)
$$

正则化自动解决了参数化中的冗余，并且令$\sum_{k=1}^K\hat\beta_{kj}=0,j=1,\ldots,p$(练习18.3). 注意到常数项$\beta_{k0}$没有正则化（并且应该被设成0）。结果得到的优化问题是凸的，而且可以通过牛顿算法或者其他数值技巧求解。详细细节在Zhu和Hastie(2004)的工作中给出。Friedman等人(2010)提供了计算两个和多个类别逻辑斯底回归模型的正则化路径。表18.1，第6行报告了多重类别逻辑斯底回归模型的结果，称之为“多重”。可以证明(Rosset等人2004a)，对于可分数据，当$\lambda\rightarrow 0$，正则（两个类别）逻辑斯底回归估计（重正态）收敛到最大边界分类器(12.2节)。这给出了支持向量机的一个受欢迎的替代类型，特别是在多重类别的情形中，这将在下面讨论。

## 支持向量分类器

两个类别情形的支持向量机在12.2节中讨论。当$p>N$时，特别受欢迎因为一般情形下类别是可以被超平面完全可分的，除非在不同类别中有相同的特征向量。没有任何正则化，支持向量分类器寻找具有最大空白的分离超平面；也就是，超平面得到训练数据之间最大的差距。有点吃惊的是，当$p>>N$时未正则化支持向量机通常和最优正则版本表现得一样好。过拟合经常似乎不是一个问题，部分是因为误分类损失的不敏感。

有许多不同的方式将两个类别的支持向量分类器一般化为$K>2$个类别的情形。在一对一方法(ovo)中，我们计算所有的$\binom{K}{2}$成对分类器。对每个测试点，预测类别是在大多数成对比较中获胜的那个。在一对多方法(ova)中，每个类别与所有的其他类别在K次两类别比较中进行比较。为了对某个测试点分类，我们计算$K$个分类器中的每一个计算置信度（到超平面的符号距离）。胜利者是有最高置信度的类别。最后，Vapnik(1998)，Waston和Watins(1999)提出推广了两类别准则(12.7)的（有点复杂）多重类别准则。

Tibshirani和Hastie（2007）提出边际树(margin tree)分类器，这是支持向量分类器用在二叉树中，很像在CART（第9章）中一样。类别以系统形式进行组织，举个例子，对于将病人分类成不同的癌症类型是很有用的。

表18.1的第三行显示了支持向量机采用ova方法的结果，Ramaswamy等人(2001)报告了（并且我们证实了）这个方法对于这个问题表现得很好。误差与第6行的误差很相似，正如我们在上一节的最后做出的评论那样。误差率对$C$的选择不是很敏感[p420中式(12.8)的正则化参数]，对于值$C>0.001$.因为$p>N$，支持向量超平面可以通过设置$C=\infty$完美地分离训练数据。

## 当$p>>N$时的计算捷径

这一节中讨论的计算技巧应用到任何对参数二次正则化的线性模型拟合中。这包含这节中讨论的所有方法。当$p>N$时，通过在14.5节中介绍的奇异值分解方法，计算可以在$N$维空间中进行，而不是$p$。这里是几何上的直观：恰恰像三维空间中两点总是在一条直线上一样，在$p$维空间中的$N$个点位于$(N-1)$维仿射子空间中。

给定$N\times p$的数据矩阵$\mathbf X$，令

$$
\begin{align}
\mathbf{X & = UDV^T}\quad (18.12)\\
& = \mathbf{RV^T}\qquad (18.13)
\end{align}
$$

为$\mathbf X$的奇异值分解；也就是，$\mathbf V$是有着正交列的$p\times N$矩阵，$\mathbf U$是 $N\times N$正交矩阵，$\mathbf D$是元素为$d_1\ge d_2\ge d_N\ge 0$的对角矩阵。矩阵$\mathbf R$是 $N\times N$的，列为$r_i^T$.

作为一个简单的例子，我们首先考虑岭回归的估计
$$
\hat\beta = (\mathbf{X^TX}+\lambda\mathbf I)^{-1}\mathbf{X^Ty}\qquad (18.14)
$$
用$\mathbf RV^T$替换$\mathbf X$，并且进行一些运算，可以证明他它等于

$$
\hat\beta = \mathbf{V(R^TR+\lambda I)^{-1}\mathbf{R^Ty}}\qquad (18.15)
$$

（练习18.4）。因此$\hat\beta = \mathbf V\hat\theta$，其中$\hat\theta$是采用$N$个观测$(r_i,y_i),i=1,2,\ldots,N$的岭回归估计。换句话说，我们可以简单地讲数据矩阵从$\mathbf X$降维成$\mathbf R$，并且对$\mathbf R$的行进行操作。这个技巧将$p>N$时的计算花费从$O(p^3)$降为$O(pN^2)$

这些结果可以一般化为所有参数为线性并且有二次惩罚的模型。考虑任意的监督学习问题，我们采用线性函数$f(X)=\beta_0+X^T\beta$来建立条件分布$Y\mid X$的参数模型。我们通过在数据上最小化损失函数$\sum_{i=1}^NL(y_i,f(x_i))$和 $\beta$上的二次惩罚来拟合参数$\beta$。逻辑斯底回归是个很有用的例子。接着我们有下面简单的定理：

令$f^*(r_i)=\theta_0+r_i^T\theta$， $r_i$ 如(18.13)定义，并且考虑成对优化问题：
$$
\begin{align}
(\hat\beta_0,\hat\beta) &= arg\underset{\beta_0,\beta\in R^p}{min}\sum\limits_{i=1}^NL(y_i,\beta_0+x_i^T\beta)+\lambda\beta^T\beta\qquad (18.16)\\
(\hat\theta_0,\hat\theta) &= arg\underset{\theta_0,\theta\in R^N}{min}\sum\limits_{i=1}^NL(y_i,\theta_0+r_i^T\theta)+\lambda\theta^T\theta\qquad (18.17)
\end{align}
$$

于是有$\hat\beta_0=\hat\theta_0, \hat\beta=\mathbf V\hat\theta$.

这个定理说我们可以用$N$维的向量替换$p$维向量，并且像之前一样进行带惩罚的拟合，但是有更少的预测变量。$N$维向量的解$\hat\theta$接着通过简单的矩阵运算转化为$p$维向量的解。这个结果是部分的统计学传说，理应当被广泛知道——更多细节详见Hastie和Tibshirani(2004)等人的工作。

几何上，我们旋转特征到坐标系统中，使得除了前$N$个坐标都变成0。这个旋转是允许的，因为二次惩罚在旋转下是不变的，并且线性模型是等价的。

这个结果可以应用到本章中讨论的许多学习算法，比如正则化（多类别）逻辑斯底回归，线性判别分析（练习18.6），以及支持向量机。这也应用到有二次正则化的神经网络（11.5.2节）。然而，注意到，这并不应用到类似lasso的方法中，这些方法对系数加的是非平方惩罚（$L_1$）。

一般地，我们采用交叉验证去选择参数$\lambda$。可以看到（练习18.12）我们仅需要对原始数据构造一次$\mathbf R$，并且用这个作为CV折中的每个数据。

12.3.7节中的支持向量机的“核技巧”在不同的情形下利用了这节中的降维。假设我们有$N\times N$的（内积）矩阵$\mathbf{K=XX^T}$。由（18.12）我们有$\mathbf{K=UD^2U^T}$，所以$\mathbf K$捕捉了与$\mathbf R$一样的信息。练习18.13显示了我们怎样在这节中利用这个想法用$\mathbf K$和它的SVD分解来拟合岭逻辑斯底回归。




