# $L_1$正则的线性分类器

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-04-27:2017-04-27                    |


18.3节的方法使用$L_2$惩罚来对参数进行正则化，正如在岭回归中一样。所有的估计参数是非零的，因此没有应用特征选择。在这一节我们讨论采用$L_1$惩罚的正则，也因此提供了自动的特征选择。

!!! note "weiya 注"
	特征选择的效果就是取部分的特征，于是那部分的特征所对应的系数为0。

回忆3.4.2节的lasso，
$$
\underset{\beta}{min}\frac{1}{2}\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\vert\beta_j\vert\qquad (18.18)
$$

我们写成了(3.52)的拉格朗日格式。这里讨论的，使用$L_1$惩罚导致部分的解参数$\hat\beta_j$恰恰为0，对于充分大的调整参数$\lambda$而言。

在3.8.1节我们已经讨论了LARS算法，一个计算所有的$\lambda$的lasso解的有效过程。当$p>N$时（正如本章中），当$\lambda$趋于0，lasso恰巧拟合了训练数据。事实上，通过凸对偶，可以证明当$p>N$时，非零参数的个数对于所有$\lambda$至多为$N$（举个例子，Rosset和Zhu在2007年的工作）。因此lasso提供了特征选择的形式。

lasso回归可以通过对输出编码为$\pm 1$应用到两类别分类问题中，并且对预测应用一个截距（通常为0）。对于多余两个的类别，有许多可能的方式，包括在18.3.3节讨论的ova和ovo方法。我们在18.3节中对癌症数据应用ova方法。结果显示在表18.1的第4行中。它的效果是最好的。

分类问题的更自然的方式是使用lasso惩罚来正则化逻辑斯底回归。文献中已经提出一些实现方法，包括类似LARS(Park和Hastie，2007)的路径算法。因为路径是分段光滑但是非线性，精确的算法比LARS算法更慢，并且当$p$很大时不是很适用。

Friedman等人(2010)提供了用于拟合$L_1$惩罚的逻辑斯底和多项式回归模型。他们应用18.3.2节中(18.10)中的对称多项式逻辑斯底回归模型，并且最大化带惩罚的对数似然

$$
\underset{\{\beta_{0k},\beta_k\in R^p\}_1^K}{max}[\sum\limits_{i=1}^NlogPr(g_i\mid x_i)-\lambda\sum\limits_{k=1}^K\sum\limits_{j=1}^p\vert\beta_{kj}\vert]\qquad (18.19)
$$

这与式子(18.11)相对应。他们的算法通过坐标轮换(cyclical coordinate descent)（3.8.6节），并且利用$p>>N$时解是稀疏的事实，以及$\lambda$邻域的解趋向于非常相似。这个方法用在表18.1的第7行中，通过交叉验证来选取全局调整参数$\lambda$。它的表现与最优方法很相似，除了自动特征选择一起选择了269个基因。类似的方法在Genkin等人(2007)的工作中使用了；尽管它们从贝叶斯角度提出它们的模型，事实上他们计算后验概率，解决了惩罚的最大似然问题。

在基因应用中，变量间通常有强相关关系；基因趋向于在分子通路上起作用。lasso惩罚在某种程度上与强但相关的变量的集中的选择是不相关的（练习3.28）。另一方面，岭惩罚趋向于将相关变量的系数收缩（p99的练习3.29）。弹性网惩罚（Zou和Hastie,2005）是一种妥协的方式，并且有形式

$$
\sum\limits_{i=1}^p(\alpha\vert\beta_j\vert+(1-\alpha)\beta_j^2)\qquad (18.20)
$$

第二项鼓励高相关的特征进行平均，而第一项促进在平均特征系数中的稀疏解。弹性网惩罚可以用到任何线性模型中，特别是用于回归或分类中。

因此上述的带弹性网惩罚的多项式问题变成

$$
\underset{\{\beta_{0k,\beta_k\in R^p}\}_1^K}{max}[\sum\limits_{i=1}^NlogPr(g_i\mid x_i)-\lambda\sum\limits_{k=1}^K\sum\limits_{j=1}^p(\alpha\vert\beta_{kj}\vert)+(1-\alpha)\beta_{kj}^2]\qquad (18.21)
$$

参数$\alpha$确定了惩罚的混合，并且经常在qualitative grounds上预先选取。当$p>N$时，弹性网得到多于$N$个的非零参数，这是相比于lasso的潜在优点。表18.1的第8行采用这个模型，通过交叉验证选择$\alpha$和 $\lambda$。我们采用0.05到1.0之间的20个$\alpha$值，以及在整个值域的对数尺度下均匀分布的100个$\lambda$值。位于$\alpha\in [0.75,0.80]$
之间的值给出了最小的CV误差，对于所有的tied solution，$\lambda<0.001$。尽管在所有方法中有最低的测试误差，但是空白是小的并且不显著。有趣的是，当CV对每个$\alpha$单独计算，在$\alpha=0.10$时达到最小的测试误差，但是这不是在2维CV中选择的值。

![](../images/18/fig18.5.png)
> 白血病数据的正则化逻辑斯底回归路径。左图是lasso路径，右图是$\alpha=0.8$的弹性网路径。在路径的终端（最左端），lasso有19个非零系数，弹性网有39个非零系数。弹性网的平均效应导致比lasso更多的非零系数，但是规模更小。


图18.5显示了在两类别白血病lasso和弹性网的系数路径（Golub等人，1999）。有38个样本的7129个基因表达测量，其中的27个是类别ALL中（实际的淋巴白血病），11个是类别AML中（实际的骨髓白血病）。也有一个含34个样本(20,14)的测试集。因为数据是线性可分的，解在$\lambda=0$ 是未定义的（练习18.11），对于非常小的$\lambda$会退化。因此路径被拟合概率截断为0和1。左图中有19个非零系数，右图中有39个非零系数。图18.6（左图）显示了训练和测试数据上lasso逻辑斯底回归的误分类误差，同时也显示了训练数据的10折交叉验证。右图采用二项偏差来衡量误差，并且更加光滑。小的样本大小导致曲线中相当大的取样误差，即使个体曲线相对光滑（举个例子，见p220的图7.1）。两张图都表示极限解$\lambda \downarrow 0$是充分的，导致在测试集上3/34的误分类。弹性网的对应图片是数值相似，没有显示出来。

对于$p>>N$，系数的极限对于所有正则的逻辑斯底回归模型收敛，所以实际上软件实现中，$\lambda>0$要么是显式的要么是隐式的集合。然而，系数的重正态模型收敛，并且这些极限解可以看成是线性最优分类超平面(SVM)的一个有趣的替代。$\alpha=0$时的极限解与SVM是一致的（见18.3.2节的最后），但是选择所有的7129个基因。当$\alpha = 1$时，极限解与$L_1$的分离超平面是一致的（Rosset等人，2004a），并且至多包含38个基因。当$\alpha$从1降低，弹性网的解在分离超平面中包含更多的基因。

## 应用Lasso到蛋白质质谱

蛋白质质谱成为了一种分析血液中蛋白质的受欢迎的技巧，而且可以用作诊断疾病或者理解潜在的过程。

对于每个血液的血清样本$i$，我们对于许多时间点$t_j$观测到密度$x_{ij}$。这个密度与观测到

