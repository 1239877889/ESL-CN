# 14.5 主成分，主曲线和主曲面

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-11-01:2016-10-21                    |

主成分已经在3.4.1节中讨论了，主成分阐释了岭回归的收缩机理。主成分是一系列数据的投射，主成分是相互不相关的并且按照方差大小排序。在下面一部分我们将要把主成分表示成N个点$x_i\in R^p$的多重线性逼近。接着我们在14.5.2节讨论一些非线性正则化。最近一些其他的关于多重非线性逼近的方法将在14.9节讨论。

## 14.5.1 主成分

$R^p$中的数据的主成分给出了这些数据在秩$q\le p$下最好的线性逼近。

记观测值为$x_1,x_2,\ldots,x_N$,然后考虑用秩为$q$的线性模型来表示它们
$$
f(\lambda)=\mu+\mathbf V_q\lambda\qquad (14.49)
$$
其中，$\mu$是$R^p$中的位置矢量，$\mathbf V_q$为有$q$个正交单位向量作为其列的$p\times q$的矩阵，$\lambda$是一个$q$维的系数向量。这是一个秩为$q$的仿射超平面的系数表示。图14.20和图14.21分别图示了$q=1$和$q=2$的情形。通过对数据进行最小二乘拟合这个模型意味着最小化重建误差（reconstruction error）
$$
\underset{\mu,\{\lambda_i\},\mathbf V_q}{min}\sum\limits_{i=1}^N\Vert x_i-\mu-\mathbf V_q\lambda_i\Vert^2\qquad (14.50)
$$
我们可以对上式关于$\mu$和$\lambda_i$（练习14.7）求微分得到
$$
\begin{array}{lll}
\hat\mu&=&\bar x\qquad(14.51)\\
\hat\lambda_i&=&\mathbf V_q^T(x_i-\bar x)\qquad (14.52)
\end{array}
$$
