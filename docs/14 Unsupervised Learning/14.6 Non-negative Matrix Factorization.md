# 非负矩阵分解

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-09-03                   |

非负矩阵分解(Lee and Seung,1999)[^1]是最近提出来的一个用来替代主成分分析的方法，这个方法中数据以及成分假定为非负的。这对于建立非负数据的模型很有用，比如图象数据。

$N\times p$数据矩阵$\mathbf X$由下式近似
$$
\mathbf X \approx \mathbf W\mathbf H\qquad (14.72)
$$

其中$\mathbf W$是$N\times r$，$\mathbf H$是$r\times p, r\le max(N,p)$的。我们假设$x_{ij}, w_{ik}, h_{kj}\ge 0$。

通过最大化下式来确定$\mathbf W$ 和$\mathbf H$
$$
L(\mathbf W, \mathbf H)=\sum\limits_{i=1}^N\sum\limits_{j=1}^p[x_{ij}log(\mathbf {WH}_{ij}-（\mathbf{WH})_{ij})]\qquad (14.73)
$$

这是某模型的对数似然，模型中$x_{ij}$服从均值为$(\mathbf{WH}_{ij})$的Poisson分布——这个模型对于非负数据是合理的。

下面的替代算法(Lee and Seung, 2001)[^2]收敛到$L(\mathbf W, \mathbf H)$的局部最大值：

$$
\begin{array}{ll}
w_{ik}&\leftarrow w_{ik}\frac{\sum_{j=1}^ph_{kj}x_{ij}/(\mathbf{WH}_{ij})}{\sum_{j=1}^ph_{kj}}\\
h_{kj}&\leftarrow h_{kj}\frac{\sum_{j=1}^pw_{ik}x_{ij}/(\mathbf{WH}_{ij})}{\sum_{j=1}^pw_{ik}}
\end{array}\qquad (14.74)
$$

这个算法可以由最大化$L(\mathbf W, \mathbf H)$的最小过程来导出（练习14.23），并且与对数线性模型的迭代比例算法相关（练习14.24）。

TODO

## 典型分析

TODO


[^1]: Lee, D. and Seung, H. (1999). Learning the parts of objects by non-negative matrix factorization, Nature 401: 788.
[^2]: Lee, D. and Seung, H. (2001). Algorithms for non-negative matrix factorization, Advances in Neural Information Processing Systems, (NIPS 2001), Vol. 13, Morgan Kaufman, Denver., pp. 556–562.
