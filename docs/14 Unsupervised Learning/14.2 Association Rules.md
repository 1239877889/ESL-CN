# 关联规则

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-20:2017-02-20                    |

关联规则分析已经成为挖掘贸易数据的受欢迎的工具。目标是寻找变量$X=(X_1,X_2,\ldots,X_p)$在数据中出现最频繁的联合值。大部分应用在二值数据$X_j\in\{0,1\}$中，也称作“市场篮子”分析。这种情形下观测为销售交易，比如出现在商店收银台的东西。变量表示所有在商店中出售的东西。对于观测$i$，每个变量赋两个值中的一个；如果第$j$个物品作为该次交易购买东西的一部分则$x_{ij}=1$，而如果没有购买则$x_{ij}=0$。这些频繁有联合值的变量表示物品经常被一起购买。这个信息对于货架、跨营销的促销活动、商品目录的设计，以及基于购买模式的消费者划分都是很有用的。

更一般地，关联分析的基本目标是寻找特征向量$X$的原始$X$值$v_1,\ldots,v_L$ 的集合，使得概率密度 $Pr(v_l)$在这些值上的取值相对大。在一般框架下，这个问题可以看成是“模式寻找（mode finding）”或者“碰撞狩猎（bump hunting）”。如所阐释的，这个问题是不可能的困难。每个$Pr(v_l)$的自然估计是观测$X=v_l$时的分数。对于涉及多于少量变量的问题，每个变量可以假定多余少量的值，对于其中$X=v_l$的观测值的数目用于可靠估计几乎总是太小。

进行第一次简化来修改目标。与其寻找$Pr(x)$很大的$x$值（values），不如在$X$空间中寻找相对它们大小和支撑集的大概率部分的区域（regions）。令$\cal S_j$表示第$j$个变量的所有可能值的集合（支撑集），并且令$s_j\subseteq\cal S_j$为这些值的子集。修改后的目标可以叙述成试图寻找变量值的子集$s_1,\ldots,s_p$来使得每个变量的概率同时相对地大，假设每个子集中都有一个值，则也就是使得
$$
Pr\Big[\bigcap_{j=1}^p(X_j\in s_j)\Big]\qquad (14.2)
$$
相对大。子集的交$\cap_{j=1}^p(X_j\in s_j)$称作联合规则（conjunctive rule）。对于定性变量子集$s_j$为邻接区间；对于类别型变量子集是明确界定的。注意到如果子集实际上是整个集合$s_j=\cal S_j$，经常是这种情形，变量$X_j$被称为没有出现在规则（14.2）中。

## 市场篮子分析

求解（14.2）的一般方法将在14.2.5节讨论。这在许多应用中是很有用的。然而，它们对于非常多（$p\approx 10^4,N\approx 10^8$）的交易数据是不可行的，市场篮子分析经常应用到这些数据上。对（14.2）的进一步简化是需要的。首先，只考虑了两种类型的子集；$s_j$要么包含$X_j$的单个值$s_j=v_{0j}$，或者$X_j$的整个值的集合，$s_j=\cal S_j$。这将（14.2）简化为寻找元素为整数的子集$\cal J\subset\{1,\ldots,p\}$，以及对应的值$v_{0j},j\in\cal J$，使得

$$
Pr\Big[\bigcap_{j\in \cal J}(X_j=v_{0j})\Big]\qquad (14.3)
$$

相对大。图14.1阐释了这个假设。

![](../img/14/fig14.1.png)

> 图14.1. 对应规则的简化。这里有两个输入$X_1$和$X_2$，分别取4和6个不同的值。红色方块表示高密度的区域。为了简化计算，我们假设导出的子集要么对应输入的单个值，要么对应所有值。有了这个假设，我们可以找到图中中间或者右边的模式，而不是左边的模式。

可以应用虚拟变量（dummy variables）的技巧将（14.3）转换为只涉及二值变量的问题。这里我们假设支撑集 $\cal S_j$ 对于每个变量 $X_j$ 都是有限的。具体地，构造新的变量集$Z_1,\ldots,Z_K$，对于由每个原始变量$X_1,X_2,\ldots,X_p$可获得的值$v_{lj}$中的每一个，创建一个这样的变量。虚拟变量的数目$K$为
$$
K=\sum\limits_{j=1}^p\vert \cal S_j\vert\qquad 
$$
其中$\vert \cal S_j\vert$为从$X_j$得到的唯一值的个数。

!!! notes "weiya 注"
    每个$v_{lj}$（可看成是$p$维列向量）都有一个虚拟变量$Z_{\ell}$。
    ![](../img/14/pho14.1.png)

如果与其相关联的变量取$Z_k$对应的值，则每个虚拟变量被赋值为$Z_k=1$，否则$Z_k=0$（**weiya注：**见上面注中的图片）。 这将（14.3）转换为寻找整数集${\cal K}\subset\{1,\ldots,K\}$使得下式的值大。
$$
Pr\Big[\bigcap_{k\in\cal K}(Z_k=1)\Big]=Pr\Big[\prod\limits_{k\in\cal K}Z_k=1\Big]\qquad (14.4)
$$
这是标准的市场篮子问题的组成。集合$\cal K$称为“项目集（items set）”。在项目集中的变量$Z_k$的个数称为“大小（size）”（注意到这个大小不大于$p$）。（14.4）的估计值取在数据集中式（14.4）中关联为真的观测的分数：
$$
\widehat{Pr}\Big[\prod\limits_{k\in\cal K}(Z_k=1)\Big]=\frac{1}{N}\sum\limits_{i=1}^N\prod_{k\in\cal K}z_{ik}\qquad (14.5)
$$
这里$z_{ik}$为$Z_k$的第$i$种情形的值。这（式(14.5)??）称作项目集$\cal K$的“支持（support）”或“流行（prevalence）”$T(\cal K)$。$\prod_{k\in\cal k}z_{ik}=1$的观测$i$称作“包含（contain）”项目集$\cal K$中。

在关联规则挖掘中确定支撑的下界$t$，并且寻找所有可以由变量$Z_1,\ldots,Z_k$组成的项目集$\cal K_l$，并且支撑集大于$t$，也就是
$$
\{{\cal K_l}\mid T({\cal K_l})>t\}\qquad (14.6)
$$

## Apriori 算法

如果调整阈值$t$使得式（14.6）仅由所有$2^K$个可能项集合的一小部分组成，则可以通过用于非常大的数据库的可行计算来获得该问题（14.6）的解。“Apriori”算法（Argawal等人，1995）探究维数灾难的一些方面来用数据的小部分传递求解（14.6）。具体地，对于给定的支撑阈值$t$:

- $\{{\cal K}\mid T(\cal K)>t \}$的基数相对小。
- 任意项目集$\cal L$包含$\cal K$中项的子集必须有比$\cal K$大的支撑或者相等的支撑，${\cal L\subseteq K}\Rightarrow T({\cal L})\ge T({\cal K})$

第一次传递数据计算所有单项目集合的支撑。舍弃那些支撑小于阈值的项目集。第二次传递数据计算所有可以由第一次传递中保留下来的单项目集合组成对的大小为2的项目集合的支撑。换句话说，为了产生所有大小为$\cal K=m$的频繁项目集，我们仅仅需要考虑那些候选项目，那些候选项目使得所有大小为$m-1$的祖先项目得到的大小为$m$的项目集是频繁的。舍弃那些支撑小于阈值的大小为2的项目集。每个后继的传递数据只考虑了那些可以通过结合上一次传递数据存留的项目集与第一次传递数据保留下的项目集得到的项目集。数据传递过程一直进行下去，直到来自上一次传递的所有候选规则的支撑都小于指定阈值。（？？？一定会小？？）Apriori算法仅仅要求对每个$\vert\cal K\vert$的值的一侧数据传递，这是很重要的，因为我们假设数据不能放在计算机的主存中。如果数据是充分稀疏（或者如果阈值$t$充分高），则在合理次数之后终止过程，甚至对于非常大的数据集也是如此。

许多额外的技巧可以作为这个策略的一部分来提高速度和收敛（Agrawal等人，1995）。Apriori算法标志数据挖掘技术的主要进步。

通过Apriori算法返回的每个高支撑的数据集$\cal K$（14.6）被放到“关联规则”的集合中。项目$Z_k,k\in\cal K$被分成两个分离的子集，$A\cup B=\cal K$，并且写成
$$
A\Rightarrow B\qquad (14.7)
$$
第一项子集$A$被称作“先行者”，第二个子集$B$被称为“后果”。关联规则定义为有一些性质，基于在数据库中先行项目集和后果项目集的流行程度。规则$T(A\Rightarrow B)$的“支撑”是在先行项目集和后果项目集的并中的观测的分数，恰恰是引出它们的项目集$\cal K$的支撑。可以看成在随机选择的市场篮子中同时观测项目集$Pr(A\; and\; B)$的概率的估计（14.5）。该规则的“置信度（confidence）”或“可预测性（predictability）”$C(A\Rightarrow B)$是它的支撑除以先行者的支撑
$$
C(A\Rightarrow B)=\frac{T(A\Rightarrow B)}{T(A)}\qquad (14.8)
$$
可以看成是$Pr(B\mid A)$的估计。记号$Pr(A)$是在篮子中出现项目集$A$的概率，是$Pr(\prod_{k\in A}Z_k=1)$的缩写。“期望置信度”定义为后果的支撑$T(B)$，是没有条件的概率$Pr(B)$的估计。最后，规则的“lift”定义为置信度除以期望置信度
$$
L(A\mid B)=\frac{C(A\Rightarrow B)}{T(B)}
$$
这是关联衡量$Pr(A\; and\; B)/Pr(A)Pr(B)$的估计。

举个例子，假设项目集为${\cal K}=\{\text{peanut butter, jelly, bread}\}$，并且考虑规则$\{\text{peanut butter, jelly}\}\Rightarrow \{\text{bread}\}$。0.03的支撑值表示peanut butter，jelly和bread同时出现在3%的市场篮子中。这个规则的0.82置信度表示当购买了peanut butter和jelly，82%的情形下也会购买bread。如果bread在43%的市场篮子中，则规则$\{\text{peanut butter, jelly}\Rightarrow \text{bread}\}$的lift为1.95。

这个分析的目标是得到支撑和置信度（14.8）都高的关联规则（14.7）。Apriori算法返回由支撑阈值$t$（14.6）定义的所有高支撑的项目集。设定置信度阈值$c$，报告所有可以从这些项目集（14.6）中组成的置信度大于$c$的规则，也就是
$$
\{A\Rightarrow B\mid C(A\Rightarrow B)>c\}\qquad (14.9)
$$
对于大小为$\vert\cal K\vert$的项目集$\cal K$，有$2^{\vert{\cal K}\vert-1}-1$条形式为$A\Rightarrow ({\cal K}-A),A\subset \cal K$的规则（？？？？？？？？？？）。

!!! notes "weiya 注"
$$
\frac{1}{2}(2^{\vert\cal K\vert}-2)=2^{\vert{\cal K}-1\vert}-1
$$
Agrawal等人（1995）提出Apriori算法的一个变体，它可以从由项目集（14.6）构造的所有可能的规则中快速确定哪些规则会在置信阈值（14.9）下存留下来。

整个分析的输出是满足下面约束的关联规则（14.7）的集合。
$$
T(A\Rightarrow B)>t\qquad and\qquad C(A\Rightarrow B)>c
$$
这些一般保存在数据库中，可以被用户查询到。一般的查询请求可能是按照置信度，lift或者支撑的大小顺序排列规则。更具体地，可能会要查询在antecedent中含特定的项目或在consequent中含特定的项目的条件下的list。举个例子，一条查询请求可能如下：

> 显示ice skates为consequent，置信度大于80%且支撑大于2%的所有交易。

这可以提供能够预测ice skates销量的项（antecedent）的信息。关注特定的结果（consequent）便将问题转换成了监督学习的框架。

关联规则成为了在市场篮子是相关的设定下用于分析非常大的交易数据库的流行工具。这是当数据可以转换成多维邻接表的形式时。输出是以容易理解并且解释的关联规则（14.4）的形式展现的。Apriori算法允许分析可以用到大的数据库中，更大的数据库适用于其他类型的分析。关联规则是数据挖掘最大的成功之一。

除了对我们可以应用的数据有限制外，关联规则还有其它的限制。计算可行性的关键是支撑阈值（14.6）。项目集的解的个数，它们的大小，以及对数据需要传递的次数随着下界的下降指数型增长。因此，不会发现有高置信度或者lift，但是低支撑的规则。举个例子，比如$\text{vodka}\Rightarrow \text{caviar}$的高置信度规则将不会找到，因为后果（consequent）caviar的低销售量。

## 例子：市场篮子分析

我们将在适中的人口统计数据库中说明Apriori算法的使用。数据集包含$N=9409$分问卷，由旧金山湾区（San Francisco Bay Area）的购物商场里的消费者填写的（Impact Resources, Inc., Columbus OH, 1987）。这里我们采用前14个与人口统计有关的问题的回答来说明。可见数据中包含顺序型和（无序）类别变量，后者中的许多具有多个值。并且有许多缺失数据。

我们采用Apriori算法的免费软件实现，这归功于Christian Borgelt。（见[http://fuzzy.cs.uni-magdeburg.de/~borgelt](http://www.borgelt.net/)）。除去缺失数据的观测，每个顺序型预测变量在中值处分开并且用两个虚拟变量来编码；每个含有$k$个类别的类别型预测变量用$k$个虚拟变量编码。得到$6876\times 50$阶矩阵，6876为观测个数，50为虚拟变量个数。

这个算法总共找到6288条关联规则，涉及$\le 5$个预测变量，支撑至少为10%。理解这个大的规则集合本身是具有挑战的数据分析工作。我们这里将不会试图解决，但仅仅在图14.2中说明每个虚拟变量在数据中的相对频率（上）和在关联规则中的相对频率（下）。流行的类别趋向于在规则中频繁出现，例如，排第一的类别是language（English）。然而，其他的像职业（occupation）则表示不足（under-represented）,除了第一和第五个水平。

![](../img/14/fig14.2.png)

> 图14.2. 市场篮子分析：每个虚拟变量（对输入类别编码）在数据中的相对频率（上），以及在由Apriori算法找到的关联规则中的相对频率（下）

下面是通过Apriori算法找出的关联规则的三个例子：

- 关联规则1：25%的支撑，99.7%的置信度，1.03的lift

![](../img/14/ar1.png)

- 关联规则2：13.4%的支撑，80.8%的置信度，2.13的lift

![](../img/14/ar2.png)

- 关联规则3：26.5%的支撑，82.8%的置信度，2.15的lift

![](../img/14/ar3.png)

我们根据第一和第三条规则的高支撑选择它们。第二条规则是有高收入的consequent的关联规则，并且可以用来试图挑出高收入的个体。

正如上面叙述的，我们对每个输入预测变量构造虚拟变量，举个例子，根据收入低于和高于中位数得到 $Z_1=I(\text{income}<\$40,000)$和$Z_2=I(\text{income}\ge \$40,000)$。如果我们仅仅对寻找与高收入类别的关联感兴趣，我们可能会包含$Z_2$但不包含$Z_1$。实际市场篮子问题中经常是这种情形，我们感兴趣的是找到与现存的相对罕见项的关联，而不是跟它缺失有关的关联。

## 非监督作为监督学习

这里我们讨论将密度估计问题转化为某监督函数近似的技巧。这形成了将在下一节描述的广义关联规则的基础。

令$g(x)$为需要估计的未知数据概率密度，且$g_0(x)$为用作参考的确定的概率密度函数。举个例子，$g_0(x)$可能是在变量定义域上的均匀概率密度。其他的概率在下面讨论。假定数据集$x_1,x_2,\ldots,x_N$是从$g(x)$中抽取的独立同分布的样本。大小为$N_0$的样本可以通过蒙特卡洛法从$g_0(x)$中抽取。混合这两个数据集，并且对从$g(x)$抽取的样本赋权$w=N_0/(N+N_0)$，对从$g_0(x)$中抽取的样本赋权$w_0=N/(N+N_0)$，得到从混合密度$(g(x)+g(x_0))/2$中抽取的随机样本。如果对从$g(x)$中抽取的样本赋值为$Y=1$，对从$g_0(x)$中抽取的样本赋值为$Y=0$，则
$$
\begin{align}
\mu(x)=E(Y|x)&=\frac{g(x)}{g(x)+g_0(x)}\\
&=\frac{g(x)/g_0(x)}{1+g(x)/g_0(x)}\qquad (14.10)
\end{align}
$$
可以通过将下面的混合样本作为训练数据采用监督学习的方法来估计
$$
(y_1,x_1),(y_2,x_2),\ldots,(y_{N+N_0},x_{N+N_0})\qquad (14.11)
$$
估计的结果$\hat \mu(x)$可以反解得$g(x)$的估计
$$
\hat g(x)=g_0(x)\frac{\hat \mu(x)}{1-\hat\mu(x)}\qquad (14.12)
$$

广义逻辑斯蒂回归（4.4节）在非常适用于这个应用，因为下面的对数odd是直接估计的
$$
f(x)=log\frac{g(x)}{g_0(x)}\qquad (14.13)
$$
此时我们有
$$
\hat g(x)=g_0(x)e^{\hat f(x)}\qquad (14.14)
$$
图14.3显示了一个例子。我们在左边图中产生大小为200的训练集。右边显示了在图中长方形区域内均匀产生的参考点（蓝色）。训练样本被标号为1，而参考样本被标号为0，并且采用逻辑斯蒂回归模型对数据进行拟合，逻辑斯蒂回归模型使用自然样条的张量积（5.2.1节）。$\mu(x)$的概率等高线显示在右图中；它们也是密度估计$\hat g(x)$的等高线，因为$\hat g(x)=\hat\mu(x)/(1-\hat\mu(x))$是单调函数。等高线大致捕捉了数据的密度。

![](../img/14/fig14.3.png) 

> 图14.3. 通过分类的密度估计。（左）200个数据点的训练集。（右）加上在矩形区域内均匀产生200个参考数据点的训练集。训练样本标记为类别1，参考数据点为类别0，对数据用半参逻辑斯蒂回归模型进行拟合。图中显示了$\hat g(x)$的等高线。

原则上任意参考点都可以用作（14.14）的