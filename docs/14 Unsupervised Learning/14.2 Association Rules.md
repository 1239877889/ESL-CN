# 关联规则

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-20:2017-02-20                    |

关联规则分析已经成为挖掘贸易数据的受欢迎的工具。目标是寻找变量$X=(X_1,X_2,\ldots,X_p)$在数据中出现最频繁的联合值。大部分应用在二值数据$X_j\in\{0,1\}$中，也称作“市场篮子”分析。这种情形下观测为销售交易，比如出现在商店收银台的东西。变量表示所有在商店中出售的东西。对于观测$i$，每个变量赋两个值中的一个；如果第$j$个物品作为该次交易购买东西的一部分则$x_{ij}=1$，而如果没有购买则$x_{ij}=0$。这些频繁有联合值的变量表示物品经常被一起购买。这个信息对于货架、跨营销的促销活动、商品目录的设计，以及基于购买模式的消费者划分都是很有用的。

更一般地，关联分析的基本目标是寻找特征向量$X$的原始$X$值$v_1,\ldots,v_L$ 的集合，使得概率密度 $Pr(v_l)$在这些值上的取值相对大。在一般框架下，这个问题可以看成是“模式寻找（mode finding）”或者“碰撞狩猎（bump hunting）”。如所阐释的，这个问题是不可能的困难。每个$Pr(v_l)$的自然估计是观测$X=v_l$时的分数。对于涉及多于少量变量的问题，每个变量可以假定多余少量的值，对于其中$X=v_l$的观测值的数目用于可靠估计几乎总是太小。

进行第一次简化来修改目标。与其寻找$Pr(x)$很大的$x$值（values），不如在$X$空间中寻找相对它们大小和支撑集的大概率部分的区域（regions）。令$\cal S_j$表示第$j$个变量的所有可能值的集合（支撑集），并且令$s_j\subseteq\cal S_j$为这些值的子集。修改后的目标可以叙述成试图寻找变量值的子集$s_1,\ldots,s_p$来使得每个变量的概率同时相对地大，假设每个子集中都有一个值，则也就是使得
$$
Pr\Big[\bigcap_{j=1}^p(X_j\in s_j)\Big]\qquad (14.2)
$$
相对大。子集的交$\cap_{j=1}^p(X_j\in s_j)$称作联合规则（conjunctive rule）。对于定性变量子集$s_j$为邻接区间；对于类别型变量子集是明确界定的。注意到如果子集实际上是整个集合$s_j=\cal S_j$，经常是这种情形，变量$X_j$被称为没有出现在规则（14.2）中。

## 市场篮子分析

求解（14.2）的一般方法将在14.2.5节讨论。这在许多应用中是很有用的。然而，它们对于非常多（$p\approx 10^4,N\approx 10^8$）的交易数据是不可行的，市场篮子分析经常应用到这些数据上。对（14.2）的进一步简化是需要的。首先，只考虑了两种类型的子集；$s_j$要么包含$X_j$的单个值$s_j=v_{0j}$，或者$X_j$的整个值的集合，$s_j=\cal S_j$。这将（14.2）简化为寻找元素为整数的子集$\cal J\subset\{1,\ldots,p\}$，以及对应的值$v_{0j},j\in\cal J$，使得

$$
Pr\Big[\bigcap_{j\in \cal J}(X_j=v_{0j})\Big]\qquad (14.3)
$$

相对大。图14.1阐释了这个假设。

![](../img/14/fig14.1.png)

> 图14.1. 对应规则的简化。这里有两个输入$X_1$和$X_2$，分别取4和6个不同的值。红色方块表示高密度的区域。为了简化计算，我们假设导出的子集要么对应输入的单个值，要么对应所有值。有了这个假设，我们可以找到图中中间或者右边的模式，而不是左边的模式。

可以应用虚拟变量（dummy variables）的技巧将（14.3）转换为只涉及二值变量的问题。这里我们假设支撑集 $\cal S_j$ 对于每个变量 $X_j$ 都是有限的。具体地，构造新的变量集$Z_1,\ldots,Z_K$，对于由每个原始变量$X_1,X_2,\ldots,X_p$可获得的值$v_{lj}$中的每一个，创建一个这样的变量。虚拟变量的数目$K$为
$$
K=\sum\limits_{j=1}^p\vert \cal S_j\vert\qquad 
$$
其中$\vert \cal S_j\vert$为从$X_j$得到的唯一值的个数。

!!! notes "weiya 注"
    每个$v_{lj}$（可看成是$p$维列向量）都有一个虚拟变量$Z_{\ell}$。
    ![](../img/14/pho14.1.png)

如果与其相关联的变量取$Z_k$对应的值，则每个虚拟变量被赋值为$Z_k=1$，否则$Z_k=0$（**weiya注：**见上面注中的图片）。 这将（14.3）转换为寻找整数集${\cal K}\subset\{1,\ldots,K\}$使得下式的值大。
$$
Pr\Big[\bigcap_{k\in\cal K}(Z_k=1)\Big]=Pr\Big[\prod\limits_{k\in\cal K}Z_k=1\Big]\qquad (14.4)
$$
这是标准的市场篮子问题的组成。集合$\cal K$称为“项目集（items set）”。在项目集中的变量$Z_k$的个数称为“大小（size）”（注意到这个大小不大于$p$）。（14.4）的估计值取在数据集中式（14.4）中关联为真的观测的分数：
$$
\widehat{Pr}\Big[\prod\limits_{k\in\cal K}(Z_k=1)\Big]=\frac{1}{N}\sum\limits_{i=1}^N\prod_{k\in\cal K}z_{ik}\qquad (14.5)
$$
这里$z_{ik}$为$Z_k$的第$i$种情形的值。这（式(14.5)??）称作项目集$\cal K$的“支持（support）”或“流行（prevalence）”$T(\cal K)$。$\prod_{k\in\cal k}z_{ik}=1$的观测$i$称作“包含（contain）”项目集$\cal K$中。

在关联规则挖掘中确定支撑的下界$t$，并且寻找所有可以由变量$Z_1,\ldots,Z_k$组成的项目集$\cal K_l$，并且支撑集大于$t$，也就是
$$
\{{\cal K_l}\mid T({\cal K_l})>t\}\qquad (14.6)
$$

## Apriori 算法

如果调整阈值$t$使得式（14.6）仅由所有$2^K$个可能项集合的一小部分组成，则可以通过用于非常大的数据库的可行计算来获得该问题（14.6）的解。“Apriori”算法（Argawal等人，1995）探究维数灾难的一些方面来用数据的小部分传递求解（14.6）。具体地，对于给定的支撑阈值$t$:

- $\{{\cal K}\mid T(\cal K)>t \}$的基数相对小。
- 任意项目集$\cal L$包含$\cal K$中项的子集必须有比$\cal K$大的支撑或者相等的支撑，${\cal L\subseteq K}\Rightarrow T({\cal L})\ge T({\cal K})$

第一次传递数据计算所有单项目集合的支撑。舍弃那些支撑小于阈值的项目集。第二次传递数据计算所有可以由第一次传递中保留下来的单项目集合组成对的大小为2的项目集合的支撑。换句话说，为了产生所有大小为$\cal K=m$的频繁项目集，我们仅仅需要考虑那些候选项目，那些候选项目使得所有大小为$m-1$的祖先项目得到的大小为$m$的项目集是频繁的。舍弃那些支撑小于阈值的大小为2的项目集。每个后继的传递数据只考虑了那些可以通过结合上一次传递数据存留的项目集与第一次传递数据保留下的项目集得到的项目集。数据传递过程一直进行下去，直到来自上一次传递的所有候选规则的支撑都小于指定阈值。（？？？一定会小？？）Apriori算法仅仅要求对每个$\vert\cal K\vert$的值的一侧数据传递，这是很重要的，因为我们假设数据不能放在计算机的主存中。如果数据是充分稀疏（或者如果阈值$t$充分高），则在合理次数之后终止过程，甚至对于非常大的数据集也是如此。

许多额外的技巧可以作为这个策略的一部分来提高速度和收敛（Agrawal等人，1995）。Apriori算法标志数据挖掘技术的主要进步。

通过Apriori算法返回的每个高支撑的数据集$\cal K$（14.6）被放到“关联规则”的集合中。项目$Z_k,k\in\cal K$被分成两个分离的子集，$A\cup B=\cal K$，并且写成
$$
A\Rightarrow B\qquad (14.7)
$$
第一项子集$A$被称作“先行者”，第二个子集$B$被称为“后果”。关联规则定义为有一些性质，基于在数据库中先行项目集和后果项目集的流行程度。规则$T(A\Rightarrow B)$的“支撑”是在先行项目集和后果项目集的并中的观测的分数，恰恰是引出它们的项目集$\cal K$的支撑。可以看成在随机选择的市场篮子中同时观测项目集$Pr(A\; and\; B)$的概率的估计（14.5）。该规则的“置信度（confidence）”或“可预测性（predictability）”$C(A\Rightarrow B)$是它的支撑除以先行者的支撑
$$
C(A\Rightarrow B)=\frac{T(A\Rightarrow B)}{T(A)}\qquad (14.8)
$$
可以看成是$Pr(B\mid A)$的估计。记号$Pr(A)$是在篮子中出现项目集$A$的概率，是$Pr(\prod_{k\in A}Z_k=1)$的缩写。“期望置信度”定义为后果的支撑$T(B)$，是没有条件的概率$Pr(B)$的估计。最后，规则的“lift”定义为置信度除以期望置信度
$$
L(A\mid B)=\frac{C(A\Rightarrow B)}{T(B)}
$$
这是关联衡量$Pr(A\; and\; B)/Pr(A)Pr(B)$的估计。

举个例子，假设项目集为${\cal K}=\{\text{peanut butter, jelly, bread}\}$，并且考虑规则$\{\text{peanut butter, jelly}\}\Rightarrow \{\text{bread}\}$。0.03的支撑值表示peanut butter，jelly和bread同时出现在3%的市场篮子中。这个规则的0.82置信度表示当购买了peanut butter和jelly，82%的情形下也会购买bread。如果bread在43%的市场篮子中，则规则$\{\text{peanut butter, jelly}\Rightarrow \text{bread}\}$的lift为1.95。

这个分析的目标是得到支撑和置信度（14.8）都高的关联规则（14.7）。Apriori算法返回由支撑阈值$t$（14.6）定义的所有高支撑的项目集。设定置信度阈值$c$，报告所有可以从这些项目集（14.6）中组成的置信度大于$c$的规则，也就是
$$
\{A\Rightarrow B\mid C(A\Rightarrow B)>c\}\qquad (14.9)
$$
对于大小为$\vert\cal K\vert$的项目集$\cal K$，有$2^{\vert{\cal K}\vert-1}-1$条形式为$A\Rightarrow ({\cal K}-A),A\subset \cal K$的规则（？？？？？？？？？？）。

!!! notes "weiya 注"
$$
\frac{1}{2}(2^{\vert\cal K\vert}-2)=2^{\vert{\cal K}-1\vert}-1
$$
Agrawal等人（1995）提出Apriori算法的一个变体，它可以从由项目集（14.6）构造的所有可能的规则中快速确定哪些规则会在置信阈值（14.9）下存留下来。

整个分析的输出是满足下面约束的关联规则（14.7）的集合。
$$
T(A\Rightarrow B)>t\qquad and\qquad C(A\Rightarrow B)>c
$$
这些一般保存在数据库中，可以被用户查询到。一般的查询请求可能是按照置信度，lift或者支撑的大小顺序排列规则。更具体地，可能会要查询在antecedent中含特定的项目或在consequent中含特定的项目的条件下的list。举个例子，一条查询请求可能如下：

> 显示ice skates为consequent，置信度大于80%且支撑大于2%的所有交易。

这可以提供能够预测ice skates销量的项（antecedent）的信息。关注特定的结果（consequent）便将问题转换成了监督学习的框架。

关联规则成为了在市场篮子是相关的设定下用于分析非常大的交易数据库的流行工具。这是当数据可以转换成多维邻接表的形式时。输出是以容易理解并且解释的关联规则（14.4）的形式展现的。Apriori算法允许分析可以用到大的数据库中，更大的数据库适用于其他类型的分析。关联规则是数据挖掘最大的成功之一。

