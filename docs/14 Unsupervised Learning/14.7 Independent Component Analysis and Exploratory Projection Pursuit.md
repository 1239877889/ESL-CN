# 14.7 独立成分分析和探索投影寻踪

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-09-03                   |
|更新 |2018-01-20|

多元数据经常被看成是从未知来源中多重间接测量的数据，一般不能直接测量。包括以下例子：

- 在教育或心理学测试中，采用问卷的答案来衡量潜在的智商以及其他的心理特征。
- EEG脑扫描通过放置在头部的不同位置的感受器的电子信号来间接衡量脑的不同部分的神经元活性。
- 股票交易价格随着时间持续变化，并且反映了各种各样的未测量的因素，如市场信心，外部影响以及其他很难识别和测量的推动力。

因子分析(Factor Analysis)是在统计学领域为了识别潜在因素的一个经典方法。因子分析模型通常是用在高斯分布中，某种程度上阻碍了它的适用性。最近，独立成分分析(Independent Component Analysis)成为了因子分析的强劲对手，我们将会看到，它对非高斯分布的依赖是其成功的根本来源。

## 潜变量和因子分析

奇异值分解$\mathbf X=\mathbf{UDV}^T$(14.54)有潜变量的表示。记$\mathbf S=\sqrt{N}\mathbf U$，以及$\mathbf A^T=\mathbf{DV}^T/\sqrt{N}$，我们有$\mathbf{X=SA}^T$，因此$\mathbf X$的每一列是$\mathbf S$的列的线性组合。现在因为$\mathbf U$是正交的，并且和之前一样假设$\mathbf X$的列均值为0（因此$\mathbf U$也是），这意味着$\mathbf S$列均值为0，而且不相关，有单位方差。采用随机变量，我们可以把SVD或者对应的主成分分析看成是下列潜变量模型的一个估计

$$
\begin{align}
X_1&=a_{11}S_1+a_{12}S_2+\cdots+a_{1p}S_p\\
X_2&=a_{21}S_1+a_{22}S_2+\cdots+a_{2p}S_p\\
\vdots &\qquad\vdots\\
X_p&=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pp}S_p
\end{align}
\qquad (14.78)
$$

或者简单地写成$X=\A S$。相关的$X_j$都表示称不相关的、单位方差的变量$S_\ell$的线性展开。尽管这不是太满意，因为对于任意给定的$p\times p$的正交矩阵$\R$，我们可以写出

$$
\begin{align}
X&=\A S\\
&=\A\R^T\R S
&=\A^*S^*
\end{align}
\qquad (14.79)
$$

并且$\Cov (S^*)=\R\Cov(S)\R^T=\I$。因此存在许多这样的分解，也因此不可能将任意特定的潜变量作为唯一的潜在来源。SVD分解确实有这样的性质，以最优的方式得到使得任意$q < p$的截断分解。

经典的因子分析模型，主要由心理测量学(psychometrics)的研究者发展而来。某种程度上缓解了这个问题；举个例子，Mardia et al. (1979)[^1]。当$q < p$，因子分析模型有如下形式

$$
\begin{align}
X_1&=a_{11}S_1+a_{12}S_2+\cdots+a_{1q}S_q+\varepsilon_1\\
X_2&=a_{21}S_1+a_{22}S_2+\cdots+a_{2q}S_q+\varepsilon_2\\
\vdots &\qquad\vdots\\
X_p&=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pq}S_q+\varepsilon_p
\end{align}
\qquad (14.80)
$$

或者写成$X=\A S+\epsilon$。这里$S$是 $q < p$个揭示潜变量或者因子的向量，$\A$ 是$p\times q$的因子载荷(loadings)矩阵，$\varepsilon_j$是零均值不相干的扰动。想法是潜变量$S_\ell$是$X_j$公共方差的来源，意味着它们直接的相关性结构，而$\varepsilon_j$对每个$X_j$是唯一的，并且解释了剩下的方差。一般地，$S_\ell$和$\varepsilon_j$假设为高斯随机变量，并且采用极大似然法来拟合模型。参数都存在于下面的协方差阵中

$$
\Sigma=\A\A^T+\D_\varepsilon \qquad (14.81)
$$

其中$\D_\varepsilon = diag[\Var(\varepsilon_1),\ldots, \Var(\varepsilon_p)]$。$S_\ell$为高斯并且不相关，这使得它们在统计上是独立随机变量。因此一系列的教育考试成绩可以认为是有潜在独立的因子，比如智力(intelligence)，动机(drive)等等所决定的。$\A$的列被称为因子载荷(factor loadings)，而且用来命名因子和解释因子。

不幸的是，唯一性问题(14.79)仍然存在，因为在式(14.81)中，对于任意$q\times q$的正交矩阵$\R$，$\A$和$\A\R^T$是等价的。这导致了因子分析中的主观性，因为用户可以寻找因子更易解释的旋转版本。这点使得许多分析学家对因子分析表示之一，而且这可能是它在当代统计中不受欢迎的原因。尽管我们这里不继续讨论细节，但是SVD分解在(14.81)的估计上发挥重要作用。举个例子，$\Var(\varepsilon_j)$假设相等，SVD的前$q$个成分确定了由$\A$张成的子空间。

因为每个$X_j$独立的扰动$\varepsilon_j$，因子分析可以看成是对$X_j$的相关结构进行建模，而非对协方差结构建模。这个可以通过对(14.81)的协方差结构进行标准化后得到（练习14.14）。

!!! note "weiya注: Ex. 14.14"
    练习14.14表明，因子分析实质上是对相关矩阵进行分解。详见解答见[Ex. 14.14](https://github.com/szcf-weiya/ESL-CN/issues/51)


这是因子分析与PCA的重要区别，尽管这不是我们讨论的重点。练习14.15讨论了一个简单的例子，因为这个差异，因子分析和主成分的解完全不同。

!!! note "weiya注: Ex. 14.15"
    练习14.15用一个实际例子说明了，因子分析和主成分得到的第一因子和第一主成分完全不同。详细解答见[Ex. 14.15](https://github.com/szcf-weiya/ESL-CN/issues/50)

## 独立分量分析

独立分量分析(ICA)模型与(14.78)有相同的形式，除了$S_\ell$假设为统计上独立而非不相关。

!!! note "weiya注：独立与不相关"
    统计上，连续型随机变量$X$与$Y$独立的定义为
    $$
    p(x, y)=p_X(x)p_Y(y)\;\forall x,y
    $$
    而不相关的定义为
    $$
    Cov(X, Y)=0
    $$
    独立意味着不相关，但反之不对。对于二元正态随机变量，两者等价。

直观上，不相关决定着多元变量分布的二阶交叉矩(协方差)，而一般地，统计独立决定了所有的交叉矩。这些额外的矩允许我们找到唯一的$\A$中的元素。因为多元正态分布由二阶矩独立确定，因此这是一个例外，任意高斯独立组分可以像之前一样乘以一个旋转来确定。因此如果假设$S_\ell$是独立且非高斯的，则可以避免(14.78)和(14.80)的唯一性问题。

这里我们将要讨论(14.78)中的全$p$个组分的模型，其中$S_\ell$是独立的且有单位方差；因子分析模型(14.80)的ICA版本也同样存在。我们的处理基于Hyvärinen and Oja (2000)的综述文章。

我们希望恢复$X=\A S$中的混合矩阵$\A$。不失一般性，我们假设$X$已经满足$\Cov(X)=\I$；这一般可以通过前面描述的SVD实现。反过来，因为$S$的协方差也为$\I$，这意味着$\A$是正交的。所以求解ICA问题等价于寻找正交的$\A$使得随机变量向量$S=\A^T X$的组分是独立(且是非高斯的)。

图14.37显示了在分离两个混合信号方面ICA的能力。这也是经典的cocktail party problem的一个例子，不同的麦克风$X_j$接受来自不同独立源$S_\ell$（音乐、不同人说的话等等）的混合信号。ICA通过利用原始信号源的独立性和非高斯性，能够进行盲信号分离(blind source separation)。

ICA许多流行的方式是基于熵。密度为$g(y)$的随机变量$Y$的相对熵(differential entropy) $H$由下式给出

$$
H(Y)=-\int g(y)\LOG g(y)dy\qquad (14.82)
$$

信息理论中一个著名的结论是在所有等方差的随机变量中，高斯随机变量有最大的熵。最后，随机向量Y$的组分之间的互信息量$I(Y)$是独立的一个自然度量：

$$
I(Y)=\sum\limits_{j=1}^pH(Y_j)-H(Y)\qquad (14.83)
$$

值$I(Y)$称为$Y$的密度$g(y)$与其独立版本$\prod\limits_{j=1}^pg_j(y_j)$之间的Kullback-Leibler距离，其中$g_j(y_j)$是 $Y_j$的边缘密度. 如果 $X$ 有协方差$\I$, 且$Y=\A^TX$, 其中$\A$是正交, 则易证

$$
\begin{align}
I(Y)&= \sum\limits_{j=1}^pH(X)-\LOG\vert det\A\vert\qquad (14.84)\\
&=\sum\limits_{j=1}^pH(Y_j)-H(X)\qquad (14.85)
\end{align}
$$

!!! note "weiya注: (14.85)的证明"
	只要证
	$$
	H(Y)=H(X)+\LOG \vert \det \A \vert
	$$
	对(14.82)进行变量替换有
	$$
	H(Y)=-\int g(\A'x)\LOG g(\A'x)\vert \det \J\vert dx\qquad (*)
	$$
	其中$\J_{ij}=\frac{\partial y_i}{\partial x_j}$.
	又
	$$
	y_i = \sum\limits_{j=1}^p (A')_{ij}x_j
	$$
	故
	$$
	\J_{ij}=(\A')_{ij}
	$$
	所以$\det\J = \det \A'=\det \A$.
	另外, $x$的密度函数为
	$$
	f(x)=g(\A'x)\cdot\vert \det\J\vert 
	$$
	于是(*)式可以写成
	$$
	\begin{align}
	H(Y) &= -\int \frac{f(x)}{\vert\det\A\vert} \LOG\frac{f(x)}{\vert\det\A\vert}\cdot\vert\det\A\vert dx\\
	&=-\int f(x)[\LOG f(x)-\LOG\vert\det\A\vert]dx\\
	&=H(x)+\LOG\vert\det\A\vert
	\end{align}
	$$
	证毕.

寻找$\A$来最小化$I(Y)=I(\A^TX)$是寻找使得组分间的独立性最强的正交变换. 考虑到式(14.84), 这等价于最小化$Y$的分离组分的熵的和, 反过来意味着最大化它们与高斯分布的距离.



## 探索投影寻踪

TODO

## 独立分量分析的一种直接方法

TODO

[^1]: Mardia, K., Kent, J. and Bibby, J. (1979). Multivariate Analysis, Academic Press.
