# 14.7 独立成分分析和探索投影寻踪

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-09-03                   |

多元数据经常被看成是从未知来源中多重间接测量的数据，一般不能直接测量。包括以下例子：

- 在教育或心理学测试中，采用问卷的答案来衡量潜在的智商以及其他的心理特征。
- EEG脑扫描通过放置在头部的不同位置的感受器的电子信号来间接衡量脑的不同部分的神经元活性。
- 股票交易价格随着时间持续变化，并且反映了各种各样的未测量的因素，如市场信心，外部影响以及其他很难识别和测量的推动力。

因子分析(Factor Analysis)是在统计学领域为了识别潜在因素的一个经典方法。因子分析模型通常是用在高斯分布中，某种程度上阻碍了它的适用性。最近，独立成分分析(Independent Component Analysis)成为了因子分析的强劲对手，我们将会看到，它对非高斯分布的依赖是其成功的根本来源。

## 潜变量和因子分析

奇异值分解$\mathbf X=\mathbf{UDV}^T$(14.54)有潜变量的表示。记$\mathbf S=\sqrt{N}\mathbf U$，以及$\mathbf A^T=\mathbf{DV}^T/\sqrt{N}$，我们有$\mathbf{X=SA}^T$，因此$\mathbf X$的每一列是$\mathbf S$的列的线性组合。现在因为$\mathbf U$是正交的，并且和之前一样假设$\mathbf X$的列均值为0（因此$\mathbf U$也是），这意味着$\mathbf S$列均值为0，而且不相关，有单位方差。采用随机变量，我们可以把SVD或者对应的主成分分析看成是下列潜变量模型的一个估计

$$
\begin{align}
X_1&=a_{11}S_1+a_{12}S_2+\cdots+a_{1p}S_p\\
X_2&=a_{21}S_1+a_{22}S_2+\cdots+a_{2p}S_p\\
\vdots &\vdots\\
X_p&=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pp}S_p
\end{align}
\qquad (14.78)
$$

或者简单地写成$X=\A S$。相关的$X_j$都表示称不相关的、单位方差的变量$S_\ell$的线性展开。尽管这不是太满意，因为对于任意给定的$p\times p$的正交矩阵$\R$，我们可以写出

$$
\begin{align}
X&=\A S\\
&=\A\R^T\R S
&=\A^*S^*
\end{align}
\qquad (14.79)
$$

并且$\Cov (S^*)=\R\Cov(S)\R^T=\I$。因此存在许多这样的分解，也因此不可能将任意特定的潜变量作为唯一的潜在来源。SVD分解确实有这样的性质，以最优的方式得到使得任意$q < p$的截断分解。

经典的因子分析模型，主要由心理测量学(psychometrics)的研究者发展而来。某种程度上缓解了这个问题；举个例子，Mardia et al. (1979)[^1]。当$q < p$，因子分析模型有如下形式

$$
\begin{align}
X_1&=a_{11}S_1+a_{12}S_2+\cdots+a_{1p}S_q+\varepsilon_1\\
X_2&=a_{21}S_1+a_{22}S_2+\cdots+a_{2p}S_q+\varepsilon_2\\
\vdots &\vdots\\
X_p&=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pp}S_q+\varepsilon_p
\end{align}
\qquad (14.80)
$$

或者写成$X=\A S+\epsilon$。这里$S$是$q < p$个揭示潜变量或者因子的向量，$\A$是$p\times q$的因子载荷(loadings)矩阵，$\varepsilon_j$是零均值不相干的扰动。想法是潜变量$S_\ell$是$X_j$公共方差的来源，意味着它们直接的相关性结构，而$\varepsilon_j$对每个$X_j$是唯一的，并且解释了剩下的方差。一般地，$S_\ell$和$\varepsilon_j$假设为高斯随机变量，并且采用极大似然法来拟合模型。参数都存在于下面的协方差阵中

$$
\Sigma=\A\A^T+\D_\varepsilon \qquad (14.81)
$$

其中$\D_\varepsilon = diag[\Var(\varepsilon_1),\ldots, \Var(\varepsilon_p)]$。$S_\ell$为高斯并且不相关，这使得它们在统计上是独立随机变量。因此一系列的教育考试成绩可以认为是有潜在独立的因子，比如智力(intelligence)，动机(drive)等等所决定的。$\A$的列被称为因子载荷(factor loadings)，而且用来命名因子和解释因子。

不幸的是，唯一性问题(14.79)仍然存在，因为在式(14.81)中，对于任意$q\times q$的正交矩阵$\R$，$\A$和$\A\R^T$是等价的。这导致了因子分析中的主观性，因为用户可以寻找因子更易解释的旋转版本。这点使得许多分析学家对因子分析表示之一，而且这可能是它在当代统计中不受欢迎的原因。尽管我们这里不继续讨论细节，但是SVD分解在(14.81)的估计上发挥重要作用。举个例子，$\Var(\varepsilon_j)$假设相等，SVD的前$q$个成分确定了由$\A$张成的子空间。

因为每个$X_j$独立的扰动$\varepsilon_j$，因子分析可以看成是对$X_j$的相关结构进行建模，而非对协方差结构建模。这个可以通过对(14.81)的协方差结构进行标准化后得到（练习14.14）。这是因子分析与PCA的重要区别，尽管这不是讨论的重点。练习14.15讨论了一个简单的例子，因为这个差异，因子分析和主成分的解完全不同。

!!! note "weiya注: Ex. 14.15"
    练习14.15用一个实际例子说明了，因子分析和主成分得到的第一因子和第一主成分完全不同。

## 独立分量分析

TODO

## 探索投影寻踪

TODO

## 独立分量分析的一种直接方法

TODO
