# 12.7 混合判别分析

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf#page=446) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2018-08-08                   |

线性判别分析可以看成是 **原型 (prototype)** 分类器。每个类别都由其形心表示，并且使用某种合适的度量，将其分到最近的形心处。在许多情形下，单个原型不足以表示不均匀的类别，而混合模型是更合适的。在这节中，我们回顾高斯混合模型，并且展示怎么从之前讨论的 FDA 和 PDA 中推广得到。第 $k$ 类别的高斯混合模型密度为

$$
P(X\mid G=k)=\sum\limits_{r=1}^{R_k}\pi_{kr}\phi(X;\mu_{kr},\mathbf\Sigma),\tag{12.59}
$$

其中混合比例 $\pi_{kr}$ 和为 $1$。第 $k$ 类有 $R_k$ 个原型，并且在我们的情形中，始终采用相等的协方差作为度量。对每个类别给定这样一个模型，类别后验概率由下式给出

$$
P(G=k\mid X=x)=\frac{\sum_{r=1}^{R_k}\pi_{kr}\phi(X;\mu_{kr},\mathbf \Sigma)\Pi_k}{\sum_{\ell=1}^K\sum_{r=1}^{R_\ell}\pi_{\ell r}\phi(X;\mu_{\ell r},\mathbf \Sigma)\Pi_\ell}\tag{12.60}
$$

其中 $\Pi_k$ 表示类别的先验概率。

我们在第 8 章看到两个组分的特殊情形的计算。如 LDA 中一样，我们通过极大似然估计参数，采用如下基于 $P(G,X)$ 的联合对数似然：

$$
\sum\limits_{k=1}^K\sum\limits_{g_i=k}\log\Big[\sum\limits_{r=1}^{R_k}\pi_{kr}\phi(x_i;\mu_{kr},\mathbf \Sigma)\Pi_k\Big]\tag{12.61}
$$

如果直接处理对数中的和，会变成很复杂的优化问题。计算混合分布的极大似然估计的经典的、自然的方法是 EM 算法 (Dempster et al., 1977)，它具有良好的收敛性质。EM 在下面两步间轮换：

- E-step：给定当前的参数，对类别 $k$ 的每个观测 ($g_i=k$) 计算类别 $k$ 的子类 $c_{kr}$ 的 responsibility：
$$
W(c_{kr}\mid x_i, g_i)=\frac{\pi_{kr}\phi(x_i;\mu_{kr},\mathbf \Sigma)}{\sum\limits_{\ell=1}^{R_k}\pi_{k\ell}\phi(x_i;\mu_{k\ell},\mathbf \Sigma)}.\tag{12.62}
$$
- M-step: 采用 E-step 的权重，计算每个类别中每个高斯组分的参数的加权 MLE。

在 E-step，算法将类别 $k$ 中的观测的单位权重赋给属于该类的不同子类。如果观测点离某个子类的形心很近，且离其它形心很远，则它会收到离该子类很近的权重。另一方面，如果观测点在两个子类中间则它会得到两个子类近似相等的权重。

在 M-step，类别 $k$ 的观测使用 $R_k$ 次，来估计 $R_k$ 个组分密度中每一个的参数，对于每个组分由不同的权重。EM 算法在第 8 章中详细介绍。算法要求初始化，这可能有影响，因为混合模型一般都是多种模型。我们的软件（后面计算上的考虑一节中有提到）允许多种策略；这里我们描述默认的策略。用户提供每个类中子类的个数 $R_k$。在类别 $k$ 中，用多重随机起始点的 $k$ 均值聚类模型对数据进行拟合。这将观测值划分到 $R_k$ 个不相交的群中，其中构造了包含 $0$ 和 $1$ 的初始权重矩阵。

我们对始终相等的组分协方差矩阵 $\mathbf\Sigma$ 的假设带来额外的简化；我们可以像 LDA 中一样在混合形式中结合秩的约束。为了理解这一点，回顾下 LDA 的一些事实（[4.3.3 节](../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis/index.html)）。rank-$L$ 的 LDA 拟合等价于高斯模型的极大似然拟合，其中每个类别中不同的均值向量约束到 $\IR^p$ 的 rank-$L$ 子空间（[练习 4.8](https://github.com/szcf-weiya/ESL-CN/issues/143)）。我们可以从混合模型中继承这条性质，并且在满足在所有 $\sum_kR_k$ 个形心上的秩的约束的条件（$\mathrm{rank}\{\mu_{k\ell}\}=L$）下最大化对数似然 $(12.61)$。

又一次可以用 EM 算法，并且 M-step 事实上是加权版本的 LDA，有 $R=\sum_{k=1}^KR_k$ 个类别。而且，我们可以采用之前一样的最优得分来解决加权 LDA 问题，使得我们在这一步可以使用 FDA 或者 PDA。可以预料，除了“类别”数可以增加，第 $k$ 类中的观测书面也会类似增加 $R_k$ 倍。事实证明如果线性算子用于最优得分回归不是这种情形。增广的响应变量矩阵 $\mathbf Y$ 折叠成模糊的响应矩阵 $\mathbf Z$，直觉上令人愉快。举个例子，假设有 $K=3$ 个类别，并且每个类中有 $R_k=3$ 个子类。则 $\mathbf Z$ 可能是

![](../img/12/eq12.63.PNG)

其中类别 $k$ 行的元素对应 $W(c_{kr}\mid x,g_i)$。


