# 12.5 FDA

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf#page=459) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-12-15                   |

在这一节我们描述一种在导出的响应变量上使用线性回归的 LDA 方法。这相应地导出非参的以及更灵活的替换 LDA 的方法。如第 4 章一样，我们假设我们有定量变量 $G$，它落入 $K$ 个类别 ${\cal G} = \{1,\ldots,K\}$ 中的其中一个类别，每个都有测量特征 $X$。假设 $\theta: \cal G\mapsto\IR^1$ 是对类别打分的函数，使得转换后的类别表情通过 $X$ 上的线性回归来最优预测：如果我们的训练样本形式为 $(g_i,x_i),i=1,2,\ldots,N$，则我们求解带关于$\theta$约束的下式来避免平凡解（在训练数据上均值为零单位方差）

$$
\underset{\beta,\theta}{\min}\sum\limits_{i=1}^N(\theta(g_i)-x_i^T\beta)^2\tag{12.52}
$$

这得到类别间的一维划分。

更一般地，我们可以寻找至多 $L\le K-1$ 个类别标签的独立得分， $\theta_1,\theta_2,\ldots,\theta_L$，并且 $L$ 个对应的线性映射 $\eta_\ell(X)=X^T\beta_\ell,\ell=1,\ldots,L$，它们的选择使得 $\IR^p$ 中的多重回归达到最优。得分 $\theta_\ell(g)$ 以及映射 $\beta_\ell$ 的选择使得均方残差最小

$$
ASR=\frac 1N\sum\limits_{\ell=1}^L[\sum\limits_{i=1}^N(\theta_\ell(g_i)-x_i^T\beta_\ell)^2]\tag{12.53}
$$

得分集假设为相互正交的并且关于合适的内积进行了标准化来避免平方的零解。

为什么我们要采用这种方式？可以证明 4.3.3 节导出的判别（典则）向量 $\nu_\ell$ 的序列等于 $\beta_\ell$ 乘以一个常数 (Mardia et al., 1979; Hastie et al., 1995)。更多地，测试点 $x$ 到第 $k$ 个类别重心 $\hat\mu_k$ 的 Mahalanobis 距离由下式给出

$$
\delta_J(x,\hat\mu_k)=\sum\limits_{\ell=1}^{K-1}w_\ell(\hat\eta_\ell(x)-\bar\eta_\ell^k)^2+D(x)\tag{12.54}
$$

其中 $\bar \eta_\ell^k$ 是第 $k$ 类的 $\hat\eta_\ell(x_i)$ 的均值，且 $D(x)$ 不依赖 $k$。这里 $w_\ell$ 是坐标权重，并且用第 $\ell$ 个最优的得分的均方残差 $r_\ell^2$ 来拟合

$$
w_\ell = \frac{1}{r_\ell^2(1-r_\ell^2)}\tag{12.55}
$$

在 4.3.2 节中，我们看到典则距离是高斯情形下分类所需要的全部条件，并且每个类别间有相等的协方差。总结下，就是：

> LDA 可以通过一系列的线性回归来实现，将测试点划分到拟合空间中距离最近的那个类别重心的类中。这类似可以应用到降维后的版本，或者当 $L=K-1$ 时的全秩版本。

这个结果的真正力量在于它所引用的泛化。我们可以用更灵活、非参的拟合来替换线性回归拟合 $\eta_\ell(x)=x^T\beta_\ell$，并且类似地达到比 LDA 更灵活的分类器。我们考虑了广义可加拟合，样条函数，MARS 模型以及类似的。在这种更一般的形式下，回归问题通过下面准则来定义

$$
ASR(\{\theta_\ell,\eta_\ell\}^{\ell=1}^L)=\frac 1N\sum\limits_{\ell=1}^L[\sum\limits_{i=1}^N(\theta_\ell(g_i)-\eta_\ell(x_i))^2+\lambda J(\eta_\ell)]\tag{12.56}
$$

其中 $J$ 是关于某种形式的非参回归的合适的正则器，比如光滑样条，可加样条以及低阶 ANOVA 样条模型。其中也包括由核生成的函数类和相关惩罚，如 12.3.3 节中一样。

