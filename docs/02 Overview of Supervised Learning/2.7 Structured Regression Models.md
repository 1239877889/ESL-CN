# 2.7 结构化的回归模型

原文     | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf)
      ---|---
翻译     | szcf-weiya
时间     | 2016-08-01

尽管最近邻和其它局部的方法都是直接关注在某一点估计函数，在高纬遇到困难。甚至在低维下有很多结构化的充分利用数据的方式的情形这些方法也不合适。这个部分介绍了结构化方式的类别。在我们讨论之前，我们更深入地讨论这些类别的需要。

## 问题的困难度

对任意函数$f$,考虑RSS准则
\begin{equation}
RSS(f)=\sum\limits_{i=1}^{N}(y_i-f(x_i))^2
\label{2.37}
\end{equation}
对式(\ref{2.37})最小化导致许多解决方法:任意过所有训练点$(x_i,y_i)$的函数$\hat{f}$是一个解。选择的任意特定的解可能是一个在与训练点不同的测试点不是良好的预测。如果在每个$x_i$值有多个观测对$x_i,y_{i\ell},\ell =1,\ldots,N_i$，风险会被限制。在这种情形下，解通过每个$x_i$对应的所有$y_{i\ell}$的平均值，见练习2.6。这种情形类似我们已经在统计判别理论章节中的已经访问的点；当然，(2.37)$是$p18中式(2.11)的有限样本的版本。如果样本规模$N$充分大使得重复是保证和密集排列的，这些解决方案可能都倾向于限制条件期望。

为了得到有限$N$的有用结果，我们必须限制满足条件的式($\ref{2.37}$)的解到一个小的解的集合。怎样决定限制条件的本质是基于数据之外的一些考虑。这些限制条件经常通过$f_0$的参数表示，或者在学习方法本身里面构建，有隐式也有显式。这些限制的解决方案的类别是本书讨论的重点。一件事应该变得清晰。任何插入到$f$上的限制条件导致式($\ref{2.37}$)的唯一解并不能消除解的多样性带来的模棱两可。有无数种导致唯一解可能的限制条件，所以模棱两可简单地被转换为限制条件的选择。

一般地，大多数学习方法插入的限制条件可以用一种或其他的限制条件的复杂度来描述。这通常也意味着在输入空间的小邻域的一些规则的行为。这就是，对于在某种度量下充分互相接近的所有输入点$x$，$\hat{f}$展现了一些特殊的结构比如说接近常值，线性或者低次的多项式。然后估计是通过平均或者在邻域多项式拟合得到。

限制的强度由邻域的大小所支配。邻域的规模越大，限制越强，而且解对特定限制的选择也很敏感。举个例子，在无限个邻域内局部常值拟合根本不是限制；在一个比较大的邻域内进行局部线性拟合几乎是全局线性模型，而且限制性是非常强的。

限制的本质取决于采用的度量。一些像核方法、局部回归和基于树的方法，直接明确了度量以及邻域的规模。至今为止讨论的最近邻方法是基于函数局部为常值的假设；距离目标输入$x_0$越近，函数值不会改变太多，而且可以平均输出得到的$\hat{f}(x_0)$也会很接近。其它像样条、神经网络以及基于函数的方法隐性定义了邻域的局部行为。在5.4.1我们将要讨论等价核(*equivalent kernel*)的概念（见书p157的图5.8），描述了任意输出的线性方法的局部依赖。这些在很多情形下的等价核恰恰像上面讨论过的显式定义的系数核——在目标点达到巅峰然后从该点光滑下降。

现在必须澄清一个事实。任何在等方性的邻域中试图产生局部不同的函数会在高纬中遇到问题——再一次是维数的灾难。相反地，所有克服维数问题的方法在测量邻域时有一个对应的——经常是隐式的或者适应的——度量，该度量基本要求是不允许邻域在各个方向都同时小。
