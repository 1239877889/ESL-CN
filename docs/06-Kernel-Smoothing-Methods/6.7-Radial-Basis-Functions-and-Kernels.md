# 径向基函数和核

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-03-06：2017-03-06                    |

在第五章中，函数表示成基函数展开的形式：$f(x)=\sum_{j=1}^M\beta_jh_j(x)$。采用基函数展开的灵活建模的艺术包含选取合适的基函数族，并且接着控制由选择，正则化，或者两者都有的表示的复杂度。部分基函数族的元素是局部定义的，比如，B样条在$R$中局部定义。如果在特定区域中需要更多的灵活性，接着这个区域需要用更多的基函数来表示（在B样条转换为更多结点的情形中）。$R$局部基函数的张量积实现了$R^p$中的局部基函数。不是所有的基函数都是局部的——举个例子，对于样条的截断幂基，或者在神经网络中使用的S型基函数$\sigma(\alpha_0+\alpha x)$（见第11章）。虽然如此，复合函数$f(x)$可以显示出局部行为，因为参数的特定符号和值造成全局影响的抵消。举个例子，对于同样的函数空间截断幂基有等价的B样条基；取消恰恰就是这种情形。

核方法通过在目标点$x_0$局部的区域中拟合简单的模型来实现灵活性。局部化通过加权核$K_\lambda$来实现，并且单个观测权重为$K_\lambda(x_0,x_i)$。

通过将核函数$K_\lambda(\xi,x)$作为基函数，径向基函数结合这些想法。这得到模型
$$
\begin{align}
f(x) &= \sum\limits_{j=1}^MK_{\lambda_j}(\xi_j,x)\beta_j\\
&= \sum\limits_{j=1}^MD(\frac{\Vert x-\xi_j\Vert}{\lambda_j})\beta_j\qquad (6.28)
\end{align}
$$
其中每个基元素由位置或者原型参数$\xi_j$以及缩放参数$\lambda_j$。 $D$的一个流行的选择是标准高斯密度函数。有许多方式来学习参数$\{\lambda_j,\xi_j,\beta_j\},j=1,\ldots,M$。为了简化，我们关注回归的最小二乘方法，并且采用高斯核。

- 有关关于所有参数的平方和
$$
\underset{\{\lambda_j,\xi_j,\beta_j\}_1^M}{\min}\sum\limits_{i=1}^M(y_i-\beta_0-\sum\limits_{j=1}^M\beta_jexp\{-\frac{(x_i-\xi_j)^T(x_i-\xi_j)}{\lambda_j^2}\})^2\qquad (6.29)
$$

​		这个模型一般称为RBF网络，这是一个S型神经网络的另一个选择，将在第11章讨论；$\xi_j$和$\lambda_j$在参数中有重要作用。这个准则是有着多重局部最小点的非凸函数，并且优化的算法类似神经网络中的算法。

- 分开估计$\beta_j$和$\{\lambda_j,\xi_j\}$。给定前者，后者的估计是简单的最小二乘问题。通常单独给出$X$的分布，以非监督的方式选择核参数$\lambda_j$和$\xi_j$。其中一种方式是在给定中心$\xi_j$和缩放$\lambda_j$，对训练$x_i$拟合高斯混合密度模型。其特征更特别指定（adhoc）的方式使用聚类方法来确定原型$\xi_j$，并且将$\lambda_j=\lambda$看成是超参数。这些方式的显然缺点是条件分布$\Pr(Y\mid X)$，并且特别地，$\E(Y\mid X)$在集中的地方是没有作用的。在正的一边，可以更简单地实现。
