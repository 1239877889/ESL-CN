# 离散变量的无向图模型

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-25:2017-02-25                    |

离散变量的无向马尔科夫网络是很流行的，而且特别地，二值变量的成对马尔科夫网络更普遍。在统计力学领域有时称为Ising模型，在机器学习领域称为玻尔兹曼机(Boltzmann machines)，其中顶点称为“结点(nodes)”或“单元(units)”，而且是取两个值的。

另外，每个结点处的值可以被观测到(visible)或者观测不到(hidden)。结点通常以层来组织，类似神经网络。玻尔兹曼机对于非监督学习和监督学习都是很有用的，特别队友结构化输入数据，比如图像，但是也受到计算上的限制。图17.6显示了一个约束的玻尔兹曼机（后面讨论），其中一些变量是隐藏的，而且只有一些结点对是相连的。我们所有$p$个结点都是可见的简单情形，并且边的对$(j,k)$取自$E$。

用$X_j$标记结点$j$的二值变量，它们联合分布的Ising模型由下式给出
$$
p(X,\mathbf\Theta)=exp[\sum\limits_{(j,k)\in E}\theta_{jk}X_jX_k-\Phi(\mathbf{\Theta})]\text{  for  }X\in{\cal X}\qquad (17.28)
$$
其中${\cal X}=\{0,1\}^p$。和上一节的高斯模型一样，只有成对交叉项才建立模型。Ising 模型是在统计力学中发展的，而且现在更一般地用来建立成对交叉的联合影响。$\mathbf{\Phi(\Theta)}$为分割函数的对数，而且由下式定义
$$
\Phi(\mathbf \Theta)=\mathrm{log}\sum\limits_{x\in\cal X}[\mathrm {exp}(\sum\limits_{(j,k)\in E})\theta_{jk}x_jx_k]\qquad (17.29)
$$
