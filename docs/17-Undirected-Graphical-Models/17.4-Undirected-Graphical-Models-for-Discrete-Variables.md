# 17.4 离散变量的无向图模型

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-25:2017-02-26                    |
| 时间   | 2017-08-27                    |

> 写在前面
>
> 这一节中我觉得最有应用价值的是RBM，这跟深度学习很相关，在LISA lab, University of Montreal编的那本Deep Learning Tutorial (Release 0.1) 中，有一节就是将如何用RBM做手写数字识别。那段程序大概六个月前就已经接触了，但一直没搞清楚它在干嘛:disappointed:。其实，本节最后作者提到的用RBM做手写数字识别中的某一步其实就是那段程序干的事情。事实上，那段程序并没有真正地完成识别问题，仅仅是其中的一步——“采用对比发散训练包含784个可见单元和500个隐藏单元的RBM，来建立图像集的模型”，后续步骤还应该有“将这个RBM的隐藏状态作为训练第二个RBM的数据，第二个RBM含500个可见单元和500个隐藏单元。最后，第二个RBM的隐藏状态用作训练含2000个隐藏单元的RBM的输入特征，并且用上标签数据。” 或许，这导致当时没有理解程序所要完成的目标的原因吧:confounded:  
>
> 程序可以在[这里](https://github.com/szcf-weiya/ESL-CN/tree/master/code/rbm) 中找到，或者参考[官方教程](http://deeplearning.net/tutorial/deeplearning.pdf)。
>
> 另外，在我的[英文博客](https://stats.hohoweiya.xyz//statisticallearning/2017/08/17/Restricted-Boltzmann-Machines/)中有RBM的简要总结，欢迎提出宝贵建议:joy:
> @2017.08.27

离散变量的无向马尔科夫网络是很流行的，而且特别地，二值变量的成对马尔科夫网络更普遍。在统计力学领域有时称为Ising模型，在机器学习领域称为玻尔兹曼机 (Boltzmann machines)，其中顶点称为“结点(nodes)”或“单元(units)”，而且是取两个值的。

另外，每个结点处的值可以被观测到(visible)或者观测不到 (hidden)。结点通常以层来组织，类似神经网络。玻尔兹曼机对于非监督学习和监督学习都是很有用的，特别对有结构化输入数据，比如图像，但是也受到计算上的限制。图17.6显示了一个约束的玻尔兹曼机（后面讨论），其中一些变量是隐藏的，而且只有一些结点对是相连的。我们所有$p$个结点都是可见的简单情形，并且边的对$(j,k)$取自$E$。

用$X_j$标记结点$j$的二值变量，它们联合分布的Ising模型由下式给出
$$
p(X,\mathbf\Theta)=exp[\sum\limits_{(j,k)\in E}\theta_{jk}X_jX_k-\Phi(\mathbf{\Theta})]\text{  for  }X\in{\cal X}\tag{17.28}
$$
其中${\cal X}=\\{0,1\\}^p$。和上一节的高斯模型一样，只有成对交叉项才进行建模。Ising 模型是在统计力学中发展的，而且现在更一般地用来建立成对交叉的联合影响。$\Phi\mathbf{(\Theta)}$为分割函数的对数，而且由下式定义
$$
\Phi(\mathbf \Theta)=\mathrm{log}\sum\limits_{x\in\cal X}[\mathrm {exp}(\sum\limits_{(j,k)\in E}\theta_{jk}x_jx_k)]\tag{17.29}
$$

分割函数保证在样本空间中概率相加起来为1。$\theta_{jk}X_jX_k$项表示（对数）势函数（17.5）的特定参量化，而且因为技术上的原因需要包含常数结点$X_0\equiv 1$（练习17.10），与其他结点都有边相连。在统计领域中，这个模型等价于多路计算表（multiway tables of counts）的一阶交叉泊松对数线性模型（Bishop等人，1975；McCullagh和Nelder,1989;Agresti，2002）。

Ising模型表示每个结点在其它结点条件下的逻辑斯蒂形式（练习17.11）
$$
Pr(X_j=1\mid X_{-j}=x_{-j})=\frac{1}{1+exp(-\theta_{j0}-\sum_{(j,k)\in E})\theta_{jk}x_k}\tag{17.30}
$$
其中$X_{-j}$记除了$j$的所有结点。因此参数$\theta_{jk}$衡量了在给定其它结点条件下，$X_j$在$X_k$上的依赖性。

## 当图结构已知时估计参数

从模型中给出一些数据，我们可以怎么估计参数？假设我们有观测$x_i=(x_{i1},x_{i2},\ldots,x_{ip})\in\\{0,1\\}^p,i=1,\ldots,N$。对数似然为
$$
\begin{align}
\ell(\mathbf\Theta)&=\sum\limits_{i=1}^N\mathrm{log\; Pr}_{\mathbf\Theta}(X_i=x_i)\\
&=\sum\limits_{i=1}^N\Big[\sum\limits_{(j,k)\in E}\theta_{jk}x_{ij}x_{ik}-\Phi(\mathbf\Theta) \Big]\tag{17.31}
\end{align}
$$
对数似然的梯度为
$$
\frac{\partial\ell(\mathbf\Theta)}{\partial\theta_{jk}}=\sum\limits_{i=1}^Nx_{ij}x_{ik}-N\frac{\partial \Phi(\mathbf\Theta)}{\partial\theta_{jk}}\tag{17.32}
$$
并且
$$
\begin{align}
\frac{\partial \Phi(\mathbf\Theta)}{\partial \theta_{jk}}&=\sum\limits_{x\in\cal X}x_jx_k\cdot p(x,\mathbf\Theta)\\
&=E_{\mathbf\Theta}(X_jX_k)\tag{17.33}
\end{align}
$$
令梯度等于0得到
$$
\hat E(X_jX_k)-E_{\mathbf \Theta}(X_jX_k)=0\tag{17.34}
$$
其中我们定义
$$
\hat E(X_jX_k)=\frac{1}{N}\sum\limits_{i=1}^Nx_{ij}x_{ik}\tag{17.35}
$$
关于数据的期望分布取期望。注意(17.34)式，我们看到极大似然估计简单地把结点之间的内积估计和观测的内积匹配上了。这是指数族模型得分（梯度）等式的标准形式，令充分统计量等于模型下的期望。

为了寻找极大似然估计，我们可以利用梯度搜索或者牛顿法。然而$E_{\mathbf \Theta}(X_jX_k)$涉及$p(X,\mathbf\Theta)$在$X$的$\vert{\cal X}\vert=2^p$种可能值中的$2^{p-2}$ （？？？）种情形的列举，而且对于大的$p$一般是不可行的（比如，大于30）。对于小$p$，有一系列标准的统计方法可以使用：

- **泊松对数线性建模**， 我们将问题看成是大型回归问题（练习17.12）。响应变量$\mathbf y$是数据的多路表合的每个单元中的$2^p$个计数的向量。预测矩阵$\mathbf Z$有$2^p$个行，最多$1+p+p^2$个表征每个单元的列，尽管这个数字取决于图的稀疏程度。计算代价本质是该规模下回归问题的代价，计算代价为$O(p^42^p)$并且对于$p<20$是可行的。牛顿更新一般通过迭代重赋权最小二乘发发类似计算，并且步数通常是在个位数之内。详见Agresti(2002)和McCullagh和Nelder(1989)。标准软件（比如R语言的glm包）可以用来拟合这个模型。
- **梯度下降**至多需要$O(p^22^{p-2})$的计算量来计算梯度，但是可能回避二阶牛顿法需要更多的梯度步数。然而，可以处理$p\le 30$的稍微大点的问题。这些计算可以通过发现稀疏图中特别的团结构，采用junction-tree算法进行降低计算量。这里没有给出细节。
- **迭代比例过滤**(IPF)在梯度式(17.34)中采用柱坐标系。每一步更新一个参数使得梯度等式正好为0。这在柱坐标系中进行直到所有梯度为0。一个完整的周期与梯度赋值花费同样的计算量，但是可能更有效。Jirouśek和Přeučil (1995)采用junction trees实现了IPF的一个有效版本。

当$p$大（>30）可以采用其它的方法来近似梯度

- 均值域近似（Peterson和Anderson，1987）用$E_{\mathbf\Theta}(X_j)E_{\mathbf \Theta}(X_k)$来估计$E_{\Theta}(X_jX_k)$，并且将输入变量用它们的均值替换，得到一系列关于参数$\theta_{jk}$的非线性等式。
- 为了得到近似解，使用吉布斯采样（8.6节）来近似$E_{\mathbf\Theta}(X_jX_k)$，通过从估计模型$Pr_{\mathbf\Theta}(X_j\mid X_{-j})$来逐步抽样（见Ripley(1996)）

我们没有讨论可分解模型，因为极大似然估计不需要任何迭代可以在闭形式中找到。举个例子，这些模型来自树：截断树拓扑的特殊图。当要关心计算易处理性，树表示一个有用的模型类，并且它们回避本节中提出的计算上的考虑。详见Whittaker(1990)的第12章的例子。

##　隐藏结点

我们可以通过包含潜在或隐藏结点来增加离散马尔科夫网络的复杂性。假设变量的子集$X_{\cal H}$是未观测的或者“隐藏的”，并且剩余的变量$X_{\cal V}$是观测的或者“可见的”。则观测数据的对数似然为
$$
\begin{align}
\ell(\mathbf\Theta)&=\sum\limits_{i=1}^N\mathrm{log}[Pr_{\mathbf\Theta}(X_{\cal V}=x_{i\cal V})]\\
&=\sum\limits_{i=1}^N[\mathrm{log}\sum\limits_{x_{\cal H}\in{\cal X_H}}exp\sum\limits_{(j,k)\in E}(\theta_{jk}x_{ij}x_{jk}-\Phi(\mathbf\Theta))]\tag{17.36}
\end{align}
$$
对$x_{\cal H}$求和意味着我们对隐藏单元的所有可能$\{0,1\}$值进行求和。梯度为
$$
\frac{d\ell(\mathbf\Theta)}{d\theta_{jk}}=\hat E_{\cal V}E_{\mathbf \Theta}(X_jX_k\mid X_{\cal V})-E_{\mathbf\Theta}(X_jX_k)\tag{17.37}
$$
第一项是当$X_jX_k$都可见是的经验均值；如果其中一个或都隐藏，则在给定可见数据下进行第一次插值，然后在隐藏变量上平均。第二项是$X_jX_k$的无条件期望。

第一项中的内层期望可以用条件期望的基本法则和伯努利随机变量的性质进行赋值。具体地，对于观测$i$，
$$
E_{\mathbf\Theta}(X_jX_k\mid X_{\cal V}=x_{i\cal V}) =
\left\{
\begin{array}{ll}
x_{ij}x_{ik}& \text{if }j,k\in\cal V\\
x_{ij}Pr_{\cal\mathbf\Theta}(X_k=1\mid X_{\cal V}=x_{i\cal V})&\text{if }j\in\cal V,k\in\cal H\\
Pr_{\mathbf\Theta}(X_j=1,X_k=1\mid X_{\cal V}=x_{i\cal V})&\text{if }j,k\in{\cal H}(17.38)
\end{array}
\right.
$$
现在需要进行两个独立的吉布斯采样；第一个是从上面的模型中采样估计$E_{\mathbf \Theta}(X_jX_k)$，第二个是估计$E_{\mathbf\Theta}(X_jX_k\mid X_{\cal V}=x_{i\cal V})$。对于后者，固定（“夹住”）可见单元为观测值，并且仅仅对隐藏变量进行取样。吉布斯采样必须对训练集中的每一个观测进行，在梯度搜索的每一阶段。结果是这个过程可以变得非常慢，甚至对于中等大小的模型也是如此。在17.4.4节我们考虑进一步的模型约束来使得计算可行。

## 图结构的估计

Lee等人(2007)和Wainwright等人(2007)建议使用二值成对马尔科夫网络的lasso惩罚。第一个作者研究含惩罚的对数似然的精确最小化的共轭梯度过程。瓶颈是梯度中$E_{\mathbf\Theta}(X_jX_k)$的计算；通过junction tree 算法的精确计算对与稀疏图是可行的但是对稠密的图变得难以处理。

第二个作者提出近似的解法，类似对于高斯图模型的Meinshausen和Bühlmann（2006）的方法。它们将每个结点看成其他结点的函数的$L_1$惩罚逻辑斯蒂回归，接着以某种方式对称边的参数。举个例子，如果$\hat\theta_{jk}$为从输出变量为结点$j$的逻辑斯蒂回归模型中得到的$j-k$边参数的估计，“最小”的对称化设定$\hat\theta_{jk}$为$\hat\theta_{jk}$或$\hat\theta_{kj}$中绝对值最小的，类似地定义“最大”准则。他们证明在确定条件下，随着样本大小变为无穷大，两种近似都估计正确的非零边。Hoefling和Tibshirani(2008)将图lasso扩大为离散马尔科夫网络，得到比共轭梯度稍快的过程，但是依旧必须处理$E_{\mathbf\Theta}(X_jX_k)$的计算。他们也在广泛的模拟研究中比较精确和近似解，并且找到“最小”和“最大”近似仅仅比精确解略微不精确，对于估计非零边和估计边参数的真实值都是这样，而且快很多。此外，他们可以处理稠密图的情形因为他们不需要计算$E_{\mathbf\Theta}(X_jX_k)$.

最后，我们指出高斯和二值模型的关键区别。在高斯情况下，$\mathbf\Sigma$和其逆经常都是感兴趣的，而且图lasso过程都实现了这两个的估计。然而，Meinshausen和Bühlmann (2006)对于高斯图模型的近似类似Wainwright等人（2007）对二值情形的近似，仅仅得到$\mathbf\Sigma^{-1}$的估计。相反地，在二值数据的马尔科夫模型中，$\mathbf\Theta$是我们感兴趣的，而它的逆不是我们感兴趣的。Wainwright等人（2007）近似的方法有效地估计$\mathbf\Theta$，因此是二值问题的吸引人的解。

## 限制玻尔兹曼机

这节我们考虑受神经网络影响的一种特殊的图模型结构，该结构中，单元是按层进行组织的。限制玻尔兹曼机(RBM)包含一层可见单元和一层隐藏单元，单层之间没有联系。如果隐藏单元的连接被移除掉，计算条件期望变得很简单（如17.37和17.38）。

!!! note "weiya 注："
		这样（17.38）式变成，对于观测$i$，
		$$
		E_{\Theta}(X_jX_k\mid X_{\cal V}=x_{i\cal V})=
		\left\{
			\begin{array}{ll}
			x_{ij}x_{ik}&\text{if }j,k\in\cal V\\
			Pr_{\Theta}(X_j=1,X_k=1\mid X_{\cal V}=x_{i\cal V})&\text{if }j,k\in\cal H
			\end{array}
			\right.
		$$

图17.6显示了一个例子；可见层被分成了输入变量$\cal V_1$和输出变量$\cal V_2$，且有一个隐藏层$\cal H$。我们将这样一个网络表示成

$$
{\cal V}_1\leftrightarrow {\cal H\leftrightarrow\cal V}_2\tag{17.39}
$$

![](../img/17/fig17.6.png)

> 图17.6. 同一层中的结点没有连接的约束玻尔兹曼机（RBM）。可见单元细分成了$\cal V_1$和 $\cal V_2$，允许RBM建立特征$\cal V_1$和标签$\cal V_2$之间的联合密度。

举个例子， $\cal V_1$可以是手写字体图像的二值像素，$\cal V_2$可以有10个单元，每个单元是观测的类别标签0-9中的一个。

这个模型的约束形式简化了估计(17.37)中期望的吉布斯采样，因为每一层的变量在给定其它层中的变量时独立于其它变量。因此它们可以采用由式（7.30）给出的条件概率一起取样。

最终的模型没有玻尔兹曼机一般，但是仍然很有用；举个例子，可以经过学习从图像中提取有趣的特征。

通过对图17.6中RBM的每一层中的变量进行交替取样，从联合密度模型中产生样本是可能的。如果可见层的$\cal V_1$部分在交替取样时固定为特定的特征向量，从给定$\cal V_1$的标签分布中取样是可能的。另外测试项的分类也可以通过比较观测到的特征和每个类别标签的未标准化联合密度来实现。我们不需要计算分割函数因为对于所有的组合都是一样的（？？？？）。

正如所注意到的约束玻尔兹曼机有与单层隐藏层神经网络一样的一般形式（11.3节）。神经网络中边是有向的，隐藏单元经常是实值的，并且拟合准则不一样。神经网络在输入特征的情况下，最小化目标与模型预测之间的误差（交叉熵）。相反地，约束玻尔兹曼机最大所有可见单元（也就是，特征和目标）的联合分布的对数似然。可以从输入特征中提取对预测标签有用的信息，但是，与监督学习方法不同，可能也使用一些隐藏单元对特征向量中的结构进行建模，而这与预测标签不是直接相关的。然而，这些特征当与从隐藏层中导出的特征结合起来会变得有用。

不幸的是，约束玻尔兹曼机中的吉布斯采样会非常慢，因为需要花费很长时间达到稳态。当网络参数变得更大，这些链混合得更慢，并且我们需要更多的步骤来得到无条件的估计。Hinton(2002)根据经验注意到当我们通过在数据中启动马尔科夫链并仅仅运行几步（不是直到收敛）来估计（17.37）中的第二个期望，学习仍然有用。他称这个为对比发散(contrastive divergence)：我们给定$\cal V_1,V_2$对 $\cal H$ 取样，然后给定 $\cal H$对 $\cal V_1,V_2$取样，最后再次给定$\cal V_1,V_2$对$ \cal H$取样。思想是，当参数与解相差很多，迭代吉布斯取样达到问题是浪费的，因为仅仅一个迭代就是发现改变估计的一个好方向。

!!! note "weiya 注"
		![](../img/17/hvGibbs.png)
		截图自[http://deeplearning.net/tutorial/](http://deeplearning.net/tutorial/)

我们现在给出说明RBM用法的一个例子。采用对比发散(CD)，训练RBM来识别MNIST数据集中的手写字体是可能的（LeCun等人，1998）。2000个隐藏单元，784个表示二值像素强度的单元，以及表示标签的10-way multinomial（weiya注：取值为0-9）可见单元，RBM可以在测试集值达到1.9%的错误率。这比支持向量机达到的1.4%略高一点，与通过向后传播训练的神经网络得到的误差率相当。然而，用500个从图像中得到的不含任何标签信息的特征来替换784个像素强度，RBM的误差率会降至1.25%。首先，采用对比发散训练包含784个可见单元和500个隐藏单元的RBM，来建立图像集的模型。然后第一个RBM的隐藏状态作为训练第二个RBM的数据，第二个RBM含500个可见单元和500个隐藏单元。最后，第二个RBM的隐藏状态用作训练含2000个隐藏单元的RBM的输入特征。这种以贪婪、逐层的方式来学习特征的细节及证明在Hinton等人（2006）中给出。图17.7给出了以这种方式学习的复合模型的一个表示，也显示了一些它能够应付的变形的字体的例子。

![](../img/17/fig17.7.png)

> 图17.7 约束玻尔兹曼机处理手写字体数字的例子。网络用左边的原理图来描述。右边显示了一些该模型分类正确的难的测试图像。
