# 光滑参数的自动选择

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-12-06                               |


回归样条的光滑参数包含样条的degree，结点的数量及位置。对于光滑样条，我们只有惩罚参数$\lambda$可以选择，因为结点是所有不同的$X$取值，并且在实际中经常使用三次degree。

选择回归样条结点的位置及数目是一项复杂的组合任务，除非强制一些简化。第9章的MARS过程采用贪婪算法以及额外的近似来达到实际的妥协。这里我们不会深入讨论。

## 固定自由度

因为对于光滑样条而言，$df_\lambda=trace(\mathbf S_\lambda)$关于$\lambda$单调，这个关系是可逆的，并且通过固定自由度来确定$\lambda$。实际中，这可以通过简单的数值方法实现。举个例子，在`R`中，可以采用`smooth.splin(x,y,df=6)`来确定光滑的程度。这鼓励更加传统的模型选择的模式，其中我们可能尝试一系列不同的自由度，然后根据近似的$F$检验，残差图以及其它更客观的准则来选择一个。通过这种方式使用自由度提供了更加统一的方式来比较许多不同的光滑方法。在广义可加模型(generalized additive models)中特别有用（第9章），其中多种光滑方法可以同时应用在一个模型中。

## 偏差-方差的权衡

图5.9显示了在如下简单的模型中应用光滑样条$df_\lambda$的选择的效果：

$$
\begin{array}{ccc}
Y& =&f(X)+\varepsilon\\
f(X)&=&\frac{sin(12(X+0.2))}{X+0.2}
\end{array}
\qquad (5.22)
$$

其中$X\sim U[0,1], \varepsilon\sim N(0, 1)$。我们的训练样本包含从该模型中独立抽取的$N=100$个数据对$x_i,y_i$。

![](../img/05/fig5.9.png)

图中显示了3个不同$df_\lambda$的拟合样条。图中黄色阴影部分表示$\hat f_\lambda$的逐点标准误差，也就是，我们将区域$\hat f_\lambda(x)\pm 2\cdot se(\hat f_\lambda(x))$画成阴影。因为$\hat{\mathbf f}=\mathbf S_\lambda \mathbf y$，

$$
Cov(\hat{\mathbf f})=\mathbf S_\lambda Cov(\mathbf y)\mathbf S_\lambda^T=\mathbf S_\lambda\mathbf S_\lambda^T\qquad (5.23)
$$

对角元包含训练点$x_i$的逐点方差。偏差由下式给出

$$
Bias(\hat{\mathbf f})=\mathbf f-E(\hat{\mathbf f})=\mathbf f-\mathbf S_\lambda \mathbf f\qquad (5.24)
$$

其中$\mathbf f$是真实的$f$在训练点$X$处的取值得到的向量（未知）。期望和方差是关于从模型(5.22)中重复取样得到的大小为$N=100$的样本。类似地，$Var(\hat f_\lambda(x_0))$和$Bias(\hat f_\lambda(x_0))$可以在任意点$x_0$处计算得到（练习5.10）。图中的这三个拟合直观地展示了与选择光滑参数有关的偏差-方差权衡。
