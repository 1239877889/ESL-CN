# 光滑参数的自动选择

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-12-06                               |


回归样条的光滑参数包含样条的degree，结点的数量及位置。对于光滑样条，我们只有惩罚参数$\lambda$可以选择，因为结点是所有不同的$X$取值，并且在实际中经常使用三次degree。

选择回归样条结点的位置及数目是一项复杂的组合任务，除非强制一些简化。第9章的MARS过程采用贪婪算法以及额外的近似来达到实际的妥协。这里我们不会深入讨论。

## 固定自由度

因为对于光滑样条而言，$df_\lambda=trace(\mathbf S_\lambda)$关于$\lambda$单调，这个关系是可逆的，并且通过固定自由度来确定$\lambda$。实际中，这可以通过简单的数值方法实现。举个例子，在`R`中，可以采用`smooth.splin(x,y,df=6)`来确定光滑的程度。这鼓励更加传统的模型选择的模式，其中我们可能尝试一系列不同的自由度，然后根据近似的$F$检验，残差图以及其它更客观的准则来选择一个。通过这种方式使用自由度提供了更加统一的方式来比较许多不同的光滑方法。在广义可加模型(generalized additive models)中特别有用（第9章），其中多种光滑方法可以同时应用在一个模型中。

## 偏差-方差的权衡

图5.9显示了在如下简单的模型中应用光滑样条$df_\lambda$的选择的效果：

$$
\begin{array}{ccc}
Y& =&f(X)+\varepsilon\\
f(X)&=&\frac{sin(12(X+0.2))}{X+0.2}
\end{array}
\qquad (5.22)
$$

其中$X\sim U[0,1], \varepsilon\sim N(0, 1)$。我们的训练样本包含从该模型中独立抽取的$N=100$个数据对$x_i,y_i$。

![](../img/05/fig5.9.png)

图中显示了3个不同$df_\lambda$的拟合样条。图中黄色阴影部分表示$\hat f_\lambda$的逐点标准误差，也就是，我们将区域$\hat f_\lambda(x)\pm 2\cdot se(\hat f_\lambda(x))$画成阴影。因为$\hat{\mathbf f}=\mathbf S_\lambda \mathbf y$，

$$
Cov(\hat{\mathbf f})=\mathbf S_\lambda Cov(\mathbf y)\mathbf S_\lambda^T=\mathbf S_\lambda\mathbf S_\lambda^T\qquad (5.23)
$$

对角元包含训练点$x_i$的逐点方差。偏差由下式给出

$$
Bias(\hat{\mathbf f})=\mathbf f-E(\hat{\mathbf f})=\mathbf f-\mathbf S_\lambda \mathbf f\qquad (5.24)
$$

其中$\mathbf f$是真实的$f$在训练点$X$处的取值得到的向量（未知）。期望和方差是关于从模型(5.22)中重复取样得到的大小为$N=100$的样本。类似地，$Var(\hat f_\lambda(x_0))$和$Bias(\hat f_\lambda(x_0))$可以在任意点$x_0$处计算得到（练习5.10）。图中的这三个拟合直观地展示了与选择光滑参数有关的偏差-方差权衡。

- $df_\lambda=5$: 样条欠拟合，并且很明显坡顶被削减波谷被充填。这导致在高曲率的地方有更显著的偏差。标准误差带非常狭窄，所以我们估计了一个相当稳定的真实函数的较大偏差版本。
- $df_\lambda=9$: 这里拟合函数接近真实函数，尽管少量的偏差看起来很明显。方差没有明显地增长。
- $df_\lambda=15$: 拟合函数有点弯曲，但是接近真实函数。这种弯曲也意味着标准误差带宽度的增长——曲线开始紧密地跟随一些个体点

注意到在这些图中我们看到的是每个情形下数据的单个实现以及因此得到的拟合样条$\hat f$的实现，尽管偏差涉及到期望$E(\hat f)$。我们将这留作练习(5.10)来计算类似的图形，其中也显示出偏差。中间的曲线似乎恰恰实现了偏差与方差直接的平衡。

积分平方预测误差(EPE)在一个总结式中结合了偏差和方差：

$$
\begin{array}{rcl}
EPE(\hat f_\lambda) &=&E(Y-\hat f_\lambda(X))^2\\
&=& Var(Y)+E[Bias^2(\hat f_\lambda(X))+Var(\hat f_\lambda(X))]\\
&=& \sigma^2+MSE(\hat f_\lambda)\qquad (5.25)
\end{array}
$$

注意到这是在训练样本（得到$\hat f_\lambda$）和（独立选择的）预测点$(X,Y)$的值中进行平均。EPE是一个很自然感兴趣的量，而且确实在偏差和方差直接创造平衡。图5.9的左上图中的蓝点表明$df_\lambda=9$正好！

因为我们不知道真实函数，也无法得到EPE，并且需要一个估计。这个话题将在第7章中详细展开，并且像$K$折交叉验证，GCV和$C_p$都是通常使用的。在图5.9中我们包含了一个$N$折（舍一法）交叉验证的曲线：

$$
\begin{array}{rcl}
CV(\hat f_\lambda) &=&\frac 1N\sum_limits_{i=1}^N(y_i-\hat f_\lambda^{(-i)}(x_i))^2\qquad (5.26)\\
&=& \frac 1N\sum\limits_{i=1}^N(\frac{y_i-\hat f\lambda (x_i)}{1-S_\lambda(i, i)})\\
\end{array}
$$

上式可以从原始的拟合数据中计算每个$\lambda$值以及$\mathbf S_\lambda$的对角元素（练习5.13）

EPE和CV曲线有着相同的形状，但是整个CV曲线都是在EPE的上面。对于有些正则化，则相反，并且总体来看，CV曲线作为EPE曲线的估计是近似无偏的。
