# 18.4 $L_1$正则的线性分类器

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-04-27:2017-04-27                    |
|更新|2018-04-29|
|状态|In Progress|

[18.3 节](../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization/index.html)的方法使用 $L_2$ 惩罚来对参数进行正则化，正如在岭回归中一样。所有的估计参数是非零的，因此没有用到特征选择。在这一节我们讨论采用 $L_1$ 惩罚的正则，也因此提供了自动的特征选择。

!!! note "weiya 注"
		特征选择意味着选择变量（特征），在 $L_2$ 正则化中，没有参数的系数为 0，也就是没有剔除任何参数，所以说没有用到特征选择；而在 $L_1$ 正则中，存在参数的系数为 0，也就是会对特征进行选择。换句话说，特征选择的效果就是取部分的特征，于是那部分的特征所对应的系数为0。

回忆 [3.4.2 节](../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html)的 lasso，

$$
\underset{\beta}{\min}\frac{1}{2}\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\vert\beta_j\vert\qquad (18.18)
$$

我们写成了 (3.52) 的拉格朗日形式。这里讨论的，使用充分大的调整参数 $\lambda$，使用 $L_1$ 惩罚导致部分的解参数$\hat\beta_j$ 恰恰为 0。

在 [3.8.1 节](../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html)我们已经讨论了 LARS 算法，LARS 算法是个计算所有的 $\lambda$ 的 lasso 解的有效过程。当 $p>N$ 时（正如本章中），当 $\lambda$ 趋于 0，lasso 精确拟合了训练数据。事实上，通过凸对偶，可以证明当 $p>N$时，对于所有 $\lambda$，非零参数的个数至多为 $N$（举个例子，Rosset and Zhu, 2007[^1]）。因此 lasso 提供了特征选择的形式。

lasso 回归可以通过对输出编码为 $\pm 1$ 应用到两类别分类问题中，并且对预测变量加上一个截距（通常为 0）。对于多余两个的类别，有多种可能的方式，包括在 [18.3.3 节](/18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization/index.html)讨论的 ova 和 ovo 方法。我们在 [18.3 节](/18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization/index.html)中对癌症数据应用 ova 方法。结果显示在表 18.1 的第 4 行中。它的效果是最好的。

分类问题的更自然的方式是使用 lasso 惩罚来正则化逻辑斯底回归。文献中已经提出一些实现方法，包括类似 LARS（Park and Hastie，2007[^2]）的路径算法。因为路径是分段光滑但是非线性，精确的算法比 LARS 算法更慢，并且当 $p$ 很大时不是很适用。

Friedman et al. (2010)[^3] 提供了用于拟合 $L_1$ 惩罚的逻辑斯底和多项式回归模型。他们应用 18.3.2 节中 (18.10) 中的对称多项式逻辑斯底回归模型，并且最大化带惩罚的对数似然

$$
\underset{\{\beta_{0k},\beta_k\in R^p\}_1^K}{\max}[\sum\limits_{i=1}^N\log\;\Pr(g_i\mid x_i)-\lambda\sum\limits_{k=1}^K\sum\limits_{j=1}^p\vert\beta_{kj}\vert]\qquad (18.19)
$$

这与式子 (18.11) 相对应。他们的算法通过**坐标轮换 (cyclical coordinate descent)**（[3.8.6 节](../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms/index.html)），并且利用 $p>>N$ 时解是稀疏的事实，以及 $\lambda$ 邻域的解趋向于非常相似。这个方法用在表 18.1 的第 7 行中，通过交叉验证来选取全局调整参数 $\lambda$。它的表现与最优方法很相似，除了自动特征选择一起选择了 269 个基因。类似的方法在 Genkin et. al (2007)[^5] 的工作中使用了；尽管它们从贝叶斯角度提出它们的模型，事实上他们计算后验概率，解决了惩罚的最大似然问题。


![](../img/18/fig18.5.png)

> 白血病数据的正则化逻辑斯底回归路径。左图是 lasso 路径，右图是 $\alpha=0.8$ 的弹性网路径。在路径的终端（最左端），lasso 有19 个非零系数，弹性网有 39 个非零系数。弹性网的平均效应导致比 lasso 更多的非零系数，但是规模更小。

在基因应用中，变量间通常有强相关关系；基因趋向于在分子通路上起作用。lasso 惩罚在某种程度上与不受强相关的变量集的选择的影响（[练习 3.28](https://github.com/szcf-weiya/ESL-CN/issues/123)）。另一方面，岭惩罚趋向于将相关变量的系数收缩（[练习 3.29](https://github.com/szcf-weiya/ESL-CN/issues/124)）。**弹性网 (elastic net)**惩罚 (Zou and Hastie, 2005[^4]) 是一种妥协的方式，并且有形式

$$
\sum\limits_{i=1}^p(\alpha\vert\beta_j\vert+(1-\alpha)\beta_j^2)\qquad (18.20)
$$

第二项鼓励高相关的特征进行平均，而第一项促进在平均特征系数中的稀疏解。弹性网惩罚可以用到任何线性模型中，特别是用于回归或分类中。

因此上述的带弹性网惩罚的多项式问题变成

$$
\underset{\{\beta_{0k,\beta_k\in R^p}\}_1^K}{max}[\sum\limits_{i=1}^NlogPr(g_i\mid x_i)-\lambda\sum\limits_{k=1}^K\sum\limits_{j=1}^p(\alpha\vert\beta_{kj}\vert)+(1-\alpha)\beta_{kj}^2]\qquad (18.21)
$$

参数 $\alpha$ 确定了惩罚的混合，并且经常在 qualitative grounds 上预先选取。当 $p>N$ 时，弹性网得到多于 $N$ 个的非零参数，这是相比于 lasso 的潜在优点。表 18.1 的第 8 行采用这个模型，通过交叉验证选择 $\alpha$ 和 $\lambda$。我们采用 0.05 到1.0 之间的 20 个 $\alpha$ 值，以及在整个值域的对数尺度下均匀分布的 100 个 $\lambda$ 值。位于 $\alpha\in [0.75,0.80]$
 之间的值给出了最小的 CV 误差，对于所有的 tied solution，$\lambda<0.001$。尽管在所有方法中有最低的测试误差，但是边界分布（？）是小的并且不显著。有趣的是，当 CV 对每个 $\alpha$ 单独计算，在 $\alpha=0.10$ 时达到最小的测试误差，但是这不是在 2 维 CV 中选择的值。

![](../img/18/fig18.6.png)

TODO

## 应用 lasso 的方法到蛋白质质谱

TODO

## 对于功能型数据的 Fused Lasso

TODO

[^1]: Rosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths, Annals of Statistics 35(3): 1012–1030.
[^2]: Park, M. Y. and Hastie, T. (2007). l1-regularization path algorithm for generalized linear models, Journal of the Royal Statistical Society Series B 69: 659–677.
[^3]: Friedman, J., Hastie, T. and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent, Journal of Statistical Software 33(1): 1–22.
[^4]: Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net, Journal of the Royal Statistical Society Series B. 67(2): 301–320.
[^5]: Genkin, A., Lewis, D. and Madigan, D. (2007). Large-scale Bayesian logistic regression for text categorization, Technometrics 49(3): 291–304.