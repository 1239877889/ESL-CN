# 16.2 boosting和正则化路径

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-03-09                               |

在这本书第一版的第 10.12.2 节中，我们表明了由梯度加速算法产生的模型序列与在高维特征空间中拟合的正则模型是相似的。这最初是通过观测到 boosted 的线性模型和 lasso 之间（[第 3.4.2 节](../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html)）有着紧密联系得到的。我们和其他人一起研究这些联系，并且在这里放上我们对这个领域当前的思考。我们从最初的动机开始，这更自然地适合于本章的集成学习。

## 惩罚回归

**梯度加速 (gradient boosting)** 的收缩策略 (10.41) 的成功直觉上可以通过类比含有很多基展开的惩罚线性回归得到。

!!! note "Recall"
    $$
    f_m(x)=f_{m-1}(x)+\nu\cdot \sum\limits_{j=1}^J\gamma_{jm}I(x\in R_{jm})\tag{10.41}
    $$

考虑所有可能的 $J$ 个终止结点的回归树 $\cal T=\{T_k\}$ 的字典作为 $\IR^p$ 的基函数，这在训练数据上可以实现。线性模型为
$$
f(x)=\sum\limits_{k=1}^K\alpha_kT_k(x)\qquad (16.1)
$$

其中$K=card(\cal T)$。假设系数通过最小二乘进行估计。因为这些树的个数可能比最大训练集大很多，所以需要一些正则化。令 $\hat\alpha(\lambda)$ 使得

$$
\underset{\alpha}{\min}\{\sum\limits_{i=1}^N(y_i-\sum\limits_{k=1}^K\alpha_kT_k(x_i))^2+\lambda\cdot J(\alpha)\}\qquad (16.2)
$$

$J(\alpha)$ 是关于系数的函数，并且一般惩罚较大的值。比如

$$
\begin{align}
J(\alpha) &=\sum\limits_{k=1}^K\vert \alpha_k\vert^2\qquad \text{岭回归}\qquad (16.3)\\
J(\alpha) &=\sum\limits_{k=1}^K\vert\alpha_K\vert\qquad\text{lasso}\qquad (16.4)
\end{align}
$$

这些都在 [3.4 节](../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html)介绍了。正如这里讨论的那样，从中等大的 $\lambda$ 到较大的 $\lambda$ 的 lasso 问题的解是趋向于稀疏的；许多 $\hat\alpha_k(\lambda)=0$。也就是，只有所有候选树的一小部分会选进模型 (16.1)。这似乎是合理的，因为似乎只有所有候选树的一小部分对任意特定的目标函数的近似是恰当的。然而，对于不同的目标值，相关的子集也会不一样。

!!! note "weiya 注："
    换句话说，对于某个 $y_i$，与该目标相关的候选树的子集为 $C_i$，这会随着目标值的不同而发生变化。

那些没有设为 $0$ 的系数被 lasso 收缩了，因为它们的绝对值比对应最小二乘值要小：$\vert\hat\alpha_k(\lambda)\vert < \vert\hat\alpha_k(0)\vert$。当 $\lambda$ 增大时，系数都进行了收缩，每一个最终都变为 $0$。

由于非常多的基函数 $T_k$，直接求解含有 lasso 惩罚 (16.4) 的 (16.2) 是不可能的。然而，存在可行的向前逐步策略，对 lasso 的效果有非常好的近似，并且与增强和向前逐步算法 10.2 非常相似。算法 16.1 给出了细节。尽管是用树的基函数 $T_k$ 表示的，算法也可以用来任意基函数的集合中。第 $1$ 行所有系数初始化为 $0$；这对应了 (16.2) 式的 $\lambda=\infty$。 每个接下来的步骤中，第 2(a) 行选择对当前残差拟合得最好的树 $T_{k^\*}$。其对应的系数 $\check{\alpha}\_{k^\*}$ 接着在第 2(b) 步增长或减小一个无穷小量，而所有其他的系数 $\check{\alpha}_{k\neq k^*}$ 都保持不变。原则上，这个过程可以一直迭代到所有的残差为 0，或者 $\beta^\*=0$。第二种情况会发生在 $K <N$ 的情形下，并且在那时候，参数值表示最小二乘的解。这对应了 (16.2) 式的 $\lambda=0$。

![](../img/16/alg16.1.png)

对算法 16.1 迭代 $M < \infty$ 次后，许多系数会变成0，也就是，它们不会再增长。其他的系数的绝对值会趋向于比对应的最小二乘解来得小，$\vert \check{\alpha}_k(M)\vert<\vert \hat{\alpha}_k(0)\vert$。因此，这个 $M$ 次迭代的解定性地来看，表现得像是 lasso，其中 $M$ 与 $\lambda$ 成反比。

![](../img/16/fig16.1.png)

图 16.1 展示了一个例子，采用[第 3 章](../03-Linear-Methods-for-Regression/3.1-Introduction/index.html)研究的前列腺数据。这里，不是采用树 $T_k(X)$ 作为基函数，而是采用原始变量 $X_k$ 本身；也就是，多元线性回归模型。左图展示了在不同的参数上界 $t=\sum_k\vert \alpha_k\vert$ 情形下，lasso 估计的系数的曲线。右图则显示了算法 16.1 逐步的结果，其中 $M=250，\epsilon=0.01$。【图 16.1 的左右图分别与图 3.10 和图 3.19 的右图一样。】两幅图的相似性令人震惊。

在一些情形下，相似性不仅仅是定量的。举个例子，如果所有的基函数 $T_k$ 互不相关，则当$\epsilon\downarrow 0, M\uparrow $，使得 $Mt\rightarrow t$，算法 16.1 得到在参数 $t=\sum_k\vert \alpha_k\vert$ 时的 lasso 一样的解（对于沿着路径的所有解也是一样的）。当然，基于树的回归因子不是不相关的。然而，当系数 $\hat\alpha_k(\lambda)$ 都是 $\lambda$ 的单调函数时，解的集合也是一样的。当变量间的相关性较低时，经常是这种情形。当 $\hat\alpha_k(\lambda)$ 不是关于 $\lambda$ 的单调函数，则解的集合不是相同的。算法 16.1 解的集合正则参数的值没有 lasso 改变得快。

Efron et al. (2004)[^1]通过在 $\varepsilon$ 极限的情形下特征化解的路径，精确地描述了这种联系。他们证明对于 lasso 和向前逐步法，系数的路径都是分段线性函数。这能够帮助提出高效的算法，使得整个路径仅以单个最小二乘拟合相同的花费来计算。这个最小角回归算法在 [3.8.1 节](../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms/index.html)有更多的细节描述。

Hastie et al. (2007)[^2]证明了这个无穷小量向前逐步算法（$FS_0$）拟合了 lasso 的单调版本，在给定系数路径的 **角长度 (arc length)** 的增长下，最优化了每一步的损失函数（参见 [16.2.3 节]()和 [3.8.1 节](../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms/index.html)）。对于 $\epsilon > 0$ 的情形角长度为 $M\epsilon$，也因此与步数成比例。

带收缩 (10.41) 的 boosting 树算法（算法 10.3）非常近似算法 16.1，其中学习速率参数 $\nu$ 对应 $\epsilon$。对于平方误差损失，唯一的区别在于每一次迭代时最优树的选取是通过标准的自上而下的贪婪生成树的算法。对于其他的损失函数，比如 AdaBoost 的指数损失和二项误差，Rosset et al. (2004a)[^3]展示了类似我们这里的结果。因此，可以将带收缩的boosting树看成在所有可能树（$J$个终止结点）上的单调ill-posed回归的形式，其中将lasso的惩罚作为正则化项。我们将在 [16.2.3 节]()继续讨论这个话题。

无收缩的选择（式子 (10.41) 中 $\nu=1$）类似向前逐步回归，并且其更 aggressive 的最优子集选择，其中对于非零系数的个数加上惩罚 $J(\alpha)=\sum_k\vert \alpha_k\vert^0$。对于少量的主要变量，最优子集方法经常取得很好的效果。但是在有较多强变量的情形下，众所周知，最优子集选择非常 greedy（Copas, 1983[^4]），与其他不太 aggressive 的算法，如 lasso 或岭回归相比，经常得到很差的结果。当采用 boosting 的收缩得到重大的改善是这种方式的又一个说明。

## "Bet on Sparsity"原则

TODO

## 正则化路径，过拟合和边缘

TODO

[^1]: Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression (with discussion), Annals of Statistics 32(2): 407–499.
[^2]: Hastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007). Forward stagewise regression and the monotone lasso, Electronic Journal of Statistics 1: 1–29.
[^3]: Rosset, S., Zhu, J. and Hastie, T. (2004a). Boosting as a regularized path to a maximum margin classifier, Journal of Machine Learning Research 5: 941–973.
[^4]: Copas, J. B. (1983). Regression, prediction and shrinkage (with discussion), Journal of the Royal Statistical Society, Series B, Methodological 45: 311–354.
