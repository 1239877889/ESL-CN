# 训练误差率的optimism

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-18:2017-02-18                    |
|校对|2017-09-17|

讨论误差率的估计可能令人困惑，因为我们必须弄清楚哪些量是固定的哪些量是随机的。在我们继续之前，我们需要一些定义，详细阐述第7.2节的内容。给出训练集$\cal T=\\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\\}$，模型$\hat f$的泛化误差为

$$
\mathrm{Err}_{\cal T} = E_{X^0,Y^0}[L(Y^0,\hat f(X^0))\mid \cal T]\qquad (7.15)
$$

注意到在表达式（7.15）中训练集$\cal T$是固定的。点$(X^0,Y^0)$是新的测试点，从数据的联合分布$F$中取的。对数据集平均得到期望误差

$$
\mathrm{Err} = \mathrm E_{\cal T}\mathrm E_{X^0,Y^0}[L(Y^0,\hat f(X^0))\mid \cal T]\qquad (7.16)
$$

它更适合于统计分析。正如之前提到的，事实证明大部分方法能够有效估计期望误差而非$E_{\cal T}$；这一点详见7.12节。

现在一般地，训练误差

$$
\overline{err} = \frac{1}{N}\sum\limits_{i=1}^NL(y_i,\hat f(x_i))\qquad (7.17)
$$

会比真实误差$\mathrm{Err}\_{\cal T}$小，因为数据被同时用来拟合方法并且评估误差（见练习2.9）。拟合方法一般适应于训练数据，因此训练误差$\overline{err}$是过度乐观估计的泛化误差$\mathrm {Err}\_{\cal T}$。

部分差异是因为取值点的选取。值$\mathrm {Err}_{\cal T}$可以看成是样本外（extra-sample）误差，因为测试输入向量不需要与训练输入向量一致。当我们去关注样本内（in-sample）误差，可以很简单地理解$\overline{err}$乐观估计的本质

$$
\mathrm{Err}_{in}=\frac{1}{N}\sum\limits_{i=1}^N\mathrm E_{Y^0}[L(Y_i^0,\hat f(x_i))\mid \cal T]\qquad (7.18)
$$

$Y^0$表示我们在每个训练点$x_i,i=1,2,\ldots,N$处观测$N$个新响应变量的值。我们定义$\mathrm{Err}_{in}$与训练误差$\overline{err}$的差为乐观（optimism）：

$$
\mathrm {op}\equiv \mathrm{Err}_{in}-\overline{err}\qquad (7.19)
$$

一般情形下这是正的，因为$\overline{err}$经常是预测误差的向下有偏估计。最后，平均乐观是在训练集上乐观的期望

$$
\omega \equiv E_{\mathbf y}(\mathrm{op})\qquad (7.20)
$$

这里训练集中的预测变量是固定的，期望在对训练集的输出值上的；因此我们已经用记号$E_{\mathbf y}$而不是$E_{\cal T}$。我们通常仅仅估计期望误差$\omega$而不是op，同样的方式我们可以估计期望误差Err而不是条件误差$\mathrm {Err}_{\cal T}$。

对于平方误差，0-1，以及其他损失函数，可以证明一般性的结论

$$
w=\frac{2}{N}\sum\limits_{i=1}^NCov(\hat y_i,y_i)\qquad (7.21)
$$

其中Cov表示协方差。因此$\overline{err}$低估真实误差的程度取决于$y_i$影响它本身估计的程度。数据越难拟合，$Cov(\hat y_i,y_i)$将越大，因此增大了乐观。练习7.4证明了平方损失函数时的结果，其中$\hat y_i$为根据回归拟合好的值。对于0-1损失，$\hat y_i\in\\{0,1\\}$是在$x_i$处的分类，另外对于熵损失，$\hat y_i\in[0,1]$是在$x_i$处类别1的拟合概率。

总结一下，我们有重要的关系式

$$
E_{\mathbf y}(Err_{in})=E_{\mathbf y}(\overline{err})+\frac{2}{N}\sum\limits_{i=1}^NCov(\hat y_i,y_i)\qquad (7.22)
$$

如果$\hat y_i$通过含$d$个输入或者基函数的线性拟合得到，上面表达式可以简化。例如，对于可加误差模型$Y=f(X)+\varepsilon$，

$$
\sum\limits_{i=1}^NCov(\hat y_i,y_i) = d\sigma_{\varepsilon}^2\qquad (7.23）
$$

因此

$$
E_{\mathbf y}(Err_{in})=E_{\mathbf y}(\overline{err})+2\cdot\frac{d}{N}\sigma_\varepsilon^2\qquad (7.24)
$$

表达式（7.23）是将在7.6节讨论的有效参数个数（effective number of parameters）的定义的基础。optimism随着我们使用的输入或基函数的个数$d$线性增长，但是当训练样本大小增大时会降低。（7.24）的其它版本对其它误差模型也近似成立，比如二进制数据和熵损失。

一种明显估计预测误差的方法是先估计optimism然后加到训练误差$\overline{err}$上。下一节将要描述的方法——$C_p$，AIC，BIC以及其它方法——对于估计关于参数是线性的特殊估计类，都是通过这种方式实现。

相反地，将在本章后面描述的交叉验证以及自助法是对样本外（extra-sample）误差Err直接估计的方法。这些一般工具可以用于任意损失函数以及非线性自适应拟合技巧。

样本内误差通常不是直接感兴趣的，因为特征的未来值不可能与它们训练集值一致。但是为了模型之间的比较，样本内误差是很方便的，并且经常能够有效地模型选择。原因在于误差的相对（而不是绝对）大小是我们所关心的。
