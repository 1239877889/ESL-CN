# 利用梯度boosting进行数值优化

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-10.07                              |

曾经，boosting被认为是结合模型的技巧，在这里模型是树。同样地，生成树的算法可看成是产生用于boosting进行结合的模型的原型。在这种情形下，在生成树的时候以通常的方式分别进行估计每棵树的最优大小（9.2节）。首先诱导出非常大（过大的）的一棵树，接着应用自下而上的过程剪枝得到估计的最优终止结点个数的树。这种方式隐含地假设了每棵树是式子(10.28)的最后一棵。

> $$
f_M(x)=\sum\limits_{m=1}^MT(x;\Theta_m)\qquad (10.28)
$$, 详见10.9节。

除了真正的最后一棵树，这显然是个很差的假设。结果是树都趋向于很大，特别是前几步迭代中。这显著降低了模型的效果，并且增大了计算量。

避免这个问题最简单的策略是限制所有树为相同的大小， $J_m=J\forall m。在每次迭代中，得到$J$个终止结点的回归树。因此$J$成为了整个boosting过程的元参数(meta-parameter)，根据手头上的数据来调整该参数以期最大化估计的效果。

可以通过考虑目标函数的性质得到$J$的有用值的想法

$$
\eta = \mathrm{arg }\underset{f}{\mathrm{min}} E_{XY}L(Y, f(X))\qquad (10.39)
$$

这里期望值是对$(X,Y)$的总体联合分布而言。目标函数$\eta(x)$是在未来数据上有最小预测风险的函数。这是我们试图近似的函数。

$\eta(X)$一个相关的性质是坐标变量$X^T=(X_1,X_2,\lots, X_p)$间交叉项的阶。这个可以通过它的ANOVA展开式得到

$$
\eta(X)=\sum\limits_{j}\eta_j(X_j)+\sum\limits_{jk}\eta_{jk}(X_j,X_k)+\sum\limits_{jkl}\eta_{jkl}(X_j,X_k, X_l)+\cdots\qquad (10.40)
$$

式子(10.40)中的第一项是只有一个预测变量$X_j$的函数和。这些函数$\eta_j(X_j)$是在所采用的误差损失准则下联合起来最能近似$\eta(X)$的项。每一个$\eta_j(X_j)$称为$X_j$的主影响。式中第二项是那些加入到主影响中将$\eta(X)$拟合得最好的含两个变量的函数。这些函数被称为每个变量对$(X_j,X_k)$的二阶交叉项。式中第三项表示三阶交叉项，以此类推。对于许多实际的问题，低阶交叉影响占主要地位。如果模型得到强烈的高阶交叉影响，比如大型的决策树，则可能正确性不好。
