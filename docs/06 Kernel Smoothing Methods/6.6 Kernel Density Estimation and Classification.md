# 核密度估计和分类

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-03-04：2017-03-04                    |

核密度估计是非监督学习过程，在历史上先于核回归。这也很自然地得到非参分类过程的简单族。

## 核密度估计
假设我们从概率密度$f_X(x)$中取随机样本$x_1,x_2,\ldots,x_N$，并且我们希望估计$x_0$处的$f_X$。为了简化，我们现在假设$X\in R$。与前面一样，一个自然的局部估计有如下形式
$$
\hat f_X(x_0)=\frac{\#x_i\in \cal N(x_0)}{N\lambda}\qquad (6.21)
$$
其中$\cal N(x_0)$为 $x_0$附近的宽度为$\lambda$的小度量邻域。这个估计非常振荡，并且偏向于光滑的Parzen估计
$$
\hat f(x_0)=\frac{1}{N\lambda}\sum\limits_{i=1}^NK_\lambda(x_0,x_i)\qquad (6.22)
$$
因为它统计离$x_0$近的观测，并且观测的权重系数随着到$x_0$的距离而减小。这种情形下$K_\lambda$受欢迎的选择是高斯核$K_\lambda(x_0,x)=\phi(\vert x-x_0\vert/\lambda)$。图6.13显示了根据CHD群中systolic blood pressure的样本值来拟合高斯核密度。令$\phi_\lambda$为均值为0标准误差为$\lambda$的高斯密度，则(6.22)有如下形式
$$
\begin{align}
\hat f_X(x)& = \frac{1}{N}\sum\limits_{i=1}^N\phi_\lambda(x-x_i)\\
&=(\hat F\star\phi_\lambda)(x)\qquad\qquad (6.23)
\end{align}
$$

样本分布$\hat F$和 $\phi_\lambda$的卷积。分布$\hat F(x)$在每个观测$x_i$处赋予$1/N$的权重，并且是跳跃的；在$\hat f_X(x)$中我们已经对每个观测$x_i$加上独立高斯噪声来光滑$\hat F$.

Parzen密度估计是局部平均的等价，并且与局部回归一起提出了改进[在密度的对数尺度；见Loader(1999)]。我们这里不继续讨论这些。在$R^p$中， 高斯密度的自然泛化意味着在（6.23）中使用高斯积的核，

$$
\hat f_X(x_0)=\frac{1}{N(2\lambda^2\pi)^{p/2}}\sum\limits_{i=1}^Ne^{-\frac{1}{2}(\Vert x_i-x_0\Vert/\lambda)^2}\qquad (6.24)
$$

## 核密度分类

利用贝叶斯定理采用直接方式来对分类做非参数密度估计。假设对于$J$个分类的问题，我们对每个类别拟合非参密度估计$\hat f_j(X),j=1,\ldots,J$，并且我们有类别先验概率$\hat \pi_j$的估计（通常是样本比例）。接着
$$
\widehat{Pr}(G=j\mid X=x_0)=\frac{\hat \pi_j\hat f_j(x_0)}{\sum_{k=1}^J\hat\pi_k\hat f_k(x_0)}\qquad (6.25)
$$

图6.14应用这个方法在心脏风险因子研究中估计CHD的患病率，并且应该与图6.12的左图进行比较。主要的差异出现在图6.14中右图的高SBP区域。这个区域中对于两个类别的数据都是稀疏的，并且因为高斯核密度估计采用度量核，所以密度估计在其他区域中是低的并且不好（高方差）。局部逻辑斯蒂回归方法（6.20）采用$k$-NN带宽的三次立方核；这有效地拓宽了这个区域中的核，并且利用局部线性假设来对估计进行光滑（在逻辑斯蒂尺度上）。

![](../img/06/fig6.14.png)

> 左图显示了在CHD和no-CHD群体中systolic blood preesure的两个独立的密度估计，每个都采用高斯核估计。右图显示了用（6.25）估计的CHD的后验概率。

如果分类是最终的目标，则很好地学习单独的类别密度可能不是必要的，并且可以实际上有误导。图6.15显示了密度都是多模式的，但是后验比例非常光滑。在从数据中学习单独的密度，可能考虑设置一个更粗糙，高方差的拟合来捕捉这些特征，这与估计后验概率的目的是不相关的。实际上，如果分类是最终目标，我们仅仅需要估计在判别边界附近的后验（对于两个类别，是集合$\{x\mid Pr(G=1\mid X=x)=\frac{1}{2}\}$）

## 朴素贝叶斯分类器

这是这些年仍然流行的技巧，尽管它的名字（也称为“白痴的贝叶斯”！）当特征空间的维数$p$很高，这种方式特别合适，使得密度估计不再吸引人。朴素贝叶斯模型假设给定类别$G=j$，特征$X_k$是独立的:

$$
f_j(X)=\prod\limits_{k=1}^pf_{jk}(X_k)\qquad (6.26)
$$

尽管这个假设一般是不对的，但是确实很大程度上简化了估计：

- 单独的类别条件的边缘密度$f_{jk}$可以采用一维核密度估计分别估计出来。这实际上是原始朴素贝叶斯过程的泛化，采用单变量高斯分布来表示这些边缘密度。
- 如果$X$的组分$X_j$是离散的，则可以使用合适的直方图估计。这提供了在特征向量中混合变量类型的完美方式。

尽管这些乐观的假设，朴素贝叶斯分类器经常比更复杂的分类器做得更好。原因与图6.15相关：尽管单独的类别密度估计可能是有偏的，这个偏差或许不会对后验概率不会有太大的影响，特别是在判别区域附近。实际上，这个问题或许可以承受为了节省方差造成的相当大的偏差，比如“天真的”假设得到的。

从(6.26)开始我们可以导出逻辑斯蒂变换（采用类别$J$作为基底）：

$$
\begin{align}
log\frac{Pr(G=\ell\mid X)}{Pr(G=J\mid X)}&=log\frac{\pi_\ell f_\ell(X)}{\pi_Jf_J(X)}\\
&=log\frac{\pi_\ell\prod_{k=1}^pf_{\ell k}(X_k)}{\pi_J\prod_{k=1}^pf_{Jk}(X_k)}\\
&=log\frac{\pi_\ell}{\pi_J}+\sum\limits_{k=1}^plog\frac{f_{\ell k}(X_k)}{f_{Jk}(X_k)}\\
&=\alpha_\ell + \sum\limits_{k=1}^pg_{\ell k}(X_k)
\end{align}
\qquad (6.27)
$$

这有广义加性模型的形式，更多细节将在第9章中描述。尽管模型以完全不同的方式来拟合；它们的差别将在练习6.9中探索。朴素贝叶斯和广义加性模型类比于线性判别分析和逻辑斯蒂回归（4.4.5节）


