# Bagging

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-03                               |

前面我们已经介绍了自助法bootstrap作为评估参数估计或预测的一种方式。这里我们展示怎么使用自助法去改善估计或者预测本身。在8.4节我们研究了自助法和贝叶斯方法之间的关系，而且发现自助法均值近似我一个后验平均。Bagging进一步探索这之间的联系。

首先考虑一个回归问题。假设我们根据我们的训练数据$\mathbf Z=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}$拟合一个模型，得到在输入$x$处的预测值$\hat f(x)$。自助法整合或者bagging在自助法样本的集合中平均了这个预测，因此降低了它的方差。对于每个自助法样本$\mathbf Z^{*b},b=1,2,\ldots,B$，我们拟合我们的模型得到预测$\hat f^{*b}(x)$.bagging估计定义为
$$
\hat f_{bag}(x)=\frac{1}{B}\sum\limits_{b=1}^B\hat f^{*b}(x)\qquad (8.51)
$$
用$\hat{\cal P}$记在每个数据点$(x_i,y_i)$上赋予相同概率$1/N$的经验分布。实际上，“真正”的bagging估计由$E_{\hat{\cal P}}\hat f^*(x)$定义，其中$\mathbf Z^*=(x_1^*,y_1^*),(x_2^*,y_2^*),\ldots,(x_N^*,y_N^*)$，并且$(x_i^*,y_i^*)\in \hat{\cal P}$.表达式（8.51）是真实bagging估计的蒙特卡洛估计，当$B\rightarrow \infty$.

bagged估计（8.51）与原始估计$\hat f(x)$仅仅当后者是数据的非线性或者数据的自适应函数才有区别。举个例子，为了使8.2.1节的B样条光滑，我们对图8.2的左下图中的曲线在每个$x$处进行平均。如果我们固定输入则B样条光滑在数据中是线性的，则当$B\rightarrow \infty$时，$\hat f_{bag}(x)\rightarrow \hat f(x)$（练习8.4）.因此bagging仅仅在图8.2的左上图中重现了原先的光滑。如果我们使用非参自助法来装袋结论近似正确。

一个更有趣的例子是回归树，其中$\hat f(x)$记为在每个输入向量$x$处树的预测（回归树在第九章中讨论）。每个自助法树都会比原先涉及不同的特征，而且可能会有不同的终止结点。袋装估计是在$x$处这些$B$棵树的平均预测。

现在假设我们的树产生一个用于$K$个类别响应变量的分类器$\hat G(x)$.这里考虑一个潜在的指示向量函数$\hat f(x)$（其中有一个1和$K-1$个0）使得$\hat G(x)=\mathrm{arg \; max}_k\;\hat f(x)$是很有用的。则袋装估计$\hat f_{bag}(x)$（8.51）是一个$K$维向量$[p_1(x),p_2(x),\ldots,p_K(x)]$,其中$p_k(x)$等于在$x$处预测为类别$k$的树的比例。袋装分类器从$B$棵树中选择得“票”最多的类别，$\hat G_{bag}(x)=\mathrm{arg \; max}_k\; \hat f_{bag}(x)$

经常我们需要在$x$处的类别概率估计，而不是分类自身。将投票比例$p_k(x)$看成是这些概率的估计是很吸引人的。一个简单的两个类别的例子显示了它们在这方面的失败。假设在$x$处类别1的真实概率为0.75，而且每个袋装分类器准确地预测了1.于是$p_1(x)=1$,这是不正确的。对于许多的分类器$\hat G(x)$, 然而，对于许多分类器$\hat G(x)$,已经有一个在$x$处估计类别概率的潜在函数$\hat f(x)$(对于树而言，类别的比例在末结点处)。另一个装袋策略是对这些取平均而不是对指示向量的得分。这个过程不仅仅改善了类别概率的估计，而且趋向于产生低方差的袋装分类器，特别是对于小$B$(见下一个例子的图8.10)

