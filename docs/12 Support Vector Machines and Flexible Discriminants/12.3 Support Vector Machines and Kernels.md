# 支持向量机和核

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-12-19:2016-12-20                   |
| 更新|2017.12.12 |

至今所描述的支持向量机是在输入特征空间中寻找线性边界。通过其他的线性方法，我们可以用基展开的方式来增大特征空间，从而让这一过程更灵活，如多项式或者样条（第5章）。一般地，在增长的空间的线性边界能够达到更好的训练类别分割，从而转换为原始空间中的非线性边界。一旦基函数$h_m(x),m=1,\ldots,M$被选定，则该过程和之前一样。我们采用输入特征$h(x_i)=(h_1(x_i),h_2(x_i),\ldots,h_M(x_i)),i=1,\ldots,N$来拟合SV分类器，并且得到（非线性）函数$\hat f(x)=h(x)^T\hat\beta+\hat\beta_0$。分类器和之前一样为$\hat G(x)=sign(\hat f(x))$.

支持向量机分类器是这个想法的拓展，其中增长空间的维数允许非常大，有时甚至可以是无限维的。似乎计算会变得难以承受。而且似乎当有了充分多的基函数，数据会变得可分，而且会发生过拟合。我们首先介绍SVM怎样处理这些问题。接着我们将看到实际中SVM分类器是采用特定的准则和正则化形式来求解函数拟合问题，而且这是包含第5章中光滑样条的更大类别的问题的一部分。读者可能希望参照5.8节，它提供了背景材料以及与接下来的两节会有一定的重复。

## 用于分类的SVM

我们可以以一种特殊的方式来表示优化问题(12.9)和它的解，这种方式只涉及通过内积来涉及输入特征。

### 常见的核函数

## SVM作为惩罚的方法

## 函数估计和重生核

## SVM和维数灾难

## SVM分类器的路径算法

## 用于回归的支持向量机

## 回归和核

## 讨论
