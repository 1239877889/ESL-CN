# 12.2 支持向量分类器

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2016-12-09                               |

在第4章中我们讨论了在两个可分类别之间构造一个最优的分离超平面。我们复习这个并且推广到不可分的情形下（类别可能不会被线性边界分离）。

我们的训练数据包含$N$对数据点$(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)$，其中$x_i\in R^p,y_i\in \{-1,1\}$.定义超平面
$$
\{x:f(x)=x^T\beta+\beta_0=0\}\qquad (12.1)
$$
其中$\beta$时单位向量$\Vert \beta\Vert=1$.由$f(x)$导出的分类准则为
$$
G(x)=sign[x^T\beta+\beta_0]\qquad (12.2)
$$
超平面的结构在4.5节讨论了，我们证明了(12.1)中的$f(x)$给出了从点$x$到超平面$f(x)=x^T+\beta_0=0$的符号距离。因为类别是可分的，则我们可以找到一个函数$f(x)=x^T\beta+\beta_0$,满足$y_if(x_i)>0,\forall i$.因此我们可以找到超平面在类别1和-1的训练点之间创造最大的空白（图12.1）。优化问题是
$$
\begin{array}{ll}
\underset{\beta,\beta_0,\Vert \beta\Vert=1}{max}\; M\\
s.t.\;y_i(x_i^T\beta+\beta_0)\ge M,\;i=1,\ldots,N,
\end{array}
\qquad (12.4)
$$
