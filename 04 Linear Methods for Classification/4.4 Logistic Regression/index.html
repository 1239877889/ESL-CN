<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://szcf-weiya.github.io/ESL-CN/04 Linear Methods for Classification/4.4 Logistic Regression/">
        <link rel="shortcut icon" href="../../img/favicon.ico">

	<title>4.4 逻辑斯蒂回归 - ESL CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css" rel="stylesheet">
        <link href="../../css/newsprint.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-85046550-1', 'auto');
          ga('send', 'pageview');

        </script>
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="../..">ESL CN</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../..">主页</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">目录 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">序言</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../Preface/2016-07-20-Preface-to-the-Second-Edition/">第二版序言</a>
</li>

        
            
<li >
    <a href="../../Preface/2016-07-21-Preface-to-the-First-Edition/">第一版序言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">1 简介</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../01 Introduction/2016-07-26-Chapter-1-Introduction/">1.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">2 监督学习概要</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.1 Introduction/">2.1 导言</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.2 Variable Types and Terminology/">2.2 变量类型和术语</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.3 Two Simple Approaches to Prediction/">2.3 两种预测的简单方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.4 Statistical Decision Theory/">2.4 统计判别理论</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.5 Local Methods in High Dimensions/">2.5 高维问题的局部方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.6 Statistical Models, Supervised Learning and Function Approximation/">2.6 统计模型，监督学习和函数逼近</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.7 Structured Regression Models/">2.7 结构化的回归模型</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.8 Classes of Restricted Estimators/">2.8 限制性估计的类别</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.9 Model Selection and the Bias-Variance Tradeoff/">2.9 模型选择和偏差-方差的权衡</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/Bibliographic Notes/">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">3 回归的线性方法</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.1 Introduction/">3.1 导言</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.2 Linear Regression Models and Least Squares/">3.2 线性回归模型和最小二乘法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.3 Subset Selection/">3.3 子集的选择</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.4 Shrinkage Methods/">3.4 收缩的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.5 Methods Using Derived Input Directions/">3.5 运用派生输入方向的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.6 A Comparison of the Selection and Shrinkage Methods/">3.6 选择和收缩方法的比较</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.7 Multiple Outcome Shrinkage and Selection/">3.7 多重输出的收缩和选择</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">4 分类的线性方法</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../4.1 Introduction/">4.1 导言</a>
</li>

        
            
<li >
    <a href="../4.2 Linear Regression of an Indicator Matrix/">4.2 指示矩阵的线性回归</a>
</li>

        
            
<li >
    <a href="../4.3 Linear Discriminant Analysis/">4.3 线性判别分析</a>
</li>

        
            
<li class="active">
    <a href="./">4.4 逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../4.5 Separating Hyperplanes/">4.5 分离超平面</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">5 基展开和正规化</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.1 Introduction/">5.1 导言</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.2 Piecewise Polynomials and Splines/">5.2 分段多项式和样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.3 Filtering and Feature Extraction/">5.3 滤波和特征提取</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.4 Smoothing Splines/">5.4 光滑样条</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">6 核光滑方法</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.1 Introduction/">6.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">7 模型评估及选择</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.1 Introduction/">7.1 导言</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.2 Bias, Variance and Model Complexity/">7.2 偏差，方差和模型复杂度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.3 The Bias-Variance Decomposition/">7.3 偏差-方差分解</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.4 Optimism of the Training Error Rate/">7.4 测试误差率的乐观</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.5 Estimates of In-Sample Prediction Error/">7.5 样本内预测误差的估计</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.6 The Effective Number of Parameters/">7.6 参数的有效个数</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.7 The Bayesian Approach and BIC/">7.7 贝叶斯方法和BIC</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.8 Minimum Description Length/">7.8 最小描述长度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.9 Vapnik-Chervonenkis Dimension/">7.9 VC维</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.10 Cross-Validation/">7.10 交叉验证</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.11 Bootstrap Methods/">7.11 自助法</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.12 Conditional or Expected Test Error/">7.12 条件测试误差或期望测试误差</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/Bibliographic Notes/">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">8 模型推断和平均</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.1 Introduction/">8.1 导言</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.2 The Bootstrap and Maximum Likelihood Methods/">8.2 自助法和最大似然法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.3 Bayesian Methods/">8.3 贝叶斯方法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.4 Relationship Between the Bootstrap and Bayesian Inference/">8.4 自助法和贝叶斯推断之间的关系</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.5 The EM Algorithm/">8.5 EM算法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.6 MCMC for Sampling from the Posterior/">8.6 MCMC向后采样</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.7 Bagging/">8.7 袋装法</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">9 增广模型，树，以及相关方法</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.0 Introduction/">9.0 导言</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.1 Generalized Additive Models/">9.1 广义加性模型</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.2 Tree-Based Methods/">9.2 基于树的方法</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">10 增强和加性树</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.1 Boosting Methods/">10.1 增强方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.2 Boosting Fits an Additive Model/">10.2 对加性模型的增强拟合</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.3 Forward Stagewise Additive Modeling/">10.3 向前逐步加性建模</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.4 Exponential Loss and AdaBoost/">10.4 指数损失和AdaBoost</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.5 Why Exponential Loss/">10.5 为什么是指数损失</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.6 Loss Functions and Robustness/">10.6 损失函数和鲁棒性</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">11 神经网络</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../11 Neural Networks/11.1 Introduction/">11.1 导言</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.2 Projection Pursuit Regression/">11.2 投影寻踪回归</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.3 Neural Networks/">11.3 神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.4 Fitting Neural Networks/">11.4 拟合神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.5 Some Issues in Training Neural Networks/">11.5 训练神经网络的一些问题</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.6 Example of Simulated Data/">11.6 模拟数据的例子</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">12 支持向量机和灵活的判别方法</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.1 Introduction/">12.1 导言</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.2 The Support Vector Classifier/">12.2 支持向量分类器</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">13 原型方法和最近邻</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.1 Introduction/">13.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">14 非监督学习</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../14 Unsupervised Learning/14.1 Introduction/">14.1 导言</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.2 Association Rules/">14.2 关联规则</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.3 Cluster Analysis/">14.3 聚类分析</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">15 随机森林</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../15 Random Forests/15.1 Introduction/">15.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu">
    <a tabindex="-1" class="nav-title">17 无向图模型</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.1 Introduction/">17.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
                <li >
                    <a rel="next" href="../4.3 Linear Discriminant Analysis/">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../4.5 Separating Hyperplanes/">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                <li>
                    <a href="https://szcf-weiya.github.io">
                        
                        Szcf-Weiya
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#44">4.4 逻辑斯蒂回归</a></li>
        
            <li><a href="#441">4.4.1 拟合逻辑斯蒂回归模型</a></li>
        
            <li><a href="#442">4.4.2 例子：南非心脏病</a></li>
        
            <li><a href="#443">4.4.3 二次拟合和推断</a></li>
        
            <li><a href="#444-l_1">4.4.4 $L_1$正则化逻辑斯蒂回归</a></li>
        
            <li><a href="#445-lda">4.4.5 逻辑斯蒂回归或者LDA？</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="44">4.4 逻辑斯蒂回归</h1>
<table>
<thead>
<tr>
<th>原文</th>
<th><a href="../../book/The Elements of Statistical Learning.pdf">The Elements of Statistical Learning</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>翻译</td>
<td>szcf-weiya</td>
</tr>
<tr>
<td>时间</td>
<td>2016-12-09:2016-12-15</td>
</tr>
</tbody>
</table>
<p>逻辑斯蒂回归来源于通过关于$x$的线性函数来建立$K$个类后验概率的模型的需要，同时保证它们的和为1且每一个都在$[0,1]$范围内。模型有如下形式
<script type="math/tex; mode=display">
\begin{array}{ll}
log\dfrac{Pr(G=1\mid X=x)}{Pr(G=K\mid X=x)}&=\beta_{10}+\beta_1^Tx\\
log\dfrac{Pr(G=2\mid X=x)}{Pr(G=K\mid X=x)}&=\beta_{20}+\beta_2^Tx\\
&\ldots\\
log\dfrac{Pr(G=K-1\mid X=x)}{Pr(G=K\mid X=x)}&=\beta_{(K-1)0}+\beta_{K-1}^Tx\\
\end{array}
\qquad (4.17)
</script>
模型由$K-1$个log-odds或logit变换来确定（反映了概率相加和为1的约束）。虽然模型采用最后一类来作为odds-rations的分母，分母的选择其实是任意的，因为在这个选择下估计是相等的。简单地计算得到
<script type="math/tex; mode=display">
\begin{array}{ll}
Pr(G=k\mid X=x)=\dfrac{exp(\beta_{k0}+\beta_k^Tx)}{1+\sum\limits_{\ell=1}^{K-1}exp(\beta_{\ell0+\beta_\ell^Tx})},& k=1,\ldots, K-1\\
Pr(G=K\mid X=x)=\dfrac{1}{1+\sum\limits_{\ell=1}^{K-1}exp(\beta_{\ell0+\beta_\ell^Tx})}&
\end{array}
\qquad (4.18)
</script>
显然它们相加等于1。为了强调对参数集$\theta={\beta_{10},\beta_1^T,\ldots,\beta_{(K-1)0},\beta_{K-1}^T}$的依赖，我们将概率记为$p_k(x,\theta)$.</p>
<p>当$K=2$时，模型非常简单，因为只有一个单线性函数。在生物统计应用中应用很广，因为经常会有二进制（两个类别）的响应变量。举个例子，病人获救或死亡，患心脏病和不患心脏病，或者某个条件存在与否。</p>
<h2 id="441">4.4.1 拟合逻辑斯蒂回归模型</h2>
<p>逻辑斯蒂回归经常通过极大似然法，采用在给定$X$的条件下$G$的条件概率。因为$Pr(G\mid X)$完全明确了条件分布，多元正态是合适的选择。$N$个观测的概率密度的对数为
<script type="math/tex; mode=display">
\ell(\theta)=\sum\limits_{i=1}^Nlog\,p_{g_i}(x_i;\theta)\qquad (4.19)
</script>
其中，$p_k(x_i;\theta)=Pr(G=k\mid X=x_i;\theta)$</p>
<p>我们将线性讨论两个类别的情形，因为算法可以相当简化。我们可以用0/1来对两个类别$g_i$的响应变量$y_i$进行编码，当$g_i=1$时$y_i=1$,当$g_i=2$时$y_i=0$.令$p_1(x;\theta)=p(x,\theta)$,$p_2(x;\theta)=1-p(x;\theta)$,概率的对数为
<script type="math/tex; mode=display">
\begin{array}{lll}
\ell(\beta)&=&\sum\limits_{i=1}^N\{y_ilogp(x_i;\beta)+(1-y_i)log(1-p(x_i;\beta))\}\\
&=&\sum\limits_{i=1}^N\{y_i\beta^Tx_i-log(1+e^{\beta^Tx_i})\}
\end{array}
\qquad (4.20)
</script>
这里$\beta={\beta_{10},\beta_1}$,而且我们假设输入向量$x_i$包含表示截距的项1.</p>
<p>为了最大化概率的对数，我们令微分为0，得到
<script type="math/tex; mode=display">
\dfrac{\partial \ell(\beta)}{\partial \beta}=\sum\limits_{i=1}^Nx_i(y_i-p(x_i;\beta))=0,\qquad (4.21)
</script>
这是关于$\beta$的$p+1$个非线性等式。注意到因为$x_i$的第一个组分为1,第一个得分等式为$\sum_{i=1}^Ny_i=\sum_{i=1}^Np(x_i;\beta)$;期望的类别数与观测的个数一致（因此有两个类别）</p>
<p>为了求解（4.21）的得分等式，我们采用Newton-Raphson算法，需要Hessian矩阵
<script type="math/tex; mode=display">
\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}=\sum\limits_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta))\qquad (4.22)
</script>
</p>
<blockquote>
<p>weiya注：</p>
<p>Newton-Raphson 也就是我们熟知的牛顿迭代法
<script type="math/tex; mode=display">
x_{i+1}=x_i-\dfrac{f(x_i)}{f'(x_i)}
</script>
</p>
</blockquote>
<p>以$\beta^{old}$开始，新的Newton更新为
<script type="math/tex; mode=display">
\beta^{new}=\beta^{old}-(\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T})^{-1}\dfrac{\partial \ell(\beta)}{\partial(\beta)}\qquad (4.23)
</script>
其中微分值由$\beta^{old}$处的值得到。</p>
<p>把得分和Hessian写成矩阵形式是很方便的。记$\mathbf y$为$y_i$的值，$\mathbf X$是值为$x_i$的$N\times (p+1)$矩阵，$\mathbf p$是拟合概率的向量且第$i$个元素为$p(x_i;\beta^{odd})$，$\mathbf W$是第$i$个对角元为$p(x_i;\beta^{odd})(1-p(x_i;\beta^{odd}))$的$N\times N$的对角矩阵。则我们有</p>
<blockquote>
<p>weiya注：</p>
<p>$\mathbf X=[x&rsquo;_1,x&rsquo;_2,\ldots,x&rsquo;_N]&rsquo;$</p>
</blockquote>
<p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\dfrac{\partial \ell(\beta)}{\partial \beta}&=\mathbf{X^T(y-p)}\qquad (4.24)\\
\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}&=-\mathbf{X^TWX}\qquad (4.25)
\end{array}
</script>
牛顿迭代为
<script type="math/tex; mode=display">
\begin{array}{ll}
\beta^{new}&=\beta^{old}+\mathbf{(X^TWX)^{-1}X^T(y-p)}\\
&=\mathbf {(X^TWX)^{-1}X^TW(X\beta^{old}+W^{-1}(y-p))}\\
&=\mathbf{(X^TWX)^{-1}X^TWz}\qquad (4.26)
\end{array}
</script>
</p>
<p>在第二和第三行我们已经重新把牛顿迭代表达成最小二乘迭代，响应变量为
<script type="math/tex; mode=display">
\mathbf z=\mathbf X\beta^{old}+\mathbf{W^{-1}(y-p)}\qquad (4.27)
</script>
有时也被称作调整后的响应变量。这些方程重复地进行求解，每一次迭代$\mathbf p$改变，因此$\mathbf W$和$\mathbf z$也改变。这个算法被称作是加权迭代最小二乘或者IRILS，因为每次迭代求解加权最小二乘问题：
<script type="math/tex; mode=display">
\beta^{new}\leftarrow arg\,\underset{\beta}{min}(\mathbf z-\mathbf X\beta)^T\mathbf W(\mathbf z-\mathbf X\beta)\qquad (4.28)
</script>
似乎$\beta=0$是迭代过程一个很好的初始值，尽管收敛性不会保证。一般地算法确实是收敛的，因为概率的对数是凹的，但是可以出现过收敛的情况。如果在罕见的情形下概率的对数值下降，对步长二分法会保证收敛性。</p>
<p>对于多类别的情况$K\ge 3$,牛顿算法也可以表达成加权迭代最小二乘，只是对于每一个观测用一个$K-1$的响应向量和非对角系数矩阵。后者阻碍了任何简化算法，在这种情况下直接使用展开向量$\theta$数值上会更加方便（练习4.4）.另一种是坐标下降方法（3.8.6节）也可以有效地最大化概率的log值。R语言的glmnet包（Friedman等人2008a）可以有效地拟合$N$和$p$都非常大的逻辑斯蒂回归。尽管是为了拟合正规化模型设计的，但是对于非正规化的拟合也适用。</p>
<p>逻辑斯蒂回归经常被用作一种数据分析和推断的工具，目标就是理解在解释输出时输入变量的角色。一般地，许多模型是通过寻找涉及变量的最简介的模型，很可能还有一些交叉项。下面的例子说明了涉及到的一些问题。</p>
<h2 id="442">4.4.2 例子：南非心脏病</h2>
<p>这里我们展示一个用二值数据分析去说明逻辑斯蒂回归的传统统计学应用。图4.12中的数据取自CORIS调查的部分数据，这个调查在南非Western Cape的三个乡村进行（Rousseauw等人1983）.调查的目标是建立在高发生率的地区的缺血性心脏病影响因子的强度模型。数据中有15到64岁的白人男性，响应变量是MI的存在或者缺失（该地区整个MI流行程度为5.1%）。在我们数据集中有160个案例，以及302个控制个体。数据在Hastie和Tibshirani（1987）的工作中有详细描述。</p>
<p><img alt="" src="../../img/04/fig4.12.png" /></p>
<blockquote>
<p>图4.12. 南非心脏病数据的散点图。每张图片是一对影响因子，案例集河控制集用颜色标出（红色的为案例）。家族心脏病史（famhist）是二值数据（是或否）</p>
</blockquote>
<p>我们通过极大似然法拟合了逻辑斯蒂回归模型，给出了如表4.2所示的结果。结果概要包含了模型中每一个系数的$Z$分数（系数出一他们的标准误差）；不重要的$Z$分数表明该系数可以从模型中剔除。这对应着检验该系数为0而其它系数不为0的零假设（也被称作Wald检验）。$Z$分数的绝对值大于2表明在5%的水平下是显著的。</p>
<p><img alt="" src="../../img/04/tab4.2.png" /></p>
<blockquote>
<p>表4.2. 南非心脏病数据的逻辑斯蒂回归结果。</p>
</blockquote>
<p>在系数表中有一些比较奇怪的结果，这必须认真对待。收缩压sbp竟然不显著！肥胖（obesity）也不显著，它的符号是负的。这个混乱是因为预测变量之间的相关关系。单独地来看，sbp和obesity都是显著的，而且都是正号。然而，当有其它相关变量时它们便不再需要了（甚至可以得到一个负号）。</p>
<p>这时分析者可能会做一些模型选择；寻找能够充分解释他们在chd上联合影响的子集变量。一种方式是删掉显著性最低的系数然后重新拟合模型。这个可以重复做下去直到没有更多的项可以从模型中剔除。这样得到了表4.3所示例的模型。</p>
<p><img alt="" src="../../img/04/tab4.3.png" /></p>
<blockquote>
<p>表4.3. 对南非心脏病数据逐步逻辑斯蒂回归的结果</p>
</blockquote>
<p>一个更好但是需要花费更多时间的策略是对每一个剔除一个变量后的模型进行重新拟合，然后进行偏差分析（analysis of deviance）确定哪个变量需要剔除。拟合模型的残差是减去两倍的概率对数，两个模型的偏差是它们个体残差的差别（类似于平方和）这个策略给出了上面同样的最终结果。</p>
<p>举个例子，怎么解释tobacco的系数0.081（标准偏差为0.026）？tobacco是一生中使用的烟草总千克数，控制集中位数为1.0kg，而这个案例集为4.1kg。因此每增加1kg的烟草使用量意味着冠状心脏病的几率为$exp(0.081)=1.084$(或者8.4%).结合标准误差我们得到95%的置信区间$exp(0.081\pm 2\times 0.026)=(1.03,1.14)$</p>
<blockquote>
<p>为什么是两倍？</p>
</blockquote>
<p>我们将在第5章再次用到这些数据，我们将会看到一些变量会有非线性影响，当进行合适的建模后不会被剔除模型。</p>
<h2 id="443">4.4.3 二次拟合和推断</h2>
<p>极大似然法的参数估计$\hat\beta$满足自一致性关系：它们是加权最小二乘拟合的系数，其中响应变量为
<script type="math/tex; mode=display">
z_i=x_i^T\hat\beta+\dfrac{y_i-\hat p_i}{\hat p_i(1-\hat p_i)}\qquad (4.29)
</script>
系数为$w_i=\hat p_i(1-\hat p_i)$，都依赖$\hat\beta$自身。除了提供了一个方便的算法，这和最小二乘的联系还有其它用处：
- 加权的残差平方和是熟悉的Pearson卡方统计量
<script type="math/tex; mode=display">
\sum\limits_{i=1}^N\dfrac{(y_i-\hat p_i)^2}{\hat p_i(1-\hat p_i)}\qquad (4.30)
</script>
是一个对偏差的二次近似。
- 极限概率理论表明如果模型是正确的，则$\hat\beta$为常数（比如，收缩到真实的$\beta$）
- 中心极限定理证明$\hat \beta$的分布收敛到$N(\beta,\mathbf{(X^TWX)^{-1}})$。这个可以模仿正态理论推断直接从加权最小二乘得到。
- 对于逻辑斯蒂回归建立模型计算量比较大，因为每个模型拟合需要迭代。受欢迎的方法是Rao score检验来检验是否包含某一项，以及Wald检验来检验是否剔除某一项。这些都不需要迭代拟合，而且都是基于当前模型的极大似然拟合。结果是两者都代表在加权最小二乘拟合中运用相同的系数加上或剔除某一项。这些计算可以高效地进行，不需要重新计算整个加权最小二乘拟合。</p>
<p>软件实现可以利用这些联系。举个例子，R中的广义线性模型（包括作为二项式模型族中的逻辑斯蒂回归）充分运用了他们。GLM（广义线性模型）对象可以当作线性模型对象来处理，而且所有对于线性模型可用的工具都可以自动应用起来。</p>
<h2 id="444-l_1">4.4.4 $L_1$正则化逻辑斯蒂回归</h2>
<p>lasso中使用了$L_1$惩罚（3.4.2节）可以被应用到任意线性回归模型的变量选择和收缩上面。对于逻辑斯蒂回归，我们最大化（4.20）的带惩罚项的版本：
<script type="math/tex; mode=display">
\underset{\beta_0,\beta}{max}\{\sum\limits_{i=1}^N[y_i(\beta_0+\beta^Tx_i)-log(1+e^{\beta_0+\beta^Tx_i})]-\lambda\sum\limits_{j=1}^p\vert \beta_j\vert\}\qquad (4.31)
</script>
对于lasso，我们一般不对截距项进行惩罚，而且对预测变量标准化后加惩罚是有意义的。准则（4.31）是凹的，而且运用非线性编程方法可以找到一个解（举个例子，Koh等人2007）。另外，运用我们在4.4.1节运用的Netwon算法的二次逼近，我们可以通过重复应用加权的lasso算法求解（4.31）.有趣的是非零系数变量的得分等式（式4.24）有如下形式
<script type="math/tex; mode=display">
\mathbf {x_j^T(y-p)} = \lambda\cdot sign(\beta_j)\qquad (4.32)
</script>
这一般化了3.4.4节中的（3.58）；活跃变量在与残差的一般化相关性结合在一起。
路径算法比如说lasso的LAR算法更加困难，因为系数曲线是逐段光滑而不是线性的。然而，可以通过二次逼近来实现。</p>
<p>图4.13显示了4.4.2节南非心脏病数据的$L_1$正则化路径。这是通过R语言glmpath包（Park和Hastie）实现的，运用了凸优化的predictor-corrector方法确定了$\lambda$在哪个非零活跃集值发生改变（图中的垂直线）。这里直线看似线性的，在其它的例子中曲率将会更加明显。</p>
<p><img alt="" src="../../img/04/fig4.13.png" /></p>
<blockquote>
<p>图4.13. 南非心脏病数据的$L_1$正则化逻辑斯蒂回归系数，画出了作为$L_1$范数函数的曲线。这些变量都标准化后有单位方差。在图像每一个点处都精确算出来了。</p>
</blockquote>
<p>坐标下降方法（3.8.6节）对于计算$\lambda$网格上的值的系数曲线是很有效的。R语言glmnet包（Friedman等人2008a）对于非常大的逻辑斯蒂回归问题拟合系数路径是非常有效的（$N$或$p$很大时）。他们的算法可以利用预测变量矩阵$X$的稀疏性，甚至对于大规模问题也适用。18.4节有更详细的解释，而且将会讨论$L_1$正则多项分布模型。</p>
<h2 id="445-lda">4.4.5 逻辑斯蒂回归或者LDA？</h2>
<p>在4.3节我们发现类别$k$和$K$类关于$x$线性函数的后验概率odds的对数（4.9）：
<script type="math/tex; mode=display">
\begin{array}{ll}
log\dfrac{Pr(G=k\mid X=x)}{Pr(G=K\mid X=x)}&=log\dfrac{\pi_k}{\pi_K}-\frac{1}{2}(\mu_k+\mu_K)^T\Sigma^{-1}(\mu_k-\mu_K)\\
&=\alpha_{k0}+\alpha_k^Tx\qquad\qquad (4.33)
\end{array}
</script>
线性是因为对类别的概率密度函数做了正态性假设及协方差矩阵相等的假设。通过构造线性逻辑斯蒂模型（4.17）有线性logits:
<script type="math/tex; mode=display">
log\frac{Pr(G=k\mid X=x)}{Pr(G=K\mid X=x)}=\beta_{k0}+\beta_k^Tx\qquad (4.34)
</script>
看起来模型似乎是一样的。尽管它们确实有相同的形式，区别在于它们系数估计的方式。逻辑斯蒂回归模型更加一般，因为它做了更少的假设。我们可以把$X$和$G$的联合概率密度写成
<script type="math/tex; mode=display">
Pr(X,G=k)=Pr(X)Pr(G=k\mid X)\qquad (4.35)
</script>
其中$Pr(X)$为输入$X$的边缘概率密度。对于LDA和逻辑斯蒂回归，右边的第二项有线性logit形式
<script type="math/tex; mode=display">
Pr(G=k\mid X=x)=\dfrac{e^{\beta_{k0}+\beta_k^Tx}}{1+\sum_{\ell=1}^{K-1}e^{\beta_{\ell0+\beta_\ell^Tx}}}\qquad (4.36)
</script>
我们再一次任意地选择最后一类作为标准。</p>
<p>逻辑斯蒂回归模型使得$X$的边缘概率密度为一个任意的边界密度函数$Pr(X)$，然后通过最大化条件概率（$Pr(G=k\mid X)$的多项式概率）来拟合$Pr(G\mid X)$的参数。尽管$Pr(X)$完全忽略掉，我们可以把边缘密度作为在全部非参和无约束情形下的估计，采用在每个观测值上权重为$1/N$的经验分布函数。</p>
<p>进行LDA时，我们通过基于下面的联合密度来最大化概率的对数值进行拟合参数
<script type="math/tex; mode=display">
Pr(X,G=k)=\phi(X;\mu_k,\Sigma)\pi_k\qquad (4.37)
</script>
其中$\phi$是高斯密度函数。标准正态的理论更容易估计4.3节的$\hat\mu_k,\hat\Sigma$和$\hat\pi_k$.因为（4.3）形式的逻辑斯蒂回归的线性参数是高斯分布参数的函数，我们通过插入对应的估计得到它们的极大似然估计。然而，不像在有条件的情形下，$Pr(X)$的边缘密度起着重要作用。它是一个混合密度
<script type="math/tex; mode=display">
Pr(X)=\sum\limits_{k=1}^K\pi_k\phi(X;\mu_k,\Sigma)\qquad (4.38)
</script>
同样涉及到参数。</p>
<p>自己增加的成分/限制起着什么作用呢？通过对额外模型假设的依赖，我们得到参数更多的信息，因此可以更加有效地估计它们（低方差）。如果事实上$f_k(x)$的真实分布为高斯，则忽略密度的边缘项最坏的情形是在误差率上有近似30%的效率损失（Efron，1975）。换句话说：当再有30%的数据，条件概率同样做得很好。</p>
<p>举个例子，远离判别边界的观测点（它们被逻辑斯蒂回归down-weighted）在估计斜方差阵中起着作用。这不是一个好消息，因为这意味着LDA对于重大的边界点不够稳健。</p>
<p>从混合模型的组成来看，很显然没有类别标签的观测都有参数的信息。产生类别标签经常是不划算的，但是没有分类的观测值是容易的。通过依赖对模型强的假设，正如这里的一样，我们可以同时运用两种类别的信息。</p>
<p>边缘概率可以认为是一个正则，从边缘角度来看在某种程度上类别密度是已知的。举个例子，如果两个类别逻辑斯蒂回归模型的数据可以很好地被超平面分隔开，参数的极大似然估计是没有定义的（比如说无限，见练习4.5）.而对于同样的数据LDA的系数有很好的定义，因为边缘密度函数不会允许这些退化。</p>
<p>实际应用中这些假设从不正确，而且$X$的有些组分是定性变量。一般感觉逻辑斯蒂回归更加安全，比LDA模型更加稳健，因为依赖更少的假设。从我们经验来看两个模型都得到非常相似的结果，甚至不恰当地运用LDA时，比如说对于定性预测变量。</p>

<div id="disqus_thread"></div>
<script>

/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
/*
var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//weiya.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script id="dsq-count-scr" src="//weiya.disqus.com/count.js" async></script></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
        <script src="../../js/base.js"></script>
        <script src="../../js/MathJax/MathJax.js?config=TeX-AMS_CHTML"></script>
        <script src="../../js/mymathjax.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>