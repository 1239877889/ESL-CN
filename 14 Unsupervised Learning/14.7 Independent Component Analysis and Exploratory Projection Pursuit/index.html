<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="The Elements of Statistical Learning(ESL) 的中文笔记">
        
        <link rel="canonical" href="https://esl.hohoweiya.xyz/14 Unsupervised Learning/14.7 Independent Component Analysis and Exploratory Projection Pursuit/index.html">
	<link rel="shortcut icon" href="../../img/favicon.ico">

	<title>14.7 独立成分分析和探索投射寻踪 - ESL CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css?v201801062" rel="stylesheet">
        <link href="../../css/newsprint.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-85046550-1', 'auto');
          ga('send', 'pageview');

        </script>

        <script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<!--
	<style>
	  .dropdown-menu {
    max-height: 500px;
    overflow-y: auto;
    overflow-x: auto;
}
	</style>
-->
<script data-cfasync="false" type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes:true
},
TeX: {
  Macros: {

    A: "{\\mathbf{A}}",
    B: "{\\mathbf{B}}",
    C: "{\\mathbf{C}}",
    D: "{\\mathbf{D}}",
    R: "{\\mathbf{R}}",
    S: "{\\mathbf{S}}",
I: "{\\mathbf{I}}",
J: "{\\mathbf{J}}",
    X: "{\\mathbf{X}}",

LOG: "{\\mathrm{log}\\;}",
    E: "{\\mathrm{E}\\;}",
    1: "{\\boldsymbol 1}",
    Cov: "{\\mathrm{Cov}\\;}",
Var: "{\\mathrm{Var}\\;}",
det: "{\\mathrm{det}\\;}",
cosh: "{\\mathrm{cosh}\\;}",
arg: "{\\mathrm{arg}\\;}",
max: "{\\mathrm{max}\\;}"

  },
  entensions: ["color.js"]
}
});
</script>
<script data-cfasync="false" type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2-beta.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    </head>
    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="">ESL CN</a>

        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../../index.html">主页</a>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">上篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">序言</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../Preface/2016-07-20-Preface-to-the-Second-Edition/index.html">第二版序言</a>
</li>

        
            
<li >
    <a href="../../Preface/2016-07-21-Preface-to-the-First-Edition/index.html">第一版序言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">1 简介</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../01 Introduction/2016-07-26-Chapter-1-Introduction/index.html">1.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">2 监督学习概要</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.1 Introduction/index.html">2.1 导言</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.2 Variable Types and Terminology/index.html">2.2 变量类型和术语</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.3 Two Simple Approaches to Prediction/index.html">2.3 两种预测的简单方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.4 Statistical Decision Theory/index.html">2.4 统计判别理论</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.5 Local Methods in High Dimensions/index.html">2.5 高维问题的局部方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.6 Statistical Models, Supervised Learning and Function Approximation/index.html">2.6 统计模型，监督学习和函数逼近</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.7 Structured Regression Models/index.html">2.7 结构化的回归模型</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.8 Classes of Restricted Estimators/index.html">2.8 限制性估计的类别</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.9 Model Selection and the Bias-Variance Tradeoff/index.html">2.9 模型选择和偏差-方差的权衡</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">3 回归的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.1 Introduction/index.html">3.1 导言</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.2 Linear Regression Models and Least Squares/index.html">3.2 线性回归模型和最小二乘法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.3 Subset Selection/index.html">3.3 子集的选择</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.4 Shrinkage Methods/index.html">3.4 收缩的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.5 Methods Using Derived Input Directions/index.html">3.5 运用派生输入方向的方法</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.6 A Comparison of the Selection and Shrinkage Methods/index.html">3.6 选择和收缩方法的比较</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.7 Multiple Outcome Shrinkage and Selection/index.html">3.7 多重输出的收缩和选择</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.8 More on the Lasso and Related Path Algorithms/index.html">3.8 Lasso和相关路径算法的补充</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/3.9 Computational Considerations/index.html">3.9 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../03 Linear Methods for Regression/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">4 分类的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.1 Introduction/index.html">4.1 导言</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.2 Linear Regression of an Indicator Matrix/index.html">4.2 指示矩阵的线性回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.3 Linear Discriminant Analysis/index.html">4.3 线性判别分析</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.4 Logistic Regression/index.html">4.4 逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.5 Separating Hyperplanes/index.html">4.5 分离超平面</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">5 基展开和正规化</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.1 Introduction/index.html">5.1 导言</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.2 Piecewise Polynomials and Splines/index.html">5.2 分段多项式和样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.3 Filtering and Feature Extraction/index.html">5.3 滤波和特征提取</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.4 Smoothing Splines/index.html">5.4 光滑样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters/index.html">5.5 光滑参数的自动选择</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.6 Nonparametric Logistic Regression/index.html">5.6 非参逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.7-Multidimensional-Splines/index.html">5.7 多维样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html">5.8 正则化和再生核希尔伯特空间理论</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html">5.9 小波光滑</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/Appendix-Computations-for-B-splines/index.html">附录-B样条的计算</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">6 核光滑方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.0 Introduction/index.html">6.0 导言</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.1 One-Dimensional Kernel Smoothers/index.html">6.1 一维核光滑器</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.2 Selecting the Width of the Kernel/index.html">6.2 选择核的宽度</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.3 Local Regression in R^p/index.html">6.3 $R^p$中的局部回归</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.4 Structured Local Regression Models in R^p/index.html">6.4 $R^p$中的结构化局部回归模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.5 Local Likelihood and Other Models/index.html">6.5 局部似然和其他模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.6 Kernel Density Estimation and Classification/index.html">6.6 核密度估计和分类</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.7 Radial Basis Functions and Kernels/index.html">6.7 径向基函数和核</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification/index.html">6.8 混合模型的密度估计和分类</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.9-Computational-Consoderations/index.html">6.9 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">中篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">7 模型评估及选择</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.1 Introduction/index.html">7.1 导言</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.2 Bias, Variance and Model Complexity/index.html">7.2 偏差，方差和模型复杂度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.3 The Bias-Variance Decomposition/index.html">7.3 偏差-方差分解</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.4 Optimism of the Training Error Rate/index.html">7.4 测试误差率的optimism</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.5 Estimates of In-Sample Prediction Error/index.html">7.5 样本内预测误差的估计</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.6 The Effective Number of Parameters/index.html">7.6 参数的有效个数</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.7 The Bayesian Approach and BIC/index.html">7.7 贝叶斯方法和BIC</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.8 Minimum Description Length/index.html">7.8 最小描述长度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.9 Vapnik-Chervonenkis Dimension/index.html">7.9 VC维</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.10 Cross-Validation/index.html">7.10 交叉验证</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.11 Bootstrap Methods/index.html">7.11 自助法</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.12 Conditional or Expected Test Error/index.html">7.12 条件测试误差或期望测试误差</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">8 模型推断和平均</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.1 Introduction/index.html">8.1 导言</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.2 The Bootstrap and Maximum Likelihood Methods/index.html">8.2 自助法和最大似然法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.3 Bayesian Methods/index.html">8.3 贝叶斯方法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.4 Relationship Between the Bootstrap and Bayesian Inference/index.html">8.4 自助法和贝叶斯推断之间的关系</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.5 The EM Algorithm/index.html">8.5 EM算法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.6 MCMC for Sampling from the Posterior/index.html">8.6 MCMC向后采样</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.7 Bagging/index.html">8.7 袋装法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.8 Model Averaging and Stacking/index.html">8.8 模型平均和堆栈</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.9 Stochastic Search/index.html">8.9 随机搜索</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">9 增广模型，树，以及相关方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.0 Introduction/index.html">9.0 导言</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.1 Generalized Additive Models/index.html">9.1 广义加性模型</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.2 Tree-Based Methods(CART)/index.html">9.2 基于树的方法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.3 PRIM(Bump Hunting)/index.html">9.3 耐心规则归纳法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.4 MARS(Multivariate Adaptive Regression Splines)/index.html">9.4 多变量自适应回归样条</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.5 Hierarchical Mixtures of Experts/index.html">9.5 专家的系统混合</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.6 Missing Data/index.html">9.6 缺失数据</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.7 Computational Considerations/index.html">9.7 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">10 增强和加性树</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.1 Boosting Methods/index.html">10.1 增强方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.2 Boosting Fits an Additive Model/index.html">10.2 boosting拟合可加模型</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.3 Forward Stagewise Additive Modeling/index.html">10.3 向前逐步加性建模</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.4 Exponential Loss and AdaBoost/index.html">10.4 指数损失和AdaBoost</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.5 Why Exponential Loss/index.html">10.5 为什么是指数损失</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.6 Loss Functions and Robustness/index.html">10.6 损失函数和鲁棒性</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.7 Off-the-Shelf Procedures for Data Mining/index.html">10.7 数据挖掘的现货方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.8 Spam Data/index.html">10.8 垃圾邮件的例子</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.9 Boosting Trees/index.html">10.9 boosting树</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.10 Numerical Optimization via Gradient Boosting/index.html">10.10 利用梯度boosting的数值优化</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.11 Right-Sized Trees for Boosting/index.html">10.11 大小合适的boosting树</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.12 Regularization/index.html">10.12 正则化</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">11 神经网络</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../11 Neural Networks/11.1 Introduction/index.html">11.1 导言</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.2 Projection Pursuit Regression/index.html">11.2 投影寻踪回归</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.3 Neural Networks/index.html">11.3 神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.4 Fitting Neural Networks/index.html">11.4 拟合神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.5 Some Issues in Training Neural Networks/index.html">11.5 训练神经网络的一些问题</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.6 Example of Simulated Data/index.html">11.6 模拟数据的例子</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.7-Example-ZIP-Code-Data/index.html">11.7 邮编数字的例子</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">12 支持向量机和灵活的判别方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.1 Introduction/index.html">12.1 导言</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.2 The Support Vector Classifier/index.html">12.2 支持向量分类器</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.3 Support Vector Machines and Kernels/index.html">12.3 支持向量机和核</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">下篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">13 原型方法和最近邻</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.1 Introduction/index.html">13.1 导言</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.2 Prototype Methods/index.html">13.2 原型方法</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.3 k-Nearest-Neighbor Classifiers/index.html">13.3 k最近邻分类器</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.4 Adaptive Nearest-Neighbor Methods/index.html">13.4 自适应的最近邻方法</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.5 Computational Considerations/index.html">13.5 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">14 非监督学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../14.1 Introduction/index.html">14.1 导言</a>
</li>

        
            
<li >
    <a href="../14.2 Association Rules/index.html">14.2 关联规则</a>
</li>

        
            
<li >
    <a href="../14.3 Cluster Analysis/index.html">14.3 聚类分析</a>
</li>

        
            
<li >
    <a href="../14.4 Self-Organizing Maps/index.html">14.4 自组织图</a>
</li>

        
            
<li >
    <a href="../14.5-Principal-Components-Curves-and-Surfaces/index.html">14.5 主成分，主曲线以及主曲面</a>
</li>

        
            
<li >
    <a href="../14.6 Non-negative Matrix Factorization/index.html">14.6 非负矩阵分解</a>
</li>

        
            
<li class="active">
    <a href="index.html">14.7 独立成分分析和探索投射寻踪</a>
</li>

        
            
<li >
    <a href="../14.8 Multidimensional Scaling/index.html">14.8 多维缩放</a>
</li>

        
            
<li >
    <a href="../14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling/index.html">14.9 非线性降维和局部多维缩放</a>
</li>

        
            
<li >
    <a href="../14.10 The Google PageRank Algorithm/index.html">14.10 谷歌的PageRank算法</a>
</li>

        
            
<li >
    <a href="../Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">15 随机森林</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../15 Random Forests/15.1 Introduction/index.html">15.1 导言</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.2 Definition of Random Forests/index.html">15.2 随机森林的定义</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.3 Details of Random Forests/index.html">15.3 随机森林的细节</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.4-Analysis-of-Random-Forests/index.html">15.4 随机森林的分析</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">16 集成学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../16 Ensemble Learning/16.1 Introduction/index.html">16.1 导言</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/16.2 Boosting and Regularization Paths/index.html">16.2 增强和正则路径</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/16.3 Learning Ensembles/index.html">16.3 学习集成</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">17 无向图模型</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.1 Introduction/index.html">17.1 导言</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.2 Markov Graphs and Their Properties/index.html">17.2 马尔科夫图及其性质</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.3 Undirected Graphical Models for Continuous Variables/index.html">17.3 连续变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.4 Undirected Graphical Models for Discrete Variables/index.html">17.4 离散变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">18 高维问题</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.1 When p is Much Bigger than N/index.html">18.1 当p大于N</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.2 Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids/index.html">18.2 对角线性判别分析和最近收缩重心</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.3 Linear Classifiers with Quadratic Regularization/index.html">18.3 二次正则的线性分类器</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.4 Linear Classifiers with L1 Regularization/index.html">18.4 一次正则的线性分类器</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.5 Classification When Features are Unavailable/index.html">18.5 当特征不可用时的分类</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.6 High-Dimensional Regression/index.html">18.6 有监督的主成分</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.7 Feature Assessment and the Multiple-Testing Problem/index.html">18.7 特征评估和多重检验问题</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/Bioliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
              <!--
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
              -->
              <!--
                <li>
                  <a href="https://esl.hohoweiya.xyz/14 Unsupervised Learning/14.7 Independent Component Analysis and Exploratory Projection Pursuit/index.html#disqus_thread">0 Comments</a>
                </li>
              -->
                <li >
                    <a rel="next" href="../14.6 Non-negative Matrix Factorization/index.html">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../14.8 Multidimensional Scaling/index.html">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>


                
                <!--
                <li>
                    <a href="https://github.com/szcf-weiya/">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
              -->
                <li>
                  <a href="https://github.com/szcf-weiya/"><i class="fa fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                <li>
                  <a href="https://stats.hohoweiya.xyz"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                <li>
                  <a href="https://blog.hohoweiya.xyz"><i class="fa fa-pencil" aria-hidden="true"></i> 随笔</a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#147">14.7 独立成分分析和探索投影寻踪</a></li>
        
            <li><a href="#_1">潜变量和因子分析</a></li>
        
            <li><a href="#_2">独立成分分析</a></li>
        
            <li><a href="#_5">探索投影寻踪</a></li>
        
            <li><a href="#_6">独立分量分析的一种直接方法</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">
<h1 id="147">14.7 独立成分分析和探索投影寻踪</h1>
<table>
<thead>
<tr>
<th>原文</th>
<th><a href="../../book/The Elements of Statistical Learning.pdf">The Elements of Statistical Learning</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>翻译</td>
<td>szcf-weiya</td>
</tr>
<tr>
<td>时间</td>
<td>2017-09-03</td>
</tr>
<tr>
<td>更新</td>
<td>2018-01-20 &amp; 2018-01-21</td>
</tr>
</tbody>
</table>
<p>多元数据经常被看成是从未知来源中多重间接测量的数据，一般不能直接测量。包括以下例子：</p>
<ul>
<li>在教育或心理学测试中，采用问卷的答案来衡量潜在的智商以及其他的心理特征。</li>
<li>EEG脑扫描通过放置在头部的不同位置的感受器的电子信号来间接衡量脑的不同部分的神经元活性。</li>
<li>股票交易价格随着时间持续变化，并且反映了各种各样的未测量的因素，如市场信心，外部影响以及其他很难识别和测量的推动力。</li>
</ul>
<p>因子分析(Factor Analysis)是在统计学领域为了识别潜在因素的一个经典方法。因子分析模型通常是用在高斯分布中，某种程度上阻碍了它的适用性。最近，独立成分分析(Independent Component Analysis)成为了因子分析的强劲对手，我们将会看到，它对非高斯分布的依赖是其成功的根本来源。</p>
<h2 id="_1">潜变量和因子分析</h2>
<p>奇异值分解$\mathbf X=\mathbf{UDV}^T$(14.54)有潜变量的表示。记$\mathbf S=\sqrt{N}\mathbf U$，以及$\mathbf A^T=\mathbf{DV}^T/\sqrt{N}$，我们有$\mathbf{X=SA}^T$，因此$\mathbf X$的每一列是$\mathbf S$的列的线性组合。现在因为$\mathbf U$是正交的，并且和之前一样假设$\mathbf X$的列均值为0（因此$\mathbf U$也是），这意味着$\mathbf S$列均值为0，而且不相关，有单位方差。采用随机变量，我们可以把SVD或者对应的主成分分析看成是下列潜变量模型的一个估计</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
X_1&=a_{11}S_1+a_{12}S_2+\cdots+a_{1p}S_p\\
X_2&=a_{21}S_1+a_{22}S_2+\cdots+a_{2p}S_p\\
\vdots &\qquad\vdots\\
X_p&=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pp}S_p
\end{align}
\qquad (14.78)
</script>
</p>
<p>或者简单地写成$X=\A S$。相关的$X_j$都表示称不相关的、单位方差的变量$S_\ell$的线性展开。尽管这不是太满意，因为对于任意给定的$p\times p$的正交矩阵$\R$，我们可以写出</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
X&=\A S\\
&=\A\R^T\R S
&=\A^*S^*
\end{align}
\qquad (14.79)
</script>
</p>
<p>并且$\Cov (S^*)=\R\Cov(S)\R^T=\I$。因此存在许多这样的分解，也因此不可能将任意特定的潜变量作为唯一的潜在来源。SVD分解确实有这样的性质，以最优的方式得到使得任意$q &lt; p$的截断分解。</p>
<p>经典的因子分析模型，主要由心理测量学(psychometrics)的研究者发展而来。某种程度上缓解了这个问题；举个例子，Mardia et al. (1979)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>。当$q &lt; p$，因子分析模型有如下形式</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
X_1&=a_{11}S_1+a_{12}S_2+\cdots+a_{1q}S_q+\varepsilon_1\\
X_2&=a_{21}S_1+a_{22}S_2+\cdots+a_{2q}S_q+\varepsilon_2\\
\vdots &\qquad\vdots\\
X_p&=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pq}S_q+\varepsilon_p
\end{align}
\qquad (14.80)
</script>
</p>
<p>或者写成$X=\A S+\epsilon$。这里$S$是 $q &lt; p$个揭示潜变量或者因子的向量，$\A$ 是$p\times q$的因子载荷(loadings)矩阵，$\varepsilon_j$是零均值不相干的扰动。想法是潜变量$S_\ell$是$X_j$公共方差的来源，意味着它们直接的相关性结构，而$\varepsilon_j$对每个$X_j$是唯一的，并且解释了剩下的方差。一般地，$S_\ell$和$\varepsilon_j$假设为高斯随机变量，并且采用极大似然法来拟合模型。参数都存在于下面的协方差阵中</p>
<p>
<script type="math/tex; mode=display">
\Sigma=\A\A^T+\D_\varepsilon \qquad (14.81)
</script>
</p>
<p>其中$\D_\varepsilon = diag[\Var(\varepsilon_1),\ldots, \Var(\varepsilon_p)]$. $S_\ell$为高斯并且不相关，这使得它们在统计上是独立随机变量。因此一系列的教育考试成绩可以认为是有潜在独立的因子，比如智力(intelligence)，动机(drive)等等所决定的. $\A$的列被称为因子载荷(factor loadings)，而且用来命名因子和解释因子。</p>
<p>不幸的是, 唯一性问题(14.79)仍然存在, 因为在式(14.81)中, 对于任意$q\times q$的正交矩阵$\R$, $\A$和 $\A\R^T$是等价的. 这导致了因子分析中的主观性, 因为用户可以寻找因子更易解释的旋转版本. 这点使得许多分析学家对因子分析表示怀疑, 而且这可能是它在当代统计中不受欢迎的原因。尽管我们这里不继续讨论细节，但是SVD分解在(14.81)的估计上发挥重要作用。举个例子，$\Var(\varepsilon_j)$假设相等，SVD的前$q$个成分确定了由$\A$张成的子空间。</p>
<p>因为每个$X_j$独立的扰动$\varepsilon_j$，因子分析可以看成是对$X_j$的相关结构进行建模，而非对协方差结构建模。这个可以通过对(14.81)的协方差结构进行标准化后得到（练习14.14）。</p>
<div class="admonition note">
<p class="admonition-title">weiya注: Ex. 14.14</p>
<p>练习14.14表明，因子分析实质上是对相关矩阵进行分解。详见解答见<a href="https://github.com/szcf-weiya/ESL-CN/issues/51">Ex. 14.14</a></p>
</div>
<p>这是因子分析与PCA的重要区别，尽管这不是我们讨论的重点。练习14.15讨论了一个简单的例子，因为这个差异，因子分析和主成分的解完全不同。</p>
<div class="admonition note">
<p class="admonition-title">weiya注: Ex. 14.15</p>
<p>练习14.15用一个实际例子说明了，因子分析和主成分得到的第一因子和第一主成分完全不同。详细解答见<a href="https://github.com/szcf-weiya/ESL-CN/issues/50">Ex. 14.15</a></p>
</div>
<h2 id="_2">独立成分分析</h2>
<p>独立成分分析(ICA)模型与(14.78)有相同的形式, 除了假设$S_\ell$是统计上独立而非不相关. </p>
<div class="admonition note">
<p class="admonition-title">weiya注: 独立与不相关</p>
<p>统计上, 连续型随机变量$X$与 $Y$独立的定义为
<script type="math/tex; mode=display">
p(x, y)=p_X(x)p_Y(y)\;\forall x,y
</script>
而不相关的定义为
<script type="math/tex; mode=display">
Cov(X, Y)=0
</script>
独立意味着不相关，但反之不对。对于二元正态随机变量，两者等价。</p>
</div>
<p>直观上, 不相关确定了多元变量分布的二阶交叉矩(协方差), 而一般地, 统计独立确定了所有的交叉矩. 这些额外的矩条件能让我们找到唯一的$\A$. 因为多元正态分布只要二阶矩就可以确定, 因此这是一个特例, 任意高斯独立成分可以像之前一样乘以一个旋转来确定. 因此如果假设$S_\ell$是独立且非高斯的, 则可以避免(14.78)和(14.80)的唯一性问题.</p>
<p>这里我们将要讨论(14.78)中的全$p$个成分的模型, 其中$S_\ell$是独立的且有单位方差; 因子分析模型(14.80)的ICA版本也同样存在. 我们的处理基于Hyvärinen and Oja (2000)<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>的综述文章.</p>
<p>我们希望恢复$X=\A S$中的混合矩阵$\A$. 不失一般性, 我们假设$X$已经白化(whitened)得到$\Cov(X)=\I$; 这一般可以通过前面描述的SVD实现. 反过来, 因为$S$的协方差也为$\I$, 这意味着$\A$是正交的. 所以求解ICA问题等价于寻找正交的$\A$使得随机变量向量$S=\A^T X$的组分是独立(且是非高斯的)。</p>
<p><img alt="" src="../../img/14/fig14.37.png" /></p>
<p>图14.37显示了在分离两个混合信号例子中ICA的能力。这也是经典的cocktail party problem的一个例子，不同的麦克风$X_j$接受来自不同独立源$S_\ell$(音乐、不同人说的话等等)的混合信号. ICA通过利用原始信号源的独立性和非高斯性，能够进行盲信号分离(blind source separation).</p>
<p>ICA许多流行的方式是基于熵. 密度为$g(y)$的随机变量$Y$的相对熵(differential entropy) $H$由下式给出</p>
<p>
<script type="math/tex; mode=display">
H(Y)=-\int g(y)\LOG g(y)dy\qquad (14.82)
</script>
</p>
<p>信息理论中一个著名的结论是在所有同方差的随机变量中，高斯随机变量有最大的熵。最后，随机向量$Y$的组分之间的互信息量(mutual information)$I(Y)$是独立性的一个自然度量:</p>
<p>
<script type="math/tex; mode=display">
I(Y)=\sum\limits_{j=1}^pH(Y_j)-H(Y)\qquad (14.83)
</script>
</p>
<p>值$I(Y)$称为$Y$的密度$g(y)$与其独立版本$\prod\limits_{j=1}^pg_j(y_j)$之间的Kullback-Leibler距离, 其中$g_j(y_j)$是 $Y_j$的边缘密度. 如果$X$有协方差$\I$, 且$Y=\A^TX$, 其中$\A$是正交, 则易证</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
I(Y)&= \sum\limits_{j=1}^pH(X)-\LOG\vert \det\A\vert\qquad (14.84)\\
&=\sum\limits_{j=1}^pH(Y_j)-H(X)\qquad (14.85)
\end{align}
</script>
</p>
<div class="admonition note">
<p class="admonition-title">weiya注: (14.85)的证明</p>
<p>只要证
<script type="math/tex; mode=display">
H(Y)=H(X)+\LOG \vert \det \A \vert
</script>
对(14.82)进行变量替换有
<script type="math/tex; mode=display">
H(Y)=-\int g(\A'x)\LOG g(\A'x)\cdot \vert \det \J\vert dx\qquad (*)
</script>
其中$\J_{ij}=\frac{\partial y_i}{\partial x_j}$.
又
<script type="math/tex; mode=display">
y_i = \sum\limits_{j=1}^p (\A')_{ij}x_j
</script>
故
<script type="math/tex; mode=display">
\J_{ij}=(\A')_{ij}
</script>
所以$\det\J = \det \A&rsquo;=\det \A$.
另外, $X$的密度函数为
<script type="math/tex; mode=display">
f(x)=g(\A'x)\cdot\vert \det\J\vert 
</script>
于是$(*)$式可以写成
<script type="math/tex; mode=display">
\begin{align}
H(Y) &= -\int \frac{f(x)}{\vert\det\A\vert} \LOG\frac{f(x)}{\vert\det\A\vert}\cdot\vert\det\A\vert dx\\
&=-\int f(x)[\LOG f(x)-\LOG\vert\det\A\vert]dx\\
&=H(X)+\LOG\vert\det\A\vert
\end{align}
</script>
证毕.</p>
</div>
<p>寻找$\A$来最小化$I(Y)=I(\A^TX)$也就是寻找使得组分间的独立性最强的正交变换. 考虑到式(14.84), 这等价于最小化$Y$的各组分的熵的和, 反过来意味着最大化它们与高斯分布的距离.</p>
<div class="admonition note">
<p class="admonition-title">weiya注</p>
<p>因为高斯随机变量的交叉熵最大, 则让各组分熵之和最小, 意味着各组分远离高斯分布.</p>
</div>
<p>为了简便, 与其采用熵$H(Y_j)$, Hyvärinen and Oja (2000)<sup id="fnref2:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>采用负熵(negentropy)$J(Y_j)$</p>
<p>
<script type="math/tex; mode=display">
J(Y_j) = H(Z_j)-H(Y_j)\qquad (14.86)
</script>
</p>
<p>其中$Z_j$是与$Y_j$同方差的高斯随机变量. 负熵是非负的, 它度量了$Y_j$与高斯随机变量之间的距离. 他们提出负熵的一个简单近似, 这个近似可以用来计算和优化数据. 图14.37至图14.39中的ICA都采用下面的近似</p>
<p>
<script type="math/tex; mode=display">
J(Y_j)\approx [EG(Y_j) - EG(Z_j)]^2\qquad (14.87)
</script>
</p>
<p>其中$G(u)=\frac 1a\LOG \cosh(au), 1\le a\le 2$. 当应用到实际数据时, 期望用数据的平均值代替. 这是这些作者们提供的<code>FastICA</code>软件中的一个选项. 更经典(以及不太鲁棒)的度量是基于四阶矩, 也因此可以通过峰度(kurtosis)来衡量与高斯分布的距离. 更多细节参见Hyvärinen and Oja (2000)<sup id="fnref3:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>. 在14.7.4节我们讨论他们寻找最优方向的近似牛顿算法.</p>
<div class="admonition note">
<p class="admonition-title">weiya注:</p>
<p>峰度是四阶标准矩
<script type="math/tex; mode=display">
Kurt(X)=\E\Big[
\Big(
\frac{X-\mu}{\sigma}
\Big)^4
\Big]
</script>
</p>
</div>
<p>总结一下, ICA应用到多元数据中, 来寻找一系列的正交投影, 使得投影数据尽可能远离高斯分布. 采用白化后的数据(协方差为$\I$), 这意味着寻找尽可能独立的组分.</p>
<p>ICA本质上从因子分析的一个解出发, 并且寻找一个旋转得到独立组分. 从这点看, ICA与在心理测量学中采用的传统方法“varimax”和“quartimax”一样, 仅仅是因子旋转的一种方式.</p>
<h3 id="_3">例子: 手写数字</h3>
<p>我们再次讨论14.5.1 节用PCA分析的手写数字“3”。 </p>
<p><img alt="" src="../../img/14/fig14.39.png" /></p>
<p>图14.39比较了前五个主成分(标准化)和前五个ICA成分， 都显示在标准化后的同一单位尺度下。 注意到每张图都是256维空间的二维投影。 所有PCA组分看上去都服从联合高斯分布，而ICA组分都服从长尾分布。这并不是很奇怪，因为PCA主要考虑方差，而ICA特地寻找非高斯的分布。所有的组分都已经标准化，所以我们看不出主成分的方差降低。</p>
<p><img alt="" src="../../img/14/fig14.40.png" /></p>
<p>图14.40展示了每个ICA主成分的两个极端的数据点， 以及均值的两个极端点。 这解释了每个组分的实际意义。 举个例子， 第五个ICA成分捕捉具有长扫尾(long sweeping tailed)的“3”。</p>
<h3 id="_4">例子: 时序脑电图数据</h3>
<p>ICA已经成为脑动力学(brain dynamics)中的重要工具， 这里介绍的例子采用ICA来理清多频脑电图(EEG)数据中信号的组分(Onton and Makeig, 2006)。</p>
<p>被试者戴上装有100个EEG电极的帽子， 这些用来记录头皮上不同部位的脑活动。</p>
<p><img alt="" src="../../img/14/fig14.41.png" /></p>
<p>图14.41(上图)显示了被试者在30分钟的周期内进行标准的&rdquo;two-back&rdquo;学习过程时， 这些电极中的9个电极在15秒内的输出结果。 大约间隔1500ms依次给被试者呈现一个字母(B, H, J, C, F或者K), 然后被试者通过按“是”或“否”的按钮来判断当前的字母与前两步出现的字母是否一致。</p>
<div class="admonition note">
<p class="admonition-title">weiya注: n-back</p>
<p>参考<a href="https://en.wikipedia.org/wiki/N-back">wiki: $n$-back</a>, $n$-back是指给被试者连续的刺激, 要求其判断当前刺激与前$n$步的刺激是否一致. 举个例子, 在3-back测试中, 若给被试者如下刺激
<img alt="" src="../../img/14/nback.png" />
则当前刺激为高亮部分时, 被试者应当判断&rdquo;是&rdquo;, 因为在前3步出现了高亮部分相同的刺激.</p>
</div>
<p>根据被试者的回答，他们得分或者失分，并且偶尔赚取bonus或者额外惩罚。脑电图信号中这个时序的数据表现出空间上的相关性———邻近的信号感受器看起来非常相似。</p>
<p>这里重要假设是每个头皮电极上记录的信号是从不同表皮活动以及非表皮区域的人工活动(如下文中提到的眨眼)中产生的独立的电势的混合。 更多ICA在这个领域的细节参见参考文献。</p>
<p>图14.41的下半部分展示了ICA组分的选择. 彩色图象用(画在头皮上的)热图表示未混合的系数向量的估计值$\hat a_j$, 它表明了活性的位置. 对应的时序数据展示了ICA组分的活动.</p>
<p>举个例子, 每次反馈信号之后被试者的眨眼(彩色的垂直直线), 它解释了IC1和IC3中的位置和人工信号. IC12是与心脏脉冲(cardiac pulse)有关的人工信号. IC4和IC7额骨(frontal)theta-band的活动, 而且这出现在回答正确后被试者身体的舒展. 更多细节参见Onton and Makeig (2006)<sup id="fnref4:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>中对这个例子的讨论, 以及ICA在脑电图模型中的应用.</p>
<h2 id="_5">探索投影寻踪</h2>
<p>Friedman and Tukey (1974)<sup id="fnref:3"><a class="footnote-ref" href="#fn:3" rel="footnote">3</a></sup>提出了探索投影寻踪(exploratory projection pursuit)，这是可视化高维数据的图象探索技巧。 他们的观点是高维数据的大多数低维（一维或二维）投影看起来是高斯分布的。 他们提出一系列projection indices的方法用于优化, 每个集中在与高斯分布的不同距离。自从他们最先提出该方法, 陆续有各种改进的建议(Huber, 1985<sup id="fnref:4"><a class="footnote-ref" href="#fn:4" rel="footnote">4</a></sup>; Friedman, 1987<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" rel="footnote">5</a></sup>)， 以及交互式图形软件包Xgobi(Swayne et al., 1991<sup id="fnref:6"><a class="footnote-ref" href="#fn:6" rel="footnote">6</a></sup>, 现在叫做GGobi)中实现的各种指标，包括熵。 这些投影指标与上文介绍的$J(Y_j)$形式一样， 其中$Y_j=a_j^TX$是$X$组分的标准化的线性组合。 实际上， 交叉熵的一些近似和替代与投影寻踪中提出的指标重合。 一般在投影寻踪中, 方向$a_j$不需要限制为正交。 Friedman (1987)<sup id="fnref2:5"><a class="footnote-ref" href="#fn:5" rel="footnote">5</a></sup>将数据在选定的投影上转换使之看起来像高斯分布， 然后搜索接下来的方向。 尽管他们的出发点不一样， 但是ICA和探索投影寻踪非常相似， 至少这里描述的表示形式。</p>
<h2 id="_6">独立分量分析的一种直接方法</h2>
<p>由定义知独立组分有如下联合乘积密度</p>
<p>
<script type="math/tex; mode=display">
f_S(s)=\prod\limits_{j=1}^pf_j(s_j)\qquad (14.88)
</script>
</p>
<p>所以这里展示一种采用广义可加模型(9.1节)来直接估计这个密度的方式. 全部细节可以在Hastie and Tibshirani (2003)<sup id="fnref:7"><a class="footnote-ref" href="#fn:7" rel="footnote">7</a></sup>中找到, 并且这个方法已经在<code>ProDenICA</code>的R包中实现了, 这可以在CRAN上下载.</p>
<p>根据与高斯分布的距离的表示, 我们将每个$f_j$写成</p>
<p>
<script type="math/tex; mode=display">
f_j(s_j)=\phi(s_j)e^{g_j(s_j)}\qquad (14.89)
</script>
</p>
<p>这是倾斜的高斯密度. 这里$\phi$是标准高斯分布密度, 并且$g_j$满足密度函数所要求的标准化条件. 和之前一样假设$X$已经预处理, 观测数据$X=\A S$的对数似然为</p>
<p>
<script type="math/tex; mode=display">
\ell(\A, \{g_j\}_1^p;\X)=\sum\limits_{i=1}^N\sum\limits_{j=1}^p[\LOG \phi(a_j^Tx_i)+g_j(a_j^Tx_i)]\qquad (14.90)
</script>
</p>
<div class="admonition note">
<p class="admonition-title">weiya注：</p>
<p>(14.90)原书中$\phi$有下标$j$， 但是因为$\phi$为标准正态， 所以应该直接忽略下标， 原书中后面的(14.91)也确实没有下标。</p>
</div>
<p>我们希望在$\A$为正交且(14.89)定义的$g_j$的条件下, 最大化上式. 对$g_j$不添加额外的约束, 模型$(14.90)$是过参数化的, 所以我们最大化下面的正则化版本</p>
<p>
<script type="math/tex; mode=display">
\sum\limits_{j=1}^p\Big[
\frac 1N \sum\limits{\LOG \phi(a_j^Tx_i) + g_j(a_j^Tx_i)} - 
\int \phi(t)e^{g_j(t)}dt - 
\lambda_j\int\{g_j^{(3)}\}^2(t)dt
\Big]\qquad (14.91)
</script>
</p>
<p>受Silverman (1986)<sup id="fnref:8"><a class="footnote-ref" href="#fn:8" rel="footnote">8</a></sup>的启发, 在(14.91)中, 我们(对每个$j$)减去了两个惩罚项:</p>
<ul>
<li>第一个在任意解$\hat g_j$上强制要求密度约束$\int \phi(t)e^{\hat g_j(t)}dt=1$</li>
<li>第二个是鲁棒性惩罚, 保证了解$\hat g_j$是结点在观测值$s_{ij}=a_j^Tx_i$的四次样条.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">weiya注：</p>
<p>下面说明为什么第一个惩罚项能够对$\phi(t)e^{\hat g_j(t)}$的密度进行约束。假设随机变量$X$的密度函数为$f(x)$， 其似然函数为$g(x)\propto \LOG f(x)$，这个问题归结为说明
<script type="math/tex; mode=display">
A_0(g)=\frac 1n\sum g(x_i)
</script>
在$\int e^g=1$的约束下的$\arg\max$等于
<script type="math/tex; mode=display">
A(g)=\frac 1n\sum g(x_i)-\int exp(g(x))dx
</script>
的$\arg\max$。
考虑$g^*=g-\LOG \int e^g$
则
<script type="math/tex; mode=display">
\int e^{g^*}=\int e^{g(x)}\cdot \frac{1}{\int e^{g(t)}dt}dx=1
</script>
于是
<script type="math/tex; mode=display">
A(g^*)=A(g)+\int exp(g(x))dx-\LOG(\int e^{g(t)}dt)-1
</script>
因$t-\LOG t\ge 1, \forall t&gt;0$，当且仅当$t=1$取等号，
则$A(g^*)\le A(g)$，也就是当且仅当$\int e^g = 1$时，$A(g)$会取得最大值，而这时候$A(g)=A_0(g)-1$，所以$A_0(g)$也会取得最大值。由此看出，$A_0(g)$在$\int e^g=1$的约束下的$\arg\max$等于（无约束的）$A(g)$的$\arg\max$。</p>
</div>
<p>可以进一步证明每个解密度$\hat f_j=\phi e^{\hat g_j}$的均值为0, 方差为1(练习14.18). 当我们增大$\lambda_j$, 这些解近似标准高斯密度$\phi$. </p>
<p><img alt="" src="../../img/14/alg14.3.png" /></p>
<p>如在算法14.3中描述, 以一种轮换的方式优化(14.91)来拟合函数$g_i$和方向$a_j$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Mardia, K., Kent, J. and Bibby, J. (1979). Multivariate Analysis, Academic Press.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Hyvärinen, A., Karhunen, J. and Oja, E. (2001). Independent Component Analysis, Wiley, New York.&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Friedman, J. and Tukey, J. (1974). A projection pursuit algorithm for exploratory data analysis, IEEE Transactions on Computers, Series C 23: 881–889.&#160;<a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Huber, P. (1985). Projection pursuit, Annals of Statistics 13: 435–475.&#160;<a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>Friedman, J. (1987). Exploratory projection pursuit, Journal of the American Statistical Association 82: 249–266.&#160;<a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:5" rev="footnote" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>Swayne, D., Cook, D. and Buja, A. (1991). Xgobi: Interactive dynamic graphics in the X window system with a link to S, ASA Proceedings of Section on Statistical Graphics, pp. 1–8.&#160;<a class="footnote-backref" href="#fnref:6" rev="footnote" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>Hastie, T. and Tibshirani, R. (2003). Independent components analysis through product density estimation, in S. T. S. Becker and K. Obermayer (eds), Advances in Neural Information Processing Systems 15, MIT Press, Cambridge, MA, pp. 649–656.&#160;<a class="footnote-backref" href="#fnref:7" rev="footnote" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>Silverman, B. (1986). Density Estimation for Statistics and Data Analysis, Chapman and Hall, London.&#160;<a class="footnote-backref" href="#fnref:8" rev="footnote" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
</ol>
</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://esl.hohoweiya.xyz/14 Unsupervised Learning/14.7 Independent Component Analysis and Exploratory Projection Pursuit/index.html";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "https://esl.hohoweiya.xyz/14 Unsupervised Learning/14.7 Independent Component Analysis and Exploratory Projection Pursuit/index.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://esl-hohoweiya-xyz.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//esl-hohoweiya-xyz.disqus.com/count.js" async></script></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
                <center>Copyright &copy; 2016-2018 weiya</center>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
        <script src="../../js/base.js"></script>
	<script src="../../js/jquery.dropdown.js"></script>
        <script src="../../search/require.js"></script>
        <script src="../../search/search.js"></script>
<!--
        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
      -->
    </body>
</html>