<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="The Elements of Statistical Learning(ESL) 的中文笔记">
        
        <link rel="canonical" href="https://esl.hohoweiya.xyz/03 Linear Methods for Regression/3.8 More on the Lasso and Related Path Algorithms/index.html">
	<link rel="shortcut icon" href="../../img/favicon.ico">

	<title>3.8 Lasso和相关路径算法的补充 - ESL CN</title>

        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../css/base.css?v201801062" rel="stylesheet">
        <link href="../../css/newsprint.css" rel="stylesheet">
        <link href="../../css/admonition_fix.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-85046550-1', 'auto');
          ga('send', 'pageview');

        </script>

        <script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<!--
	<style>
	  .dropdown-menu {
    max-height: 500px;
    overflow-y: auto;
    overflow-x: auto;
}
	</style>
-->
<script data-cfasync="false" type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes:true
},
TeX: {
  Macros: {

    A: "{\\mathbf{A}}",
    B: "{\\mathbf{B}}",
    C: "{\\mathbf{C}}",
    D: "{\\mathbf{D}}",
R: "{\\mathbf{R}}",
IR: "{\\mathrm{I\!R}}",
    S: "{\\mathbf{S}}",
I: "{\\mathbf{I}}",
J: "{\\mathbf{J}}",
X: "{\\mathbf{X}}",
Y: "{\\mathbf{Y}}",
U: "{\\mathbf{U}}",
V: "{\\mathbf{V}}",
W: "{\\mathbf{W}}",

LOG: "{\\mathrm{log}\\;}",
    E: "{\\mathrm{E}\\;}",
    1: "{\\boldsymbol 1}",
    Cov: "{\\mathrm{Cov}\\;}",
Var: "{\\mathrm{Var}\\;}",
det: "{\\mathrm{det}\\;}",
cosh: "{\\mathrm{cosh}\\;}",
arg: "{\\mathrm{arg}\\;}",
max: "{\\mathrm{max}\\;}",
min: "{\\mathrm{min}\\;}",

df: "{\\mathrm{df}}",
tr: "{\\mathrm{tr}}",

1: "{\\boldsymbol{1}}"

  },
  entensions: ["color.js"]
}
});
</script>
<script data-cfasync="false" type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2-beta.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    </head>
    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="">ESL CN</a>

        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="../../index.html">主页</a>
                </li>
            
            
            
                <li class="dropdown active">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">上篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">序言</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../Preface/2016-07-20-Preface-to-the-Second-Edition/index.html">第二版序言</a>
</li>

        
            
<li >
    <a href="../../Preface/2016-07-21-Preface-to-the-First-Edition/index.html">第一版序言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">1 简介</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../01 Introduction/2016-07-26-Chapter-1-Introduction/index.html">1.1 导言</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">2 监督学习概要</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.1 Introduction/index.html">2.1 导言</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.2 Variable Types and Terminology/index.html">2.2 变量类型和术语</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.3 Two Simple Approaches to Prediction/index.html">2.3 两种预测的简单方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.4 Statistical Decision Theory/index.html">2.4 统计判别理论</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.5 Local Methods in High Dimensions/index.html">2.5 高维问题的局部方法</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.6 Statistical Models, Supervised Learning and Function Approximation/index.html">2.6 统计模型，监督学习和函数逼近</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.7 Structured Regression Models/index.html">2.7 结构化的回归模型</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.8 Classes of Restricted Estimators/index.html">2.8 限制性估计的类别</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/2.9 Model Selection and the Bias-Variance Tradeoff/index.html">2.9 模型选择和偏差-方差的权衡</a>
</li>

        
            
<li >
    <a href="../../02 Overview of Supervised Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">3 回归的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../3.1 Introduction/index.html">3.1 导言</a>
</li>

        
            
<li >
    <a href="../3.2 Linear Regression Models and Least Squares/index.html">3.2 线性回归模型和最小二乘法</a>
</li>

        
            
<li >
    <a href="../3.3 Subset Selection/index.html">3.3 子集的选择</a>
</li>

        
            
<li >
    <a href="../3.4 Shrinkage Methods/index.html">3.4 收缩的方法</a>
</li>

        
            
<li >
    <a href="../3.5 Methods Using Derived Input Directions/index.html">3.5 运用派生输入方向的方法</a>
</li>

        
            
<li >
    <a href="../3.6 A Comparison of the Selection and Shrinkage Methods/index.html">3.6 选择和收缩方法的比较</a>
</li>

        
            
<li >
    <a href="../3.7 Multiple Outcome Shrinkage and Selection/index.html">3.7 多重输出的收缩和选择</a>
</li>

        
            
<li class="active">
    <a href="index.html">3.8 Lasso和相关路径算法的补充</a>
</li>

        
            
<li >
    <a href="../3.9 Computational Considerations/index.html">3.9 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">4 分类的线性方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.1 Introduction/index.html">4.1 导言</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.2 Linear Regression of an Indicator Matrix/index.html">4.2 指示矩阵的线性回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.3 Linear Discriminant Analysis/index.html">4.3 线性判别分析</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.4 Logistic Regression/index.html">4.4 逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/4.5 Separating Hyperplanes/index.html">4.5 分离超平面</a>
</li>

        
            
<li >
    <a href="../../04 Linear Methods for Classification/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">5 基展开和正规化</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.1 Introduction/index.html">5.1 导言</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.2 Piecewise Polynomials and Splines/index.html">5.2 分段多项式和样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.3 Filtering and Feature Extraction/index.html">5.3 滤波和特征提取</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.4 Smoothing Splines/index.html">5.4 光滑样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters/index.html">5.5 光滑参数的自动选择</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.6 Nonparametric Logistic Regression/index.html">5.6 非参逻辑斯蒂回归</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.7-Multidimensional-Splines/index.html">5.7 多维样条</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html">5.8 正则化和再生核希尔伯特空间理论</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html">5.9 小波光滑</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
            
<li >
    <a href="../../05 Basis Expansions and Regularization/Appendix-Computations-for-B-splines/index.html">附录-B样条的计算</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">6 核光滑方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.0 Introduction/index.html">6.0 导言</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.1 One-Dimensional Kernel Smoothers/index.html">6.1 一维核光滑器</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.2 Selecting the Width of the Kernel/index.html">6.2 选择核的宽度</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.3 Local Regression in R^p/index.html">6.3 $R^p$中的局部回归</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.4 Structured Local Regression Models in R^p/index.html">6.4 $R^p$中的结构化局部回归模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.5 Local Likelihood and Other Models/index.html">6.5 局部似然和其他模型</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.6 Kernel Density Estimation and Classification/index.html">6.6 核密度估计和分类</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.7 Radial Basis Functions and Kernels/index.html">6.7 径向基函数和核</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification/index.html">6.8 混合模型的密度估计和分类</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/6.9-Computational-Consoderations/index.html">6.9 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../06 Kernel Smoothing Methods/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">中篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">7 模型评估及选择</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.1 Introduction/index.html">7.1 导言</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.2 Bias, Variance and Model Complexity/index.html">7.2 偏差，方差和模型复杂度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.3 The Bias-Variance Decomposition/index.html">7.3 偏差-方差分解</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.4 Optimism of the Training Error Rate/index.html">7.4 测试误差率的optimism</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.5 Estimates of In-Sample Prediction Error/index.html">7.5 样本内预测误差的估计</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.6 The Effective Number of Parameters/index.html">7.6 参数的有效个数</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.7 The Bayesian Approach and BIC/index.html">7.7 贝叶斯方法和BIC</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.8 Minimum Description Length/index.html">7.8 最小描述长度</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.9 Vapnik-Chervonenkis Dimension/index.html">7.9 VC维</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.10 Cross-Validation/index.html">7.10 交叉验证</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.11 Bootstrap Methods/index.html">7.11 自助法</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/7.12 Conditional or Expected Test Error/index.html">7.12 条件测试误差或期望测试误差</a>
</li>

        
            
<li >
    <a href="../../07 Model Assessment and Selection/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">8 模型推断和平均</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.1 Introduction/index.html">8.1 导言</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.2 The Bootstrap and Maximum Likelihood Methods/index.html">8.2 自助法和最大似然法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.3 Bayesian Methods/index.html">8.3 贝叶斯方法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.4 Relationship Between the Bootstrap and Bayesian Inference/index.html">8.4 自助法和贝叶斯推断之间的关系</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.5 The EM Algorithm/index.html">8.5 EM算法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.6 MCMC for Sampling from the Posterior/index.html">8.6 MCMC向后采样</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.7 Bagging/index.html">8.7 袋装法</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.8 Model Averaging and Stacking/index.html">8.8 模型平均和堆栈</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/8.9 Stochastic Search/index.html">8.9 随机搜索</a>
</li>

        
            
<li >
    <a href="../../08 Model Inference and Averaging/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">9 增广模型，树，以及相关方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.0 Introduction/index.html">9.0 导言</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.1 Generalized Additive Models/index.html">9.1 广义加性模型</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.2 Tree-Based Methods(CART)/index.html">9.2 基于树的方法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.3 PRIM(Bump Hunting)/index.html">9.3 耐心规则归纳法</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.4 MARS(Multivariate Adaptive Regression Splines)/index.html">9.4 多变量自适应回归样条</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.5 Hierarchical Mixtures of Experts/index.html">9.5 专家的系统混合</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.6 Missing Data/index.html">9.6 缺失数据</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/9.7 Computational Considerations/index.html">9.7 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../09 Additive Models, Trees, and Related Methods/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">10 增强和加性树</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.1 Boosting Methods/index.html">10.1 增强方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.2 Boosting Fits an Additive Model/index.html">10.2 boosting拟合可加模型</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.3 Forward Stagewise Additive Modeling/index.html">10.3 向前逐步加性建模</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.4 Exponential Loss and AdaBoost/index.html">10.4 指数损失和AdaBoost</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.5 Why Exponential Loss/index.html">10.5 为什么是指数损失</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.6 Loss Functions and Robustness/index.html">10.6 损失函数和鲁棒性</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.7 Off-the-Shelf Procedures for Data Mining/index.html">10.7 数据挖掘的现货方法</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.8 Spam Data/index.html">10.8 垃圾邮件的例子</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.9 Boosting Trees/index.html">10.9 boosting树</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.10 Numerical Optimization via Gradient Boosting/index.html">10.10 利用梯度boosting的数值优化</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.11 Right-Sized Trees for Boosting/index.html">10.11 大小合适的boosting树</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/10.12 Regularization/index.html">10.12 正则化</a>
</li>

        
            
<li >
    <a href="../../10 Boosting and Additive Trees/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">11 神经网络</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../11 Neural Networks/11.1 Introduction/index.html">11.1 导言</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.2 Projection Pursuit Regression/index.html">11.2 投影寻踪回归</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.3 Neural Networks/index.html">11.3 神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.4 Fitting Neural Networks/index.html">11.4 拟合神经网络</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.5 Some Issues in Training Neural Networks/index.html">11.5 训练神经网络的一些问题</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.6 Example of Simulated Data/index.html">11.6 模拟数据的例子</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/11.7-Example-ZIP-Code-Data/index.html">11.7 邮编数字的例子</a>
</li>

        
            
<li >
    <a href="../../11 Neural Networks/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">12 支持向量机和灵活的判别方法</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.1 Introduction/index.html">12.1 导言</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.2 The Support Vector Classifier/index.html">12.2 支持向量分类器</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/12.3 Support Vector Machines and Kernels/index.html">12.3 支持向量机和核</a>
</li>

        
            
<li >
    <a href="../../12 Support Vector Machines and Flexible Discriminants/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">下篇 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">13 原型方法和最近邻</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.1 Introduction/index.html">13.1 导言</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.2 Prototype Methods/index.html">13.2 原型方法</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.3 k-Nearest-Neighbor Classifiers/index.html">13.3 k最近邻分类器</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.4 Adaptive Nearest-Neighbor Methods/index.html">13.4 自适应的最近邻方法</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/13.5 Computational Considerations/index.html">13.5 计算上的考虑</a>
</li>

        
            
<li >
    <a href="../../13 Prototype Methods and Nearest-Neighbors/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">14 非监督学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../14 Unsupervised Learning/14.1 Introduction/index.html">14.1 导言</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.2 Association Rules/index.html">14.2 关联规则</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.3 Cluster Analysis/index.html">14.3 聚类分析</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.4 Self-Organizing Maps/index.html">14.4 自组织图</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.5-Principal-Components-Curves-and-Surfaces/index.html">14.5 主成分，主曲线以及主曲面</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.6 Non-negative Matrix Factorization/index.html">14.6 非负矩阵分解</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.7 Independent Component Analysis and Exploratory Projection Pursuit/index.html">14.7 独立成分分析和探索投射寻踪</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.8 Multidimensional Scaling/index.html">14.8 多维缩放</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling/index.html">14.9 非线性降维和局部多维缩放</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/14.10 The Google PageRank Algorithm/index.html">14.10 谷歌的PageRank算法</a>
</li>

        
            
<li >
    <a href="../../14 Unsupervised Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">15 随机森林</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../15 Random Forests/15.1 Introduction/index.html">15.1 导言</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.2 Definition of Random Forests/index.html">15.2 随机森林的定义</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.3 Details of Random Forests/index.html">15.3 随机森林的细节</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/15.4-Analysis-of-Random-Forests/index.html">15.4 随机森林的分析</a>
</li>

        
            
<li >
    <a href="../../15 Random Forests/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">16 集成学习</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../16 Ensemble Learning/16.1 Introduction/index.html">16.1 导言</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/16.2 Boosting and Regularization Paths/index.html">16.2 增强和正则路径</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/16.3 Learning Ensembles/index.html">16.3 学习集成</a>
</li>

        
            
<li >
    <a href="../../16 Ensemble Learning/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">17 无向图模型</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.1 Introduction/index.html">17.1 导言</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.2 Markov Graphs and Their Properties/index.html">17.2 马尔科夫图及其性质</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.3 Undirected Graphical Models for Continuous Variables/index.html">17.3 连续变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/17.4 Undirected Graphical Models for Discrete Variables/index.html">17.4 离散变量的无向图模型</a>
</li>

        
            
<li >
    <a href="../../17 Undirected Graphical Models/Bibliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">18 高维问题</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.1 When p is Much Bigger than N/index.html">18.1 当p大于N</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.2 Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids/index.html">18.2 对角线性判别分析和最近收缩重心</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.3 Linear Classifiers with Quadratic Regularization/index.html">18.3 二次正则的线性分类器</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.4 Linear Classifiers with L1 Regularization/index.html">18.4 一次正则的线性分类器</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.5 Classification When Features are Unavailable/index.html">18.5 当特征不可用时的分类</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.6 High-Dimensional Regression/index.html">18.6 有监督的主成分</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/18.7 Feature Assessment and the Multiple-Testing Problem/index.html">18.7 特征评估和多重检验问题</a>
</li>

        
            
<li >
    <a href="../../18 High-Dimensional Problems/Bioliographic Notes/index.html">文献笔记</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">个人笔记 <b class="caret"></b></a>
                    <ul class="dropdown-menu">
                    
                        
  <li class="dropdown-submenu scrollable-menu">
    <a tabindex="-1" class="nav-title">模拟实验</a>
    <ul class="dropdown-menu scrollable-menu">
        
            
<li >
    <a href="../../notes/sim73/index.html">模型选择7.3</a>
</li>

        
            
<li >
    <a href="../../notes/sim77/index.html">模型选择7.7</a>
</li>

        
            
<li >
    <a href="../../notes/ICA/index.html">ICA模拟实验</a>
</li>

        
    </ul>
  </li>

                    
                    </ul>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
              <!--
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> 搜索
                    </a>
                </li>
              -->
              <!--
                <li>
                  <a href="https://esl.hohoweiya.xyz/03 Linear Methods for Regression/3.8 More on the Lasso and Related Path Algorithms/index.html#disqus_thread">0 Comments</a>
                </li>
              -->
                <li >
                    <a rel="next" href="../3.7 Multiple Outcome Shrinkage and Selection/index.html">
                        <i class="fa fa-arrow-left"></i> 上一节
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../3.9 Computational Considerations/index.html">
                        下一节 <i class="fa fa-arrow-right"></i>
                    </a>
                </li>


                
                <!--
                <li>
                    <a href="https://github.com/szcf-weiya/">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
              -->
                <li>
                  <a href="https://github.com/szcf-weiya/"><i class="fa fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                <li>
                  <a href="https://stats.hohoweiya.xyz"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                <li>
                  <a href="https://blog.hohoweiya.xyz"><i class="fa fa-pencil" aria-hidden="true"></i> 随笔</a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#38-lasso">3.8 Lasso和相关路径算法的补充</a></li>
        
            <li><a href="#_1">增长的向前逐渐回归</a></li>
        
            <li><a href="#_2">分段线性路径算法</a></li>
        
            <li><a href="#dantzig">Dantzig选择器</a></li>
        
            <li><a href="#the-grouped-lasso">The Grouped Lasso</a></li>
        
            <li><a href="#lasso">lasso的更多性质</a></li>
        
            <li><a href="#pathwise-coordinate-optimization">Pathwise Coordinate Optimization</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">
<h1 id="38-lasso">3.8 Lasso和相关路径算法的补充</h1>
<table>
<thead>
<tr>
<th>原文</th>
<th><a href="../../book/The Elements of Statistical Learning.pdf">The Elements of Statistical Learning</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>翻译</td>
<td>szcf-weiya</td>
</tr>
<tr>
<td>时间</td>
<td>2016-10-21:2016-10-21</td>
</tr>
<tr>
<td>更新</td>
<td>2017-12-05</td>
</tr>
</tbody>
</table>
<p>自从LAR算法（Efron et al., 2004<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>）的提出，许多研究都在发展对于不同问题的正则化拟合算法。另外，$L_1$正则有它自己的用处，它引领了信号处理领域的压缩传感(compressed sensing).（Donoho, 2006a<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>; Candes, 2006<sup id="fnref:3"><a class="footnote-ref" href="#fn:3" rel="footnote">3</a></sup>）在这部分我们讨论一些相关的想法和以LAR算法为先驱的其它路径算法。</p>
<h2 id="_1">增长的向前逐渐回归</h2>
<p>这里我们提出另外一种类似LAR的算法，这次集中在向前逐渐回归。有趣地是，理解一个灵活的非线性回归过程的努力导出了线性模型的一个新算法（LAR）。在阅读本书的第一版时，第16章的向前逐渐算法16.1，Brad Efron意识到对于线性模型，可以明确地构造出如图3.10所示的分段线性的lasso路径。这促使他提出3.4.4节介绍的LAR过程，以及这里提到的向前逐渐回归(forward-stagewise regression)的增长版本。</p>
<p><img alt="" src="../../img/03/alg3.4.png" /></p>
<!--
****
**算法 3.4** 增长的向前逐渐回归——$FS_\epsilon$
****
1. 从残差向量$\mathbf r$等于$\mathbf y$开始，$\beta_1,\beta_2,\ldots,\beta_p=0$. 所有的预测变量进行标准化使得均值为0、方差为1.
2. 寻找与残差向量$\mathbf r$最相关的预测变量$\mathbf x_j$
3. 更新$\beta_j\leftarrow\beta_j+\delta_j$, 其中$\delta_j=\epsilon\cdot sign[\langle \mathbf x_j,\mathbf r\rangle]$并且$\epsilon>0$是一个很小的步长，然后令$\mathbf r=\mathbf r-\delta_j\mathbf x_j$
4. 重复步骤2和步骤3，直到所有的残差向量与所有的预测变量都不相关。

****
-->

<p>考虑16.1节提出的向前逐渐boosting(forward-stagewise boosting)算法16.1的线性版本。它通过重复更新与当前残差最相关的变量的系数（乘以一个小量$\epsilon$）得到系数曲线。算法3.4给出了具体的细节。图3.19（左边）展示了前列腺癌数据中步长$\epsilon=0.01$的过程。如果$\delta_j=\langle \mathbf x_j,\mathbf r\rangle$（残差在第$j$个预测变量的最小二乘系数），则这恰恰是3.3节中介绍的一般向前逐渐过程（FS）。</p>
<p><img alt="" src="../../img/03/fig3.19.png" /></p>
<p>这里我们主要对小的$\epsilon$值感兴趣。令$\epsilon\rightarrow0$则有图3.19的右图，在这种情形下与图3.10的lasso路径相同。我们称这个极限过程为无穷小的向前逐渐回归(infinitesimal forward stagewise regression)或者$FS_0$。这个过程在非线性、自适应方法中有着很重要的作用，比如boosting（第10和16章），并且是增长的向前逐渐回归的，这是最能禁得起理论分析的版本。由于它与boosting的关系，Buhlmann and Hothorn (2007)<sup id="fnref:4"><a class="footnote-ref" href="#fn:4" rel="footnote">4</a></sup>称这个过程为&rdquo;L2boost&rdquo;</p>
<p>Efron最初认为LAR算法3.2是$FS_0$的一个实现，允许每个连结变量(tied predictor)以一种平衡的方式更新他们的系数，并且在相关性方面保持连结。然而，他接着意识到LAR在这些连结预测变量中的最小二乘拟合可以导致系数向相反的方向移动到它们的相关系数，这在算法3.4中是不可能发生的。下面对LAR算法的修正实现了$FS_0$：</p>
<div class="admonition note">
<p class="admonition-title">weiya注</p>
<p>直观上看，在算法3.2的第4步中，系数朝着联合最小二乘方向移动，注意此时方向与最小二乘方向可能一致或者相反。然而算法3.4中的移动方向始终与最小二乘方向保持一致。因此需要算法3.2b的修改。</p>
</div>
<p><img alt="" src="../../img/03/alg3.2b.png" /></p>
<!--
****
**算法 3.2b** 最小角回归：$FS_0$修正
****
4.通过求解下面的约束最小二乘问题找到新的方向
$$
\underset{b}{min}\Vert\mathbf r-\mathbf X_{\cal A}b\Vert^2_2 \;s.t.\; b_js_j\ge 0,\;j\in\cal A
$$
其中，$s_j$是$\langle\mathbf x_j,\mathbf r \rangle$的方向。
****
-->

<p>这个修正相当于一个非负的最小二乘拟合, 保持系数的符号与相关系数的符号一致。可以证明它实现了对于最大相关性的连结变量的无限小&rdquo;更新&rdquo;的最优平衡。（Hastie et al.，2007<sup id="fnref:5"><a class="footnote-ref" href="#fn:5" rel="footnote">5</a></sup>）。类似lasso，全$FS_0$路径可以通过LAR算法非常有效地计算出来。</p>
<p>作为这些事实的结果，如果LAR图象是单调不减或者单调不增，如3.19所示，则LAR，lasso，以及$FS_0$这三种算法给出了相同的图象。如果图象不是单调的但是不穿过0，则LAR和lasso是一样的。</p>
<p>因为$FS_0$与lasso不同，很自然地问它是否优化了准则。答案比lasso更加的复杂；$FS_0$系数曲线是微分方程的一个解。尽管lasso在降低系数向量$\beta$的$L_1$范数的单位残差平方和增长方面实现了最优化，但$FS_0$在沿着系数路径的$L_1$弧长的单位增长是最优的。因此它的系数曲线不会经常改变方向。</p>
<p>$FS_0$比lasso的约束更强，事实上也可以看成是lasso的单调版本；见图16.3生动的例子。$FS_0$可能在$p&gt;&gt;N$情形下很有用，它的系数曲线会更加的光滑，因此比lasso有更小的方差。更多关于$FS_0$的细节将在16.2.3节以及给出以及Hastie et al. (2007)<sup id="fnref2:5"><a class="footnote-ref" href="#fn:5" rel="footnote">5</a></sup>。图3.16包含了$FS_0$, 它的表现非常类似于lasso。</p>
<h2 id="_2">分段线性路径算法</h2>
<p>最小角回归过程探索了lasso解的路径分段线性的本质。这导出了其他正则化问题类似的“路径算法”。假设我们求解</p>
<p>
<script type="math/tex; mode=display">
\hat \beta(\lambda)=\mathrm{argmin}_\beta[R(\beta)+\lambda J(\beta)]\qquad (3.76)
</script>
</p>
<p>
<script type="math/tex; mode=display">
R(\beta)=\sum\limits_{i=1}^NL(y_i,\beta_0+\sum_{j=1}^px_{ij}\beta_j)\qquad (3.77)
</script>
</p>
<p>其中损失函数$L$和惩罚函数$J$都是凸函数。则下面是解的路径$\hat\beta(\lambda)$为分段线性的充分条件(Rosset and Zhu, 2007)<sup id="fnref:6"><a class="footnote-ref" href="#fn:6" rel="footnote">6</a></sup></p>
<ol>
<li>$R$作为$\beta$的函数是二次的或者是分段二次</li>
<li>$J$关于$\beta$分段线性</li>
</ol>
<p>这也意味着（原则上）解的路径可以有效地计算出来。例子包括平方损失和绝对误差损失，“Huberized”损失，以及关于$\beta$的$L_1, L_\infty$ 惩罚。另一个例子是支持向量机中的“hinge loss”。那里损失是分段线性，惩罚是二次的。有趣的是，这导出了对偶空间的分段线性路径算法；更多的细节在12.3.5节给出。</p>
<h2 id="dantzig">Dantzig选择器</h2>
<p>Candes and Tao (2007)<sup id="fnref:7"><a class="footnote-ref" href="#fn:7" rel="footnote">7</a></sup> 提出下面的准则：</p>
<p>
<script type="math/tex; mode=display">
\mathrm{min}_\beta\Vert\beta\Vert_1\text{ subject to }\Vert \mathbf X^T(\mathbf y-\mathbf X\beta)\Vert_\infty\le s\qquad (3.78)
</script>
</p>
<p>他们称这个解为Dantzig选择器(DS)。可以等价地写成</p>
<p>
<script type="math/tex; mode=display">
\mathrm{min}_\infty \Vert \mathbf X^T(\mathbf y-\mathbf X\beta)\Vert_\infty\text{ subject to } \Vert\beta\Vert\le t\qquad (3.79)
</script>
</p>
<p>这里$\Vert\cdot\Vert_\infty$为$L_\infty$范数，也就是该向量中绝对值最大的组分。这种形式类似lasso，用梯度绝对值的最大值替换平方误差损失。注意到当$t$变大，如果$N&lt;p$两个过程都得到最小二乘解。如果$p\ge N$，它们都得到最小的$L_1$范数的最小二乘解。然而，对于较小的$t$，DS过程得到与lasso不同的解的路径。</p>
<p>Candes and Tao (2007)<sup id="fnref2:7"><a class="footnote-ref" href="#fn:7" rel="footnote">7</a></sup>证明了求解DS是线性规划问题；为了纪念George Dantzig（线性规划中单纯形法的发明者），因此称为Dantzig。他们也证明了该方法的一系列有趣的数学问题，这些性质与重建潜在的稀疏系数向量的能力有关。如Bickel et al. (2008)<sup id="fnref:13"><a class="footnote-ref" href="#fn:13" rel="footnote">13</a></sup>所证明，这些性质对于lasso也适用。</p>
<p>不幸的是DS方法的运算性质有些不足。这个方法想法上与lasso类似，特别是当我们观察lasso的平稳条件（3.58）。和LAR算法一样，对于活跃集中的所有变量，lasso保持着与当前残差相同的内积（以及相关系数），并且将它们的系数向残差平方和的最优下降方向变化。在这个过程中，相同的相关系数单调下降（练习3.23），并且在任何时刻这个相关性大于非活跃集中的变量。而Dantzig选择器试图最小化当前残差与所有变量之间的最大内积。因此它可以达到比lasso更小的最大值，但是在这一过程中会发生奇怪的现象。如果活跃集的大小为$m$，则会有$m$个变量与最大相关性绑在一起。然而，这些变量不需要与活跃集重合！因此它可以在模型中包含这样一个变量，其与当前残差的相关性小于不在活跃集中的变量与残差的相关性(Efron et al., 2007<sup id="fnref:14"><a class="footnote-ref" href="#fn:14" rel="footnote">14</a></sup>)。这似乎不合理，而且有时会导致较差的预测误差。Efron et al. (2007)也证明了随着正则化参数$s$的变化，DS可能得到非常不稳定的系数曲线。</p>
<h2 id="the-grouped-lasso">The Grouped Lasso</h2>
<p>在一些问题中，预测变量属于预定义的群体中；举个例子，属于同一个生物路径的基因，或者表示类别型数据层次的指示变量（哑变量）。在这种情形中，或许想要对群体中每个成员一起进行收缩或者选择操作。grouped lasso便是一种实现方式。假设$p$个预测变量被分到$L$个群中，在第$\ell$个群中有$p_\ell$个成员。为了简便，我们采用矩阵$\mathbf X_\ell$来表示对应第$\ell$个群的预测变量，对应的系数向量为$\beta_\ell$。grouped-lasso最小化下列的凸准则</p>
<p>
<script type="math/tex; mode=display">
\underset{\beta\in R^p}{\mathrm{min}}(\Vert \mathbf y-\beta_0\boldsymbol 1-\sum\limits_{\ell=1}^L\mathbf X_\ell\beta_\ell\Vert_2^2+\lambda \sum\limits_{\ell=1}^L\sqrt{p_\ell}\Vert \beta_\ell\Vert_2)\qquad (3.80)
</script>
</p>
<p>其中$\sqrt{p_\ell}$项对应不同的群体大小，并且$\Vert\cdot\Vert_2$是欧几里得范数。因为一个向量$\beta_\ell$的欧式范数为0当且仅当其各组分都为0，这个过程保证了群体层次和个体水平的稀疏性。也就是，对于某些$\lambda$，预测变量的整个群体都排除在模型之外。这个过程由Bakin (1999)<sup id="fnref:8"><a class="footnote-ref" href="#fn:8" rel="footnote">8</a></sup>和Lin and Zhang (2006)<sup id="fnref:9"><a class="footnote-ref" href="#fn:9" rel="footnote">9</a></sup>提出，以及Yuan and Lin (2007)<sup id="fnref:10"><a class="footnote-ref" href="#fn:10" rel="footnote">10</a></sup>的研究和推广。推广包括更一般的$L_2$范数$\Vert \eta\Vert=(\eta^TK\eta)^{1/2}$，并且允许重复的预测变量(Zhao et al., 2008<sup id="fnref:11"><a class="footnote-ref" href="#fn:11" rel="footnote">11</a></sup>)。拟合离散的可加模型的方法之间也有联系（Lin and Zhang, 2006<sup id="fnref2:9"><a class="footnote-ref" href="#fn:9" rel="footnote">9</a></sup>; Ravikumar et al., 2008<sup id="fnref:12"><a class="footnote-ref" href="#fn:12" rel="footnote">12</a></sup>）</p>
<h2 id="lasso">lasso的更多性质</h2>
<p>许多作者已经研究了当$N$和$p$增长时，lasso的能力以及重建模型相关的过程。这个工作的例子有Knight and Fu (2000)<sup id="fnref:15"><a class="footnote-ref" href="#fn:15" rel="footnote">15</a></sup>, Greenshtein and Ritov (2004)<sup id="fnref:16"><a class="footnote-ref" href="#fn:16" rel="footnote">16</a></sup>, Tropp (2004)<sup id="fnref:17"><a class="footnote-ref" href="#fn:17" rel="footnote">17</a></sup>, Donoho (2006b)<sup id="fnref:18"><a class="footnote-ref" href="#fn:18" rel="footnote">18</a></sup>, Meinshausen (2007)<sup id="fnref:19"><a class="footnote-ref" href="#fn:19" rel="footnote">19</a></sup>, Meinshausen and Bühlmann (2006)<sup id="fnref:20"><a class="footnote-ref" href="#fn:20" rel="footnote">20</a></sup>, Tropp (2006)<sup id="fnref:21"><a class="footnote-ref" href="#fn:21" rel="footnote">21</a></sup>, Zhao and Yu (2006)<sup id="fnref:22"><a class="footnote-ref" href="#fn:22" rel="footnote">22</a></sup>, Wainwright (2006)<sup id="fnref:23"><a class="footnote-ref" href="#fn:23" rel="footnote">23</a></sup>, 以及 Bunea et al. (2007)<sup id="fnref:24"><a class="footnote-ref" href="#fn:24" rel="footnote">24</a></sup>。举个例子，Donoho (2006b)<sup id="fnref2:18"><a class="footnote-ref" href="#fn:18" rel="footnote">18</a></sup>集中在$p&gt;N$的情形并且当边界$t$变大时lasso的解。极限情形下，这给出了在所有零训练误差的模型中最小的$L_1$范数解。他证明了对模型矩阵$\mathbf X$加上具体的假设，如果真实模型为稀疏的，则解能以高概率识别出正确的预测变量。</p>
<p>许多这领域的结果对模型矩阵假设了如下条件</p>
<p>
<script type="math/tex; mode=display">
\underset{j\in \cal S^c}{\mathrm{max}}\Vert \mathbf x_j^T\mathbf X_{\cal S}(\mathbf X_{\cal S^T\mathbf X_{\cal S}})^{-1}\Vert_1\le (1-\epsilon)\text{ for some }\epsilon\in (0, 1]\qquad (3.81)
</script>
</p>
<p>这里$\cal S$标记真实的潜在模型中非零系数特征的子集，$\mathbf X_{\cal S}$是$\mathbf X$中对应的这些特征的列。类似地，$\cal S^c$是真实系数为0的特征的集合，$\mathbf X_{\cal S^c}$是对应的列。这说明$\mathbf X_{\cal S^c}$的列在$\mathbf X_{\cal S}$上的最小二乘系数不会太大，也就是，“好”的变量$\cal S$与多余的变量$\cal S^c$直接不是高度相关。</p>
<p>考虑这些系数本身，lasso收缩导致非零系数的估计偏向0，并且一般地他们不是一致的。降低这种偏差的一种方式是运行lasso来识别非零系数的集合，接着对选出的特征进行无约束线性模型拟合。另外，也可以采用lasso来选择非零预测变量，接着再次运用lasso，但是从第一步开始便只用选择出的变量。这称为relaxed lasso (Meinshausen, 2007<sup id="fnref2:19"><a class="footnote-ref" href="#fn:19" rel="footnote">19</a></sup>)。这个想法是采用交叉验证来估计lasso初始的惩罚参数，然后接着对选择出的变量再用一次惩罚参数。因为第二步中的变量与噪声变量之间的竞争变小，所以交叉验证会趋向于选择较小的$\lambda$，因此它们的系数会比初始估计时收缩得要小。</p>
<p>另外，也可以修改lasso惩罚函数使得更大的系数收缩得不要太剧烈；Fan and Li (2005)<sup id="fnref:25"><a class="footnote-ref" href="#fn:25" rel="footnote">25</a></sup>的平稳削减绝对偏差法(smoothly clipped absolute deviation, SCAD)用$J_a(\beta,\lambda)$替换$\lambda\vert\beta \vert$，其中对于$a\ge 2$</p>
<p>
<script type="math/tex; mode=display">
\frac{dJ_a(\beta,\lambda)}{d\beta}=\lambda \cdot \mathrm{sign}(\beta)[I(\vert \beta\vert\le \lambda)+\frac{(a\lambda-\vert \beta\vert)_\+}{(a-1)\lambda}I(\vert\beta\vert>\lambda)]\qquad (3.82)
</script>
</p>
<p>方括号中的第二项降低了lasso对于较大$\beta$的收缩程度，在极限状态时，当$a\rightarrow \infty$，没有收缩。图3.20显示了SCAD惩罚，以及lasso和$\beta$^{1-\nu}$。然而这个准则不是凸的，这是一个缺陷，因为它会使得计算变得很困难。adaptive lasso (Zou, 2006)<sup id="fnref:26"><a class="footnote-ref" href="#fn:26" rel="footnote">26</a></sup>采用形如$\sum_{j=1}^pw_j\vert\beta_j\vert$的加权惩罚，其中$w_j=1/\vert\hat\beta_j\vert^\nu$，$\hat\beta_j$是一般最小二乘估计并且$\nu&gt;0$。这是对3.4.3节中讨论的$\vert\beta\vert^q$惩罚的实际近似（这里$q=1-\nu$）。adaptive lasso在保证lasso吸引人的凸性基础上还得到了参数的一致估计。</p>
<p><img alt="" src="../../img/03/fig3.20.png" /></p>
<h2 id="pathwise-coordinate-optimization">Pathwise Coordinate Optimization</h2>
<p>一种替代计算lasso的LARS算法是简单的坐标下降(simple coordinate descent)。这个想法由Fu (1998)<sup id="fnref:27"><a class="footnote-ref" href="#fn:27" rel="footnote">27</a></sup>和Daubechies et al. (2004)<sup id="fnref:28"><a class="footnote-ref" href="#fn:28" rel="footnote">28</a></sup>提出，后来被Friedman et al. (2007)<sup id="fnref:29"><a class="footnote-ref" href="#fn:29" rel="footnote">29</a></sup>, Wu and Lange (2008)<sup id="fnref:30"><a class="footnote-ref" href="#fn:30" rel="footnote">30</a></sup>和其他人研究及推广。想法是固定Lagrangian形式(3.52)中的惩罚参数$\lambda$，在控制其它参数固定不变时，相继地优化每一个参数。</p>
<p>假设预测变量都经过标准化得到0均值和单位范数。用$\tilde\beta_k(\lambda)$表示惩罚参数为$\lambda$时对$\beta_k$的当前估计。我们可以分离出(3.52)的$\beta_j$,</p>
<p>
<script type="math/tex; mode=display">
R(\tilde\beta(\lambda),\beta_j)=\frac{1}{2}\sum\limits_{i=1}^N(y_i-\sum\limits_{k\neq j}x_{ik}\tilde \beta_k(\lambda)-x_{ij}\beta_j)^2+\lambda \sum\limits_{k\neq j}\vert \tilde \beta_k(\lambda)\vert+\lambda \vert \beta_j\vert
</script>
</p>
<p>其中我们压缩了截距并且为了方便引出因子$\frac 12$。这个可以看成是响应变量为部分残差$y_i-\tilde y_i^{(j)}=y_i-\sum_{k\neq j}x_{ik}\tilde \beta_k(\lambda)$。这有显式解，得到下面的更新</p>
<p>
<script type="math/tex; mode=display">
\tilde \beta_j(\lambda)\leftarrow S(\sum\limits_{i=1}^Nx_{ij}(y_i-\tilde y_i^{(j)}),\lambda)\qquad (3.84)
</script>
</p>
<p>这里$S(t,\lambda)=sign(t)(\vert t\vert-\lambda)_+$是表3.4中的软阈值算子。$S(\cdot)$中的第一个变量是部分残差在标准化变量$x_{ij}$上的简单最小二乘系数。(3.84)的重复迭代——轮流考虑每个变量直到收敛——得到lasso估计$\hat\beta(\lambda)$。</p>
<p>我们也采用这种简单的算法来有效地计算在$\lambda$的每个网格结点上lasso的解。我们从使得$\hat\beta(\lambda_{max})=0$的最小$\lambda_{max}$开始，每次降低一点点来循环考虑每个变量直到收敛。采用前一个解作为$\lambda$新值的“warm start”，$\lambda$再一次降低，并且重复该过程。这个可以比LARS算法快，特别是在大型问题中。它速度的关键在于(3.84)中的量随着$j$变化可以快速更新，并且通常更新后有$\tilde \beta_j=0$。另一方面，它在$\lambda$的网格处求解，而不是整个解的路径。同样类型的算法可以应用到elastic net，grouped lasso以及许多其它惩罚为个体参数的函数之和的模型（Friedman et al., 2010<sup id="fnref:31"><a class="footnote-ref" href="#fn:31" rel="footnote">31</a></sup>）。通过一些修改，这也可以应用到fused lasso（18.4.2节）；细节在Friedman et al. (2007)<sup id="fnref2:29"><a class="footnote-ref" href="#fn:29" rel="footnote">29</a></sup>中给出。</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression (with discussion), Annals of Statistics 32(2): 407–499.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Donoho, D. (2006a). Compressed sensing, IEEE Transactions on Information Theory 52(4): 1289–1306.&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Candes, E. (2006). Compressive sampling, Proceedings of the International Congress of Mathematicians, European Mathematical Society, Madrid, Spain.&#160;<a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Bühlmann, P. and Hothorn, T. (2007). Boosting algorithms: regularization, prediction and model fitting (with discussion), Statistical Science 22(4): 477–505.&#160;<a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>Hastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007). Forward stagewise regression and the monotone lasso, Electronic Journal of Statistics 1: 1–29.&#160;<a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:5" rev="footnote" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>Rosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths, Annals of Statistics 35(3): 1012–1030.&#160;<a class="footnote-backref" href="#fnref:6" rev="footnote" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>Candes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimation when p is much larger than n, Annals of Statistics 35(6): 2313–2351.&#160;<a class="footnote-backref" href="#fnref:7" rev="footnote" title="Jump back to footnote 7 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:7" rev="footnote" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>Bakin, S. (1999). Adaptive regression and model selection in data mining problems, Technical report, PhD. thesis, Australian National University, Canberra.&#160;<a class="footnote-backref" href="#fnref:8" rev="footnote" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Lin, Y. and Zhang, H. (2006). Component selection and smoothing in smoothing spline analysis of variance models, Annals of Statistics 34: 2272–2297.&#160;<a class="footnote-backref" href="#fnref:9" rev="footnote" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:9" rev="footnote" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>Yuan, M. and Lin, Y. (2007). Model selection and estimation in regression with grouped variables, Journal of the Royal Statistical Society, Series B 68(1): 49–67.&#160;<a class="footnote-backref" href="#fnref:10" rev="footnote" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p>Zhao, P., Rocha, G. and Yu, B. (2008). The composite absolute penalties for grouped and hierarchichal variable selection, Annals of Statistics. (to appear).&#160;<a class="footnote-backref" href="#fnref:11" rev="footnote" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>Ravikumar, P., Liu, H., Lafferty, J. and Wasserman, L. (2008). Spam: Sparse additive models, in J. Platt, D. Koller, Y. Singer and S. Roweis (eds), Advances in Neural Information Processing Systems 20, MIT Press, Cambridge, MA, pp. 1201–1208.&#160;<a class="footnote-backref" href="#fnref:12" rev="footnote" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:13">
<p>Bickel, P. J., Ritov, Y. and Tsybakov, A. (2008). Simultaneous analysis of lasso and Dantzig selector, Annals of Statistics. to appear.&#160;<a class="footnote-backref" href="#fnref:13" rev="footnote" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:14">
<p>Efron, B., Hastie, T. and Tibshirani, R. (2007). Discussion of “Dantzig selector” by Candes and Tao, Annals of Statistics 35(6): 2358–2364.&#160;<a class="footnote-backref" href="#fnref:14" rev="footnote" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:15">
<p>Knight, K. and Fu, W. (2000). Asymptotics for lasso-type estimators, Annals of Statistics 28(5): 1356–1378.&#160;<a class="footnote-backref" href="#fnref:15" rev="footnote" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:16">
<p>Greenshtein, E. and Ritov, Y. (2004). Persistence in high-dimensional linear predictor selection and the virtue of overparametrization, Bernoulli 10: 971–988.&#160;<a class="footnote-backref" href="#fnref:16" rev="footnote" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
<li id="fn:17">
<p>Tropp, J. (2004). Greed is good: algorithmic results for sparse approximation, IEEE Transactions on Information Theory 50: 2231– 2242.&#160;<a class="footnote-backref" href="#fnref:17" rev="footnote" title="Jump back to footnote 17 in the text">&#8617;</a></p>
</li>
<li id="fn:18">
<p>Donoho, D. (2006b). For most large underdetermined systems of equations, the minimal l 1 -norm solution is the sparsest solution, Communications on Pure and Applied Mathematics 59: 797–829.&#160;<a class="footnote-backref" href="#fnref:18" rev="footnote" title="Jump back to footnote 18 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:18" rev="footnote" title="Jump back to footnote 18 in the text">&#8617;</a></p>
</li>
<li id="fn:19">
<p>Meinshausen, N. (2007). Relaxed lasso, Computational Statistics and Data Analysis 52(1): 374–393.&#160;<a class="footnote-backref" href="#fnref:19" rev="footnote" title="Jump back to footnote 19 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:19" rev="footnote" title="Jump back to footnote 19 in the text">&#8617;</a></p>
</li>
<li id="fn:20">
<p>Meinshausen, N. and Bühlmann, P. (2006). High-dimensional graphs and variable selection with the lasso, Annals of Statistics 34: 1436–1462.&#160;<a class="footnote-backref" href="#fnref:20" rev="footnote" title="Jump back to footnote 20 in the text">&#8617;</a></p>
</li>
<li id="fn:21">
<p>Tropp, J. (2006). Just relax: convex programming methods for identifying sparse signals in noise, IEEE Transactions on Information Theory 52: 1030–1051.&#160;<a class="footnote-backref" href="#fnref:21" rev="footnote" title="Jump back to footnote 21 in the text">&#8617;</a></p>
</li>
<li id="fn:22">
<p>Zhao, P. and Yu, B. (2006). On model selection consistency of lasso, Journal of Machine Learning Research 7: 2541–2563.&#160;<a class="footnote-backref" href="#fnref:22" rev="footnote" title="Jump back to footnote 22 in the text">&#8617;</a></p>
</li>
<li id="fn:23">
<p>Wainwright, M. (2006). Sharp thresholds for noisy and high-dimensional recovery of sparsity using l 1 -constrained quadratic programming, Technical report, Department of Statistics, University of California, Berkeley.&#160;<a class="footnote-backref" href="#fnref:23" rev="footnote" title="Jump back to footnote 23 in the text">&#8617;</a></p>
</li>
<li id="fn:24">
<p>Bunea, F., Tsybakov, A. and Wegkamp, M. (2007). Sparsity oracle inequalities for the lasso, Electronic Journal of Statistics 1: 169–194.&#160;<a class="footnote-backref" href="#fnref:24" rev="footnote" title="Jump back to footnote 24 in the text">&#8617;</a></p>
</li>
<li id="fn:25">
<p>Fan, J. and Li, R. (2005). Variable selection via nonconcave penalized likelihood and its oracle properties, Journal of the American Statistical Association 96: 1348–1360.&#160;<a class="footnote-backref" href="#fnref:25" rev="footnote" title="Jump back to footnote 25 in the text">&#8617;</a></p>
</li>
<li id="fn:26">
<p>Zou, H. (2006). The adaptive lasso and its oracle properties, Journal of the American Statistical Association 101: 1418–1429.&#160;<a class="footnote-backref" href="#fnref:26" rev="footnote" title="Jump back to footnote 26 in the text">&#8617;</a></p>
</li>
<li id="fn:27">
<p>Fu, W. (1998). Penalized regressions: the bridge vs. the lasso, Journal of Computational and Graphical Statistics 7(3): 397–416.&#160;<a class="footnote-backref" href="#fnref:27" rev="footnote" title="Jump back to footnote 27 in the text">&#8617;</a></p>
</li>
<li id="fn:28">
<p>Daubechies, I., Defrise, M. and De Mol, C. (2004). An iterative thresholding algorithm for linear inverse problems with a sparsity constraint, Communications on Pure and Applied Mathematics 57: 1413–1457.&#160;<a class="footnote-backref" href="#fnref:28" rev="footnote" title="Jump back to footnote 28 in the text">&#8617;</a></p>
</li>
<li id="fn:29">
<p>Friedman, J., Hastie, T., Hoefling, H. and Tibshirani, R. (2007). Pathwise coordinate optimization, Annals of Applied Statistics 2(1): 302–332.&#160;<a class="footnote-backref" href="#fnref:29" rev="footnote" title="Jump back to footnote 29 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:29" rev="footnote" title="Jump back to footnote 29 in the text">&#8617;</a></p>
</li>
<li id="fn:30">
<p>Wu, T. and Lange, K. (2008). Coordinate descent procedures for lasso penalized regression, Annals of Applied Statistics 2(1): 224–244.&#160;<a class="footnote-backref" href="#fnref:30" rev="footnote" title="Jump back to footnote 30 in the text">&#8617;</a></p>
</li>
<li id="fn:31">
<p>Friedman, J., Hastie, T. and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent, Journal of Statistical Software 33(1): 1–22.&#160;<a class="footnote-backref" href="#fnref:31" rev="footnote" title="Jump back to footnote 31 in the text">&#8617;</a></p>
</li>
</ol>
</div>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = "https://esl.hohoweiya.xyz/03 Linear Methods for Regression/3.8 More on the Lasso and Related Path Algorithms/index.html";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "https://esl.hohoweiya.xyz/03 Linear Methods for Regression/3.8 More on the Lasso and Related Path Algorithms/index.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://esl-hohoweiya-xyz.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//esl-hohoweiya-xyz.disqus.com/count.js" async></script></div>
        </div>

        <footer class="col-md-12">
            <hr>
            
                <center>Copyright &copy; 2016-2018 weiya</center>
            
            <center>Powered by <a href="http://www.mkdocs.org/">MkDocs</a> and <a href="http://bootswatch.com/yeti/">Yeti</a></center>
        </footer>

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script>
        <script>var base_url = '../..';</script>
        <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
        <script src="../../js/base.js"></script>
	<script src="../../js/jquery.dropdown.js"></script>
        <script src="../../search/require.js"></script>
        <script src="../../search/search.js"></script>
<!--
        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="modal-header">
                        <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">关闭</span></button>
                        <h4 class="modal-title" id="exampleModalLabel">搜索</h4>
                    </div>
                    <div class="modal-body">
                        <p>
                            请在下面输入你要搜索的文本（仅支持英文）：
                        </p>
                        <form role="form">
                            <div class="form-group">
                                <input type="text" class="form-control" placeholder="搜索..." id="mkdocs-search-query">
                            </div>
                        </form>
                        <div id="mkdocs-search-results"></div>
                    </div>
                    <div class="modal-footer">
                    </div>
                </div>
            </div>
        </div>
      -->
    </body>
</html>