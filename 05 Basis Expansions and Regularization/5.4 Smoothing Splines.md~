# 光滑样条

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-16                               |

这里我们讨论样条基方法来完全避免通过使用最大结点集合造成的结点选择的问题。拟合的复杂度由正则化来控制。考虑下面的问题：在所有有二阶连续微分的函数$f(x)$中，找到一个使得惩罚残差平方和最小

$$
RSS(f,\lambda)=\sum\limits_{i=1}^N\{y_i-f(x_i)\}^2+\lambda\int \{f''(t)\}^2dt\qquad (5.9)
$$

其中$\lambda$是固定的光滑参数。第一项衡量和数据的近似程度，第二项对函数的曲率进行惩罚，且$\lambda$建立了而两者之间的一个平衡。两个特别的例子是：

- $\lambda=0$： $f$ 可以是任意对数据插值的函数
- $\lambda=\infty$： $f$ 简单的最小而成拟合，因为不容忍任何的二次微分。

这从非常粗糙变到非常光滑，而且希望$\lambda\in(0,\infty)$在非常粗糙与非常光滑之间表示一个对我们感兴趣的函数类。

!!! warning "weiya 注"
    对于$y=f(x)$曲线的曲率定义为
	$$
	\kappa = \frac{\vert y''\vert}{(1+y'^2)^{3/2}}
	$$
	但这里为什么只用二阶导。
	
准则（5.9）定义在无限维函数空间上——实际上，是二阶导有定义的Sobolev函数空间。值得注意的是，可以证明（5.9）有一个显式的有限维的唯一一个最小值点，是一个结点在唯一的$x_i,i=1,2,\ldots,N$处的自然三次样条。（练习5.7）。表面上看，这个函数族仍然是过参量化，因为有$N$个结点，这意味着$N$个自由度。然而，惩罚项转换为对样条系数的惩罚，使得某些向着线性拟合收缩。

因为解为自然样条，所以我们可以写成

$$
f(x)=\sum\limits_{j=1}^NN_j(x)\theta_j\qquad (5.10)
$$

其中$N_j(x)$是表示自然样条族的$N$维基函数集（5.2.1节和练习5.4）。准则因此退化成

$$
RSS(\theta,\lambda)=(\mathbf y-\mathbf N\theta)^T(\mathbf y-\mathbf N\theta)+\lambda\theta^T\Omega_N\theta\qquad (5.11)
$$

其中$\{\mathbf N\}_{ij}=N_j(x_i)$以及$\{\Omega_N\}_{jk}=\int N_j''(t)N_k''(t)dt$.可以看到解为

$$
\hat\theta = (\mathbf N^T\mathbf+\lambda\Omega_N)^{-1}\mathbf N^T\mathbf y\qquad (5.12)
$$

是广义岭回归。拟合后的光滑样条由下式给出

$$
\hat f(x) = \sum\limits_{j=1}^NN_j(x)\hat\theta_j\qquad (5.13)
$$

光滑样条的有效计算技巧将在本章的附录中讨论。

图5.6显示了对青少年骨矿物质密度（BMD）部分数据的光滑样条拟合。响应变量为在连续两次检查时脊髓的BMD的相对改变，一般相隔一年。数据点的颜色由性别来编码，得到两个独立的拟合曲线。简单的总结加强了数据中的证据，女性增长相对于男性要早两年。两种情形下光滑参数$\lambda$近似为0.00022；它的选择将在下一节中讨论。
